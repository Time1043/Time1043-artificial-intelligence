# Large Language Model

- Reference - org

  [openai api docs](https://platform.openai.com/docs/introduction), [openai-playground](https://platform.openai.com/playground),

  [langchain org](https://python.langchain.com/v0.1/docs/get_started/introduction/), [langchain (github)](https://github.com/langchain-ai/langchain), [langchain api docs (python)](https://api.python.langchain.com/en/latest/langchain_api_reference.html), 

  [ollama github](https://github.com/ollama/ollama/blob/main/docs/openai.md), [ollama lib](https://ollama.com/library), [llama OpenAI compatibility](https://ollama.com/blog/openai-compatibility), 

  [kimi github](https://github.com/LLM-Red-Team/kimi-free-api), [kimi docs](https://platform.moonshot.cn/docs/api/tool-use),  

  [xunfei spark docs](https://www.xfyun.cn/doc/spark/Web.html), [xunfei spark-ai-python (github)](https://github.com/iflytek/spark-ai-python), 

- Reference

  kimi, 

  spark, 

  baidu, 

  tongyi, 

- Reference - blog

  [blog](https://linux.do/c/reading/32), [classmate github](https://github.com/boxiyang/ChatGPT-Assistant-2/blob/main/app.py), 

- Reference - course

  [linlili course code](https://n6fo0mbcz6.feishu.cn/drive/folder/ZEpgfI7OiloJaKdf8IIc6eg5nnd), 

  [Complete Streamlit Python Course](https://www.youtube.com/watch?v=RjiqbTLW9_E&list=PLa6CNrvKM5QU7AjAS90zCMIwi9RTFNIIW), [Streamlit Tutorials](https://www.youtube.com/watch?v=FOULV9Xij_8&list=PL7QI8ORyVSCaejt2LICRQtOTwmPiwKO2n), [Build 12 Data Science Apps with Python and Streamlit](https://www.youtube.com/watch?v=JwSS70SZdyM)
  
  



# LLM (linlili)

- ä»Šéæ˜”æ¯”

  è¿‡å»ï¼šè‹¦è¯»æœºå™¨å­¦ä¹ ç®—æ³•ã€ä»å¤´è®­ç»ƒï¼›ä»£ç è°ƒæ•´æ¨¡å‹

  ç°åœ¨ï¼šå¤§æ¨¡å‹åœ¨ç†è§£åŠç”Ÿæˆè‡ªç„¶è¯­è¨€ä¸Šæå¤§æå‡ã€å¤§æ¨¡å‹APIï¼›å…è®¸è‡ªç„¶è¯­è¨€çš„è¦æ±‚

- BigPicture

  AIæ¨¡å‹æœ¬èº«ï¼šæ— æ³•è®°å¿†å†å²å¯¹è¯ã€ä¸ä¼šé˜…è¯»å¤–éƒ¨æ–‡æ¡£ã€ä¸æ“…é•¿æ•°å­¦è®¡ç®—ä¸æ‡‚å¦‚ä½•ä¸Šç½‘

  ç”¨ä»£ç æ­¦è£…ï¼šç»™æ¨¡å‹æ·»åŠ **è®°å¿†**ã€ç»™æ¨¡å‹è¯»å–**å¤–éƒ¨çŸ¥è¯†åº“**çš„èƒ½åŠ›ã€é€šè¿‡æ¨ç†ååŒè®©æ¨¡å‹èƒ½å¤Ÿæ ¹æ®ä»»åŠ¡è¦æ±‚è‡ªä¸»è°ƒç”¨ä¸€ç³»åˆ—**å¤–éƒ¨å·¥å…·**

  é€šç”¨æ¡†æ¶ï¼šå›½å†…å›½å¤–ã€äº‘ç«¯æœ¬åœ°ã€å¼€æºé—­æº (è¿ç§»)

  ![](res/Snipaste_2024-04-10_21-40-06.png)





## API (openai)

- ç”¨ä»£ç ä¸AIå¯¹è¯

  APIåŸºç¡€ï¼šå¯†é’¥ã€è¯·æ±‚ã€APIè®¡è´¹(tokenã€tiktoken)
  
  APIå‚æ•°ï¼š`max_tokens`ã€`temperature`ã€`ç½®ä¿¡åº¦é˜ˆå€¼`ã€`å­˜åœ¨æƒ©ç½š`ã€`é¢‘ç‡æƒ©ç½š` (é•¿åº¦ åˆ›é€ æ€§ éšæœºæ€§)
  
  ç”¨æ³•æç¤ºï¼šæ–‡æœ¬æ€»ç»“ã€æ–‡æœ¬æ’°å†™ã€æ–‡æœ¬åˆ†ç±»ã€æ–‡æœ¬ç¿»è¯‘ 





### å…¥é—¨è°ƒç”¨

- å¿«é€Ÿå…¥é—¨ (ä»£ç†api_key å…³æ‰æ¢¯å­)

  ç¯å¢ƒ

  ```bash
  pip install openai  # è¯·æ±‚å“åº”
  pip install tiktoken  # tokenè®¡ç®—
  
  ```

  ç¼–ç 

  åˆ›å»ºå®ä¾‹ `client = OpenAI()`

  è°ƒç”¨æ–¹æ³• `client.chat.completions.create(model, message)`

  ```python
  import yaml
  from openai import OpenAI
  
  # ä¿æŠ¤å¯†é’¥ä¿¡æ¯
  yaml_file = "../../key/key.yaml"
  with open(yaml_file, 'r') as file:
      data_key = yaml.safe_load(file)
  openai_info = data_key.get('openai-proxy', {})
  openai_api_key = openai_info.get('OPENAI_API_KEY')
  base_url = openai_info.get('BASE_URL')
  # print(openai_api_key, base_url)
  
  # åˆ›å»ºOpenAIå®¢æˆ·ç«¯å®ä¾‹ å‘é€è¯·æ±‚å¾—åˆ°å“åº”
  client = OpenAI(api_key=openai_api_key, base_url=base_url)
  response = client.chat.completions.create(
      model="gpt-3.5-turbo",
      messages=[
          {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªçƒ­çˆ±å¥½æ•…äº‹çš„æ–‡å­¦å®¶ï¼ŒåŒæ—¶ä¹Ÿæ˜¯æœ‰ç€æ·±åšçš„æ–‡å­¦åŠŸåº•çš„ä½œå®¶ã€‚"},
          {"role": "user", "content": "ç»™æˆ‘æ¨èä¸€ä¸ªå½±è§†ä½œå“å§"},
          {"role": "assistant", "content": "å¥½çš„ï¼Œæˆ‘å°†ç»™æ‚¨æ¨èã€Šdoctor whoã€‹ï¼Œå¹¶ä¸”æˆ‘æœ‰è¶³å¤Ÿæ‰“åŠ¨ä½ çš„ç†ç”±ï¼æˆ‘æœ€å–œæ¬¢å…¶ä¸­çš„ä¸€å¥è¯ï¼Œä»–æœ‰ç€ç›´å‡»çµé­‚çš„åŠ›é‡ï¼Œç›¸ä¿¡è¿™ä¹Ÿèƒ½é¼“åŠ¨ä½ ï¼"},
          {"role": "user", "content": "æ˜¯å—ï¼Ÿæˆ‘å¾ˆæœŸå¾…ä½ çš„ç†ç”±å’Œä½ å–œæ¬¢çš„é‚£å¥è¯ï¼"},
      ],
      max_tokens=300
  )
  print(response)
  
  
  """
  ChatCompletion(
      id='chatcmpl-9CRfLldAmmg5MR4jtbWdKBs0yq8so', 
      choices=[
          Choice(
              finish_reason='stop', 
              index=0, 
              logprobs=None, 
              message=ChatCompletionMessage(
                  content='å½“ç„¶ï¼ã€Šdoctor whoã€‹æ˜¯ä¸€éƒ¨è‹±å›½é•¿å¯¿ç§‘å¹»ç”µè§†å‰§ï¼Œè®²è¿°ä¸€ä½å¤–è²Œå¹´è½»ä½†å®é™…å¹´é¾„å‡ ç™¾å²çš„æ—¶é—´é¢†ä¸»â€”â€”åšå£«ï¼ˆDoctorï¼‰çš„å†’é™©æ•…äº‹ã€‚ä»–æ˜¯ä¸€åæ‹¥æœ‰æ—¶é—´æ—…è¡Œèƒ½åŠ›çš„æ—¶é—´é¢†ä¸»ï¼Œé©¾é©¶ç€TARDISï¼ˆæ—¶é—´é£èˆ¹ï¼‰åœ¨æ—¶é—´å’Œç©ºé—´ä¸­è¿›è¡Œå¥‡å¹»çš„å†’é™©ã€‚è¿™éƒ¨å‰§æ¢è®¨äº†å…³äºå‹è°Šã€å‹‡æ°”ã€æ‚²ä¼¤ã€å¸Œæœ›ç­‰äººæ€§ä¸»é¢˜ï¼ŒåŒæ—¶ä¹Ÿå¼•å‘è§‚ä¼—å¯¹ç”Ÿå‘½ã€å®‡å®™çš„æ·±åˆ»æ€è€ƒã€‚\n\næˆ‘æœ€å–œæ¬¢çš„ä¸€å¥è¯æ¥è‡ªç¬¬åä¸€ä»»åšå£«ï¼Œä»–è¯´ï¼šâ€œæˆ‘ä»¬ä¸åªæ˜¯æ„Ÿå—ç€æ—¶é—´çš„æµé€ï¼Œæˆ‘ä»¬ä¹Ÿæ˜¯æ—¶é—´çš„æµé€ã€‚æˆ‘ä»¬æ˜¯ä¸€ç§èƒ½å¤Ÿæ„ŸçŸ¥æ—¶é—´çš„å­˜åœ¨ï¼Œæˆ‘ä»¬æ˜¯æ—¶é—´æœ¬èº«ã€‚â€è¿™å¥è¯è¡¨è¾¾äº†åšå£«è¿™ä¸ªè§’è‰²å¯¹æ—¶é—´å’Œç”Ÿå‘½çš„ç‹¬ç‰¹ç†è§£ï¼Œæ·±æ·±åœ°è§¦åŠ¨äº†æˆ‘ã€‚\n\nç›¸ä¿¡çœ‹å®Œã€Šdoctor whoã€‹ï¼Œæ‚¨ä¹Ÿä¼šè¢«è¿™éƒ¨ä½œå“ä¸­æ·±åˆ»çš„æƒ…æ„Ÿå’Œæ·±åº¦çš„æ€è€ƒæ‰€æ„ŸåŠ¨ï¼å¸Œæœ›æ‚¨ä¼šå–œæ¬¢ï¼', 
                  role='assistant', 
                  function_call=None, 
                  tool_calls=None
              )
          )
      ], 
      created=1712753311, 
      model='gpt-3.5-turbo-0125', 
      object='chat.completion', 
      system_fingerprint='fp_b28b39ffa8', 
      usage=CompletionUsage(completion_tokens=334, prompt_tokens=122, total_tokens=456)
  )
  """
  
  ```
  
  



- tokenè®¡è´¹

  [web tokenizer](https://platform.openai.com/tokenizer)ã€tiktoken lib `len(encoding.encode(text))`

  ```python
  import tiktoken
  
  text = """å½“ç„¶ï¼ã€Šdoctor whoã€‹æ˜¯ä¸€éƒ¨è‹±å›½é•¿å¯¿ç§‘å¹»ç”µè§†å‰§ï¼Œè®²è¿°ä¸€ä½å¤–è²Œå¹´è½»ä½†å®é™…å¹´é¾„å‡ ç™¾å²çš„æ—¶é—´é¢†ä¸»â€”â€”åšå£«ï¼ˆDoctorï¼‰çš„å†’é™©æ•…äº‹ã€‚
  ä»–æ˜¯ä¸€åæ‹¥æœ‰æ—¶é—´æ—…è¡Œèƒ½åŠ›çš„æ—¶é—´é¢†ä¸»ï¼Œé©¾é©¶ç€TARDISï¼ˆæ—¶é—´é£èˆ¹ï¼‰åœ¨æ—¶é—´å’Œç©ºé—´ä¸­è¿›è¡Œå¥‡å¹»çš„å†’é™©ã€‚è¿™éƒ¨å‰§æ¢è®¨äº†å…³äºå‹è°Šã€å‹‡æ°”ã€æ‚²ä¼¤ã€å¸Œæœ›ç­‰äººæ€§ä¸»é¢˜ï¼Œ
  åŒæ—¶ä¹Ÿå¼•å‘è§‚ä¼—å¯¹ç”Ÿå‘½ã€å®‡å®™çš„æ·±åˆ»æ€è€ƒã€‚\n\n
  æˆ‘æœ€å–œæ¬¢çš„ä¸€å¥è¯æ¥è‡ªç¬¬åä¸€ä»»åšå£«ï¼Œä»–è¯´ï¼šâ€œæˆ‘ä»¬ä¸åªæ˜¯æ„Ÿå—ç€æ—¶é—´çš„æµé€ï¼Œæˆ‘ä»¬ä¹Ÿæ˜¯æ—¶é—´çš„æµé€ã€‚æˆ‘ä»¬æ˜¯ä¸€ç§èƒ½å¤Ÿæ„ŸçŸ¥æ—¶é—´çš„å­˜åœ¨ï¼Œæˆ‘ä»¬æ˜¯æ—¶é—´æœ¬èº«ã€‚â€
  è¿™å¥è¯è¡¨è¾¾äº†åšå£«è¿™ä¸ªè§’è‰²å¯¹æ—¶é—´å’Œç”Ÿå‘½çš„ç‹¬ç‰¹ç†è§£ï¼Œæ·±æ·±åœ°è§¦åŠ¨äº†æˆ‘ã€‚\n\n
  ç›¸ä¿¡çœ‹å®Œã€Šdoctor whoã€‹ï¼Œæ‚¨ä¹Ÿä¼šè¢«è¿™éƒ¨ä½œå“ä¸­æ·±åˆ»çš„æƒ…æ„Ÿå’Œæ·±åº¦çš„æ€è€ƒæ‰€æ„ŸåŠ¨ï¼å¸Œæœ›æ‚¨ä¼šå–œæ¬¢ï¼', 
  """
  
  encoding = tiktoken.encoding_for_model("gpt-3.5-turbo")  # è¿”å›å¯¹åº”ç¼–ç å™¨
  print(encoding.encode(text))  # è¿”å›token idç»„æˆçš„åˆ—è¡¨ [40265, 61994, 6447, 28038, 38083, 889,
  print(len(encoding.encode(text)))  # è®¡ç®—token
  
  ```

  ContextWindow

  ![](res/Snipaste_2024-04-10_21-38-39.png)





### å‚æ•°è®¾ç½®

- è°ƒå‚ (é•¿åº¦ åˆ›é€ æ€§ éšæœºæ€§)

  `max_tokens`ï¼šå¼ºç¡¬æ§åˆ¶ï¼Œä¸ä¼šæ®æ­¤è°ƒèŠ‚é•¿åº¦ï¼Œè€Œæ˜¯æ‹¦è…°æˆªæ–­ -> `å›å¤åœ¨500å­—å†…...`

  `temperature`ï¼šéšæœºæ€§åˆ›é€ æ€§ï¼Œ0åˆ°2ä¹‹é—´é»˜è®¤ä¸º1ï¼Œè¶Šä½éšæœºæ€§è¶Šä½ (å¤ªé«˜ç”šè‡³ä¸æŒ‰äººç±»è¯­è¨€)

  `top_p`ï¼šæ§åˆ¶å›ç­”çš„éšæœºæ€§å’Œåˆ›é€ æ€§ã€0åˆ°1ä¹‹é—´ (ä¸€èˆ¬ä¸è¦åŒæ—¶ä¿®æ”¹)

  - temperatueï¼šæ”¹å˜å„ä¸ªtokençš„æ¦‚ç‡åˆ†å¸ƒï¼šæ¸©åº¦è¶Šä½ï¼Œæ¦‚ç‡åˆ†å¸ƒçš„å³°é«˜ï¼Œæ¦‚ç‡è¾ƒé«˜çš„è¯é€‰æ‹©æƒé‡å¢å¤§ã€æ¦‚ç‡è¾ƒä½çš„è¯è¾ƒå®¹æ˜“å¿½ç•¥ï¼Œæ¨¡å‹çš„è¾“å‡ºå…·æœ‰ç¡®å®šæ€§
  - top_pï¼šä¸æ”¹å˜è¯çš„æ¦‚ç‡åˆ†å¸ƒï¼Œè€Œæ˜¯å…³æ³¨äºæˆªå–æ¦‚ç‡åˆ†å¸ƒçš„ä¸€ä¸ªå­é›†ï¼Œå­é›†çš„ç´¯ç§¯æ¦‚ç‡å¤§äºç­‰äºtop_p
  
  ![](res/Snipaste_2024-04-10_21-27-43.png)
  
  
  
  `frequency_penalty`ï¼šå¤šå¤§ç¨‹åº¦ä¸Šæƒ©ç½šé‡å¤å†…å®¹ã€-2åˆ°2ä¹‹é—´é»˜è®¤ä¸º0ã€**å‡ºç°å¾—è¶Šé¢‘ç¹**ä»Šåç”Ÿæˆçš„æ¦‚ç‡é™ä½ (æƒ³è¦å‡å°‘é«˜é¢‘è¯å‡ºç°æ¬¡æ•°)
  
  `presence_penalty`ï¼šé™ä½æ–‡æœ¬çš„é‡å¤æ€§ã€-2åˆ°2ä¹‹é—´é»˜è®¤ä¸º0ã€å‡ºç°äº†å°±**åŒç­‰æƒ…å†µé™ä½é¢‘ç‡** (æƒ³è¦é‡å¤è¯å°‘)
  
  ![](res/Snipaste_2024-04-10_21-44-51.png)
  
- ä»£ç å®ç°

  ```python
  # max_tokens
  response = client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[
      {
        "role": "user",
        "content": "å››å¤§æ–‡æ˜å¤å›½åˆ†åˆ«æœ‰å“ªäº›"
      }
    ],
    max_tokens=100
  )
  print(response.choices[0].message.content)
  
  # temperature
  response = client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[
      {
        "role": "user",
        "content": "å››å¤§æ–‡æ˜å¤å›½åˆ†åˆ«æœ‰å“ªäº›"
      }
    ],
    max_tokens=100,
    temperature=2
  )
  print(response.choices[0].message.content)
  
  # top_p
  response = client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[
      {
        "role": "user",
        "content": "å››å¤§æ–‡æ˜å¤å›½åˆ†åˆ«æœ‰å“ªäº›"
      }
    ],
    max_tokens=300,
    top_p=0.4
  )
  print(response.choices[0].message.content)
  
  # frequency_penalty
  response = client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[
      {
        "role": "user",
        "content": "ç”Ÿæˆä¸€ä¸ªè´­ç‰©æ¸…å•ï¼ŒåŒ…å«è‡³å°‘20ä¸ªç‰©å“ï¼Œæ¯ä¸ªç‰©å“ä¹‹é—´ç”¨é€—å·è¿›è¡Œåˆ†éš”ï¼Œä¾‹å¦‚ï¼šè‹¹æœï¼Œé¦™è•‰ï¼Œç‰›å¥¶"
      }
    ],
    max_tokens=300,
    frequency_penalty=-2
  )
  print(response.choices[0].message.content)
  
  ```
  
  



## prompt engineering

- æç¤ºè¯å·¥ç¨‹ [Best practices for prompt engineering with OpenAI API](https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-openai-api)

  ç ”ç©¶å¦‚ä½•æé«˜å’ŒAIçš„æ²Ÿé€šè´¨é‡å’Œæ•ˆç‡ï¼Œæ ¸å¿ƒæ˜¯æç¤ºçš„å¼€å‘å’Œä¼˜åŒ–
  
  è§„èŒƒã€æ ¼å¼ã€é›¶æ ·æœ¬å°æ ·æœ¬ã€æ€ç»´é“¾å’Œåˆ†æ­¥éª¤æ€è€ƒ



- æç¤ºè¯æœ€ä½³å®è·µ

  é™å®šè¾“å‡ºæ ¼å¼

  é›¶æ ·æœ¬å’Œå°æ ·æœ¬

  æ€ç»´é“¾ä¸åˆ†æ­¥éª¤æ€è€ƒ

- æç¤ºå·¥ç¨‹åŸåˆ™

  1. ä½¿ç”¨æœ€æ–°çš„æ¨¡å‹
  2. æŒ‡ä»¤æ”¾åœ¨æç¤ºçš„å¼€å¤´ï¼Œç”¨`###`æˆ–`"""`åˆ†å‰²æŒ‡ä»¤å’Œä¸Šä¸‹æ–‡
  3. å°½å¯èƒ½å¯¹ä¸Šä¸‹æ–‡å’Œè¾“å‡ºçš„é•¿åº¦ã€æ ¼å¼ã€é£æ ¼ç­‰ç»™å‡ºå…·ä½“ã€æè¿°æ€§ã€è¯¦ç»†çš„è¦æ±‚
  4. é€šè¿‡ä¸€äº›ä¾‹å­æ¥é˜æ˜æƒ³è¦çš„è¾“å‡ºæ ¼å¼
  5. å…ˆä»é›¶æ ·æœ¬æç¤ºå¼€å§‹ï¼Œæ•ˆæœä¸å¥½åˆ™ç”¨å°æ ·æœ¬æç¤º
  6. å‡å°‘ç©ºæ´å’Œä¸ä¸¥è°¨çš„æè¿°
  7. ä¸å…¶å‘ŠçŸ¥ä¸åº”è¯¥åšä»€ä¹ˆï¼Œä¸å¦‚å‘ŠçŸ¥åº”è¯¥åšä»€ä¹ˆ

  ![Snipaste_2024-04-10_21-58-50](res/Snipaste_2024-04-10_21-58-50.png)

- ä¸ºäº†åç»­

  é™å®šè¾“å‡ºæ ¼å¼ï¼šyamlã€xmlã€json (ä¸è¦åŒ…å«ä»»ä½•æ²¡å¿…è¦çš„è¡¥å……ä¿¡æ¯) 

  å°æ ·æœ¬æç¤ºï¼š`user`ã€`assistant`

  æ€ç»´é“¾ï¼š[Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903)

  - ç®—æ•°ã€å¸¸è¯†ã€ç¬¦å·æ¨ç†ç­‰å¤æ‚ä»»åŠ¡ã€‚`let's think step by step.`

  



### é™å®šè¾“å‡ºæ ¼å¼

- é™å®šè¾“å‡ºæ ¼å¼

  ```python
  # é™å®šè¾“å‡ºæ ¼å¼
  response = client.chat.completions.create(
    model="gpt-3.5-turbo-0125",
    response_format={ "type": "json_object" },
    messages=[...]
  )
  
  ```
  
- ä»£ç å®ç°
  

  ```python
  import json
  import yaml
  from openai import OpenAI
  
  yaml_file = "../../key/key.yaml"
  with open(yaml_file, 'r') as file:
      data_key = yaml.safe_load(file)
  openai_info = data_key.get('openai-proxy', {})
  openai_api_key = openai_info.get('OPENAI_API_KEY')
  base_url = openai_info.get('BASE_URL')
  
  prompt = """ç”Ÿæˆä¸€ä¸ªç”±ä¸‰ä¸ªè™šæ„çš„è®¢å•ä¿¡æ¯æ‰€ç»„æˆçš„åˆ—è¡¨ï¼Œä»¥SONæ ¼å¼è¿›è¡Œè¿”å›ã€‚
  JSONåˆ—è¡¨é‡Œçš„æ¯ä¸ªå…ƒç´ åŒ…å«ä»¥ä¸‹ä¿¡æ¯ï¼š
  order_idã€customer_nameã€order_itemã€phoneã€‚
  æ‰€æœ‰ä¿¡æ¯éƒ½æ˜¯å­—ç¬¦ä¸²ã€‚
  é™¤äº†JSONä¹‹å¤–ï¼Œä¸è¦è¾“å‡ºä»»ä½•é¢å¤–çš„æ–‡æœ¬ã€‚"""
  
  client = OpenAI(api_key=openai_api_key, base_url=base_url)
  response = client.chat.completions.create(
      model="gpt-3.5-turbo",
      messages=[
          {"role": "user", "content": prompt},
      ]
  )
  content = response.choices[0].message.content
  result = json.loads(content)
  print(result)  
  print(result[0]["phone"])  # å¯ä»¥ç›´æ¥è¢«ä»£ç è§£æ
  
  
  """
  [
      {'order_id': '001', 'customer_name': 'Alice', 'order_item': 'iPhone 12', 'phone': '123-456-7890'},
      {'order_id': '002', 'customer_name': 'Bob', 'order_item': 'Samsung Galaxy S21', 'phone': '987-654-3210'},
      {'order_id': '003', 'customer_name': 'Charlie', 'order_item': 'Google Pixel 5', 'phone': '456-789-0123'}
  ]
  
  123-456-7890
  """
  
  ```
  
  



### é›¶æ ·æœ¬å’Œå°æ ·æœ¬

- å°æ ·æœ¬æç¤º 

  é›¶æ ·æœ¬æç¤ºï¼šå³ç›´æ¥ä¸¢é—®é¢˜ç»™AIï¼Œæ²¡æœ‰ç»™ä»»ä½•ç¤ºèŒƒ

  å°æ ·æœ¬ç¤ºèŒƒï¼šè®©AIå¿«é€Ÿé€‚åº”æ–°ä»»åŠ¡ï¼Œä¸éœ€è¦å¯¹æ¨¡å‹æœ‰è®­ç»ƒ (æˆæœ¬ä½)

  ![](res/Snipaste_2024-04-10_22-19-23.png)

- ä»£ç å®ç°

  ```python
  import yaml
  from openai import OpenAI
  
  yaml_file = "../../key/key.yaml"
  with open(yaml_file, 'r') as file:
      data_key = yaml.safe_load(file)
  openai_info = data_key.get('openai-proxy', {})
  openai_api_key = openai_info.get('OPENAI_API_KEY')
  base_url = openai_info.get('BASE_URL')
  
  client = OpenAI(api_key=openai_api_key, base_url=base_url)
  response = client.chat.completions.create(
      model="gpt-3.5-turbo",
      messages=[
          {
              "role": "user",
              "content": "æ ¼å¼åŒ–ä»¥ä¸‹ä¿¡æ¯ï¼š\nå§“å -> å¼ ä¸‰\nå¹´é¾„ -> 27\nå®¢æˆ·ID -> 001"
          },
          {
              "role": "assistant",
              "content": "##å®¢æˆ·ä¿¡æ¯\n- å®¢æˆ·å§“åï¼šå¼ ä¸‰\n- å®¢æˆ·å¹´é¾„ï¼š27å²\n- å®¢æˆ·IDï¼š001"
          },
          {
              "role": "user",
              "content": "æ ¼å¼åŒ–ä»¥ä¸‹ä¿¡æ¯ï¼š\nå§“å -> æå››\nå¹´é¾„ -> 42\nå®¢æˆ·ID -> 002"
          },
          {
              "role": "assistant",
              "content": "##å®¢æˆ·ä¿¡æ¯\n- å®¢æˆ·å§“åï¼šæå››\n- å®¢æˆ·å¹´é¾„ï¼š42å²\n- å®¢æˆ·IDï¼š002"
          },
          {
              "role": "user",
              "content": "æ ¼å¼åŒ–ä»¥ä¸‹ä¿¡æ¯ï¼š\nå§“å -> ç‹äº”\nå¹´é¾„ -> 32\nå®¢æˆ·ID -> 003"
          }
      ]
  )
  content = response.choices[0].message.content
  print(content)
  
  """
  ##å®¢æˆ·ä¿¡æ¯
  - å®¢æˆ·å§“åï¼šç‹äº”
  - å®¢æˆ·å¹´é¾„ï¼š32å²
  - å®¢æˆ·IDï¼š003
  """
  
  ```
  





### æ€ç»´é“¾ä¸åˆ†æ­¥éª¤æ€è€ƒ

- æ€ç»´é“¾ä¸åˆ†æ­¥éª¤æ€è€ƒ

  é—®é¢˜ï¼šAIä¸æ“…é•¿åšæ•°å­¦ç­‰é€»è¾‘æ¨æ–­ (ç”Ÿæˆæ¯ä¸ªtokençš„æ—¶é—´å·®ä¸å¤š ä¸ä¼šå› ä¸ºè®¾è®¡æ›´å¤šæ€è€ƒè€ŒèŠ±è´¹æ›´å¤šæ—¶é—´ å›«å›µåæ£)
  
  æ€ç»´é“¾ï¼šæŠŠæ³¨æ„åŠ›**é›†ä¸­åœ¨å½“å‰æ€è€ƒæ­¥éª¤**ä¸Šï¼Œå‡å°‘ä¸Šä¸‹æ–‡çš„è¿‡å¤šå¹²æ‰° (å¤æ‚ä»»åŠ¡æœ‰æ›´å¤§æ¦‚ç‡å¾—åˆ°å‡†ç¡®ç»“æœ) 
  
  ![Snipaste_2024-05-12_16-35-08](res/Snipaste_2024-05-12_16-35-08.png)
  
- ä»£ç å®ç°

  ```python
  import yaml
  from openai import OpenAI
  
  yaml_file = "../../key/key.yaml"
  with open(yaml_file, 'r') as file:
      data_key = yaml.safe_load(file)
  openai_info = data_key.get('openai-proxy', {})
  openai_api_key = openai_info.get('OPENAI_API_KEY')
  base_url = openai_info.get('BASE_URL')
  
  client = OpenAI(api_key=openai_api_key, base_url=base_url)
  
  # wrong answer
  response = client.chat.completions.create(
      model="gpt-3.5-turbo",
      messages=[
          {
              "role": "user",
              "content": "è¯¥ç»„ä¸­çš„å¥‡æ•°åŠ èµ·æ¥ä¸ºå¶æ•°ï¼š4ã€8ã€9ã€15ã€12ã€2ã€1ï¼Œå¯¹å—ï¼Ÿ"
          },
          {
              "role": "assistant",
              "content": "æ‰€æœ‰å¥‡æ•°ç›¸åŠ ç­‰äº25ã€‚ç­”æ¡ˆä¸ºå¦ã€‚"
          },
          {
              "role": "user",
              "content": "è¯¥ç»„ä¸­çš„å¥‡æ•°åŠ èµ·æ¥ä¸ºå¶æ•°ï¼š17ã€10ã€19ã€4ã€8ã€12ã€24ï¼Œå¯¹å—ï¼Ÿ"
          },
          {
              "role": "assistant",
              "content": "æ‰€æœ‰å¥‡æ•°ç›¸åŠ ç­‰äº36ã€‚ç­”æ¡ˆä¸ºæ˜¯ã€‚"
          },
          {
              "role": "user",
              "content": "è¯¥ç»„ä¸­çš„å¥‡æ•°åŠ èµ·æ¥ä¸ºå¶æ•°ï¼š15ã€12ã€5ã€3ã€72ã€17ã€1ï¼Œå¯¹å—ï¼Ÿ"
          },
      ]
  )
  print(response.choices[0].message.content)
  
  # correct answer: thought chain
  response = client.chat.completions.create(
      model="gpt-3.5-turbo",
      messages=[
          {
              "role": "user",
              "content": "è¯¥ç»„ä¸­çš„å¥‡æ•°åŠ èµ·æ¥ä¸ºå¶æ•°ï¼š4ã€8ã€9ã€15ã€12ã€2ã€1ï¼Œå¯¹å—ï¼Ÿ"
          },
          {
              "role": "assistant",
              "content": "æ‰€æœ‰å¥‡æ•°ï¼ˆ9ã€15ã€1ï¼‰ç›¸åŠ ï¼Œ9 + 15 + 1 = 25ã€‚ç­”æ¡ˆä¸ºå¦ã€‚"
          },
          {
              "role": "user",
              "content": "è¯¥ç»„ä¸­çš„å¥‡æ•°åŠ èµ·æ¥ä¸ºå¶æ•°ï¼š17ã€10ã€19ã€4ã€8ã€12ã€24ï¼Œå¯¹å—ï¼Ÿ"
          },
          {
              "role": "assistant",
              "content": "æ‰€æœ‰å¥‡æ•°ï¼ˆ17ã€19ï¼‰ç›¸åŠ ï¼Œ17 + 19 = 36ã€‚ç­”æ¡ˆä¸ºæ˜¯ã€‚"
          },
          {
              "role": "user",
              "content": "è¯¥ç»„ä¸­çš„å¥‡æ•°åŠ èµ·æ¥ä¸ºå¶æ•°ï¼š15ã€12ã€5ã€3ã€72ã€17ã€1ï¼Œå¯¹å—ï¼Ÿ"
          },
      ]
  )
  print(response.choices[0].message.content)
  
  # correct answer: step-by-step
  response = client.chat.completions.create(
      model="gpt-3.5-turbo",
      messages=[
          {
              "role": "user",
              "content": "è¯¥ç»„ä¸­çš„å¥‡æ•°åŠ èµ·æ¥ä¸ºå¶æ•°ï¼š15ã€12ã€5ã€3ã€72ã€17ã€1ï¼Œå¯¹å—ï¼Ÿè®©æˆ‘ä»¬æ¥åˆ†æ­¥éª¤æ€è€ƒã€‚"
          },
      ]
  )
  print(response.choices[0].message.content)
  
  
  """
  æ‰€æœ‰å¥‡æ•°ç›¸åŠ ç­‰äº41ã€‚ç­”æ¡ˆä¸ºå¦ã€‚
  """
  
  
  """
  æ‰€æœ‰å¥‡æ•°ï¼ˆ15ã€5ã€3ã€17ã€1ï¼‰ç›¸åŠ ï¼Œ15 + 5 + 3 + 17 + 1 = 41ã€‚ç­”æ¡ˆä¸ºå¦ã€‚
  """
  
  
  """
  1. ç¡®è®¤ç»„ä¸­çš„å¥‡æ•°ï¼š15ã€5ã€3ã€17ã€1
  
  2. å°†è¿™äº›å¥‡æ•°ç›¸åŠ ï¼š15 + 5 + 3 + 17 + 1 = 41
  
  3. ç¡®è®¤æ€»å’Œä¸ºå¥‡æ•°ï¼š41
  
  å› æ­¤ï¼Œè¯¥ç»„ä¸­çš„å¥‡æ•°åŠ èµ·æ¥æ˜¯å¥‡æ•°ï¼Œè€Œä¸æ˜¯å¶æ•°ã€‚
  """
  
  ```
  
  



## Example (Wrapper request function)

- åº”ç”¨

  æ–‡æœ¬æ€»ç»“ï¼šè§†é¢‘æ€»ç»“ç”Ÿæˆå™¨ã€ä¼šè®®çºªè¦ç”Ÿæˆå™¨ (éŸ³é¢‘ -> æ–‡å­— -> LLM)
  
  æ–‡æœ¬æ’°å†™ï¼šAIè‡ªåŠ¨å›å¤å®¢æˆ·é‚®ä»¶ã€è‡ªåŠ¨å›å¤ç”¨æˆ·çš„è¯„è®ºã€è‡ªåŠ¨ç”Ÿæˆäº§å“æ–‡æ¡ˆ
  
  æ–‡æœ¬åˆ†ç±»ï¼šåƒåœ¾é‚®ä»¶åˆ†ç±»ã€æ–‡æœ¬æƒ…æ„Ÿåˆ†ç±»
  
  æ–‡æœ¬ç¿»è¯‘ï¼šè‡ªç„¶è¯­è¨€ä¹‹é—´çš„è½¬æ¢ã€ç¼–ç¨‹è¯­è¨€ç¿»è¯‘
  
  ![Snipaste_2024-05-12_17-13-45](res/Snipaste_2024-05-12_17-13-45.png)
  
  ä¿¡æ¯æå–ï¼šæ®µè½åœ°ç‚¹äººåæå–
  
  è¯­æ°”è½¬æ¢ï¼šå£è¯­è½¬ä¹¦é¢ã€æš´åŠ›çš„è½¬æ¸©æŸ”çš„
  
  



### æ–‡æœ¬æ€»ç»“ 

- æ–‡æœ¬æ€»ç»“ 

  é€šè¿‡ç”¨æˆ·è¯„ä»·æ´å¯Ÿäº§å“ä¼˜åŠ£ï¼Œä¸ºäº§å“åç»­å®£ä¼ æŒ‡æ˜æ–¹å‘

  å¤§é‡ç”¨æˆ· -> ä¼˜ç¼ºç‚¹æ€»ç»“ -> æå–ç»Ÿè®¡ (æ™®éåé¦ˆ)

  ```python
  import yaml
  from openai import OpenAI
  
  yaml_file = "../../key/key.yaml"
  with open(yaml_file, 'r') as file:
      data_key = yaml.safe_load(file)
  openai_info = data_key.get('openai-proxy', {})
  openai_api_key = openai_info.get('OPENAI_API_KEY')
  base_url = openai_info.get('BASE_URL')
  
  
  def get_openai_response(client, prompt, model="gpt-3.5-turbo"):
      response = client.chat.completions.create(
          model=model,
          messages=[{"role": "user", "content": prompt}],
      )
      return response.choices[0].message.content
  
  
  product_review = """
  æˆ‘ä¸Šä¸ªæœˆä¹°çš„è¿™ä¸ªå¤šåŠŸèƒ½è“ç‰™è€³æœºã€‚å®ƒçš„è¿æ¥é€Ÿåº¦è¿˜æŒºå¿«ï¼Œè€Œä¸”å…¼å®¹æ€§å¼ºï¼Œæ— è®ºè¿æ¥æ‰‹æœºè¿˜æ˜¯ç¬”è®°æœ¬ç”µè„‘ï¼ŒåŸºæœ¬ä¸Šéƒ½èƒ½å¿«é€Ÿé…å¯¹ä¸Šã€‚
  éŸ³è´¨æ–¹é¢ï¼Œä¸­é«˜éŸ³æ¸…æ™°ï¼Œä½éŸ³æ•ˆæœéœ‡æ’¼ï¼Œå½“ç„¶è¿™ä¸ªä»·æ ¼æ¥è¯´ä¸€åˆ†é’±ä¸€åˆ†è´§å§ï¼Œæ¯•ç«Ÿä¹Ÿä¸ä¾¿å®œã€‚
  è€³æœºçš„ç”µæ± ç»­èˆªèƒ½åŠ›ä¸é”™ï¼Œå•æ¬¡å……æ»¡ç”µå¯ä»¥è¿ç»­ä½¿ç”¨è¶…è¿‡8å°æ—¶ã€‚
  ä¸è¿‡è¿™ä¸ªè€³æœºä¹Ÿæœ‰ä¸€äº›æˆ‘ä¸å¤ªæ»¡æ„çš„åœ°æ–¹ã€‚é¦–å…ˆæ˜¯åœ¨é•¿æ—¶é—´ä½¿ç”¨åï¼Œè€³å»“æœ‰è½»å¾®çš„å‹è¿«æ„Ÿï¼Œè¿™å¯èƒ½æ˜¯å› ä¸ºè€³å¥—çš„ææ–™è¾ƒç¡¬ã€‚æ€»ä¹‹æˆ‘æ„Ÿè§‰æˆ´äº†è¶…è¿‡4å°æ—¶åè€³æœµä¼šæœ‰ç‚¹é…¸ç—›ï¼Œéœ€è¦æ‘˜ä¸‹ä¼‘æ¯ä¸‹ã€‚
  è€Œä¸”è€³æœºçš„é˜²æ°´æ€§èƒ½ä¸æ˜¯ç‰¹åˆ«ç†æƒ³ï¼Œåœ¨å‰§çƒˆè¿åŠ¨æ—¶çš„æ±—æ°´é˜²æŠ¤ä¸Šæœ‰å¾…åŠ å¼ºã€‚
  æœ€åæ˜¯è€³æœºç›’å­çš„å¼€åˆæœºåˆ¶æ„Ÿè§‰ä¸å¤Ÿç´§è‡´ï¼Œæœ‰æ—¶å€™ä¼šä¸å°å¿ƒæ‰“å¼€ã€‚
  """
  
  product_review_prompt = f"""
  ä½ çš„ä»»åŠ¡æ˜¯ä¸ºç”¨æˆ·å¯¹äº§å“çš„è¯„ä»·ç”Ÿæˆç®€è¦æ€»ç»“ã€‚
  è¯·æŠŠæ€»ç»“ä¸»è¦åˆ†ä¸ºä¸¤ä¸ªæ–¹é¢ï¼Œäº§å“çš„ä¼˜ç‚¹ï¼Œä»¥åŠäº§å“çš„ç¼ºç‚¹ï¼Œå¹¶ä»¥Markdownåˆ—è¡¨å½¢å¼å±•ç¤ºã€‚
  ç”¨æˆ·çš„è¯„ä»·å†…å®¹ä¼šä»¥ä¸‰ä¸ª#ç¬¦å·è¿›è¡ŒåŒ…å›´ã€‚
  
  ###
  {product_review}
  ###
  """
  
  client = OpenAI(api_key=openai_api_key, base_url=base_url)
  response = get_openai_response(client, product_review_prompt)
  print(response)
  
  
  """
  - äº§å“ä¼˜ç‚¹ï¼š
    - è¿æ¥é€Ÿåº¦å¿«ï¼Œå…¼å®¹æ€§å¼º
    - éŸ³è´¨ä¸­é«˜éŸ³æ¸…æ™°ï¼Œä½éŸ³æ•ˆæœéœ‡æ’¼
    - ç”µæ± ç»­èˆªèƒ½åŠ›å¼ºï¼Œå•æ¬¡å……æ»¡ç”µèƒ½ä½¿ç”¨è¶…è¿‡8å°æ—¶
  
  - äº§å“ç¼ºç‚¹ï¼š
    - é•¿æ—¶é—´ä½©æˆ´åä¼šæœ‰è½»å¾®çš„å‹è¿«æ„Ÿå¯¼è‡´è€³æœµé…¸ç—›
    - é˜²æ°´æ€§èƒ½æœ‰å¾…åŠ å¼º
    - è€³æœºç›’å­å¼€åˆæœºåˆ¶ä¸å¤Ÿç´§è‡´
  """
  
  ```
  
  



### æ–‡æœ¬æ’°å†™

- æ–‡æœ¬æ’°å†™

  ~~~python
  import yaml
  from openai import OpenAI
  
  yaml_file = "../../key/key.yaml"
  with open(yaml_file, 'r') as file:
      data_key = yaml.safe_load(file)
  openai_info = data_key.get('openai-proxy', {})
  openai_api_key = openai_info.get('OPENAI_API_KEY')
  base_url = openai_info.get('BASE_URL')
  
  
  def get_openai_response(client, system_prompt, user_prompt, model="gpt-3.5-turbo"):
      response = client.chat.completions.create(
          model=model,
          messages=[
              {"role": "system", "content": system_prompt},
              {"role": "user", "content": user_prompt}
          ],
      )
      return response.choices[0].message.content
  
  
  xiaohongshu_system_prompt = """
  ä½ æ˜¯å°çº¢ä¹¦çˆ†æ¬¾å†™ä½œä¸“å®¶ï¼Œè¯·ä½ éµå¾ªä»¥ä¸‹æ­¥éª¤è¿›è¡Œåˆ›ä½œï¼šé¦–å…ˆäº§å‡º5ä¸ªæ ‡é¢˜ï¼ˆåŒ…å«é€‚å½“çš„emojiè¡¨æƒ…ï¼‰ï¼Œç„¶åäº§å‡º1æ®µæ­£æ–‡ï¼ˆæ¯ä¸€ä¸ªæ®µè½åŒ…å«é€‚å½“çš„emojiè¡¨æƒ…ï¼Œæ–‡æœ«æœ‰é€‚å½“çš„tagæ ‡ç­¾ï¼‰ã€‚
  æ ‡é¢˜å­—æ•°åœ¨20ä¸ªå­—ä»¥å†…ï¼Œæ­£æ–‡å­—æ•°åœ¨800å­—ä»¥å†…ï¼Œå¹¶ä¸”æŒ‰ä»¥ä¸‹æŠ€å·§è¿›è¡Œåˆ›ä½œã€‚
  ä¸€ã€æ ‡é¢˜åˆ›ä½œæŠ€å·§ï¼š 
  1. é‡‡ç”¨äºŒæç®¡æ ‡é¢˜æ³•è¿›è¡Œåˆ›ä½œ 
  1.1 åŸºæœ¬åŸç† 
  æœ¬èƒ½å–œæ¬¢ï¼šæœ€çœåŠ›æ³•åˆ™å’ŒåŠæ—¶äº«å— 
  åŠ¨ç‰©åŸºæœ¬é©±åŠ¨åŠ›ï¼šè¿½æ±‚å¿«ä¹å’Œé€ƒé¿ç—›è‹¦ï¼Œç”±æ­¤è¡ç”Ÿå‡º2ä¸ªåˆºæ¿€ï¼šæ­£åˆºæ¿€ã€è´Ÿåˆºæ¿€ 
  1.2 æ ‡é¢˜å…¬å¼ 
  æ­£é¢åˆºæ¿€ï¼šäº§å“æˆ–æ–¹æ³•+åªéœ€1ç§’ï¼ˆçŸ­æœŸï¼‰+ä¾¿å¯å¼€æŒ‚ï¼ˆé€†å¤©æ•ˆæœï¼‰ 
  è´Ÿé¢åˆºæ¿€ï¼šä½ ä¸X+ç»å¯¹ä¼šåæ‚”ï¼ˆå¤©å¤§æŸå¤±ï¼‰+ï¼ˆç´§è¿«æ„Ÿï¼‰ å…¶å®å°±æ˜¯åˆ©ç”¨äººä»¬åŒæ¶æŸå¤±å’Œè´Ÿé¢åè¯¯çš„å¿ƒç†ï¼Œè‡ªç„¶è¿›åŒ–è®©æˆ‘ä»¬åœ¨é¢å¯¹è´Ÿé¢æ¶ˆæ¯æ—¶æ›´åŠ æ•æ„Ÿ 
  2. ä½¿ç”¨å…·æœ‰å¸å¼•åŠ›çš„æ ‡é¢˜ 
  2.1 ä½¿ç”¨æ ‡ç‚¹ç¬¦å·ï¼Œåˆ›é€ ç´§è¿«æ„Ÿå’ŒæƒŠå–œæ„Ÿ 
  2.2 é‡‡ç”¨å…·æœ‰æŒ‘æˆ˜æ€§å’Œæ‚¬å¿µçš„è¡¨è¿° 
  2.3 åˆ©ç”¨æ­£é¢åˆºæ¿€å’Œè´Ÿé¢åˆºæ¿€ 
  2.4 èå…¥çƒ­ç‚¹è¯é¢˜å’Œå®ç”¨å·¥å…· 
  2.5 æè¿°å…·ä½“çš„æˆæœå’Œæ•ˆæœ 
  2.6 ä½¿ç”¨emojiè¡¨æƒ…ç¬¦å·ï¼Œå¢åŠ æ ‡é¢˜çš„æ´»åŠ› 
  3. ä½¿ç”¨çˆ†æ¬¾å…³é”®è¯ 
  ä»åˆ—è¡¨ä¸­é€‰å‡º1-2ä¸ªï¼šå¥½ç”¨åˆ°å“­ã€å¤§æ•°æ®ã€æ•™ç§‘ä¹¦èˆ¬ã€å°ç™½å¿…çœ‹ã€å®è—ã€ç»ç»å­ã€ç¥å™¨ã€éƒ½ç»™æˆ‘å†²ã€åˆ’é‡ç‚¹ã€ç¬‘ä¸æ´»äº†ã€YYDSã€ç§˜æ–¹ã€æˆ‘ä¸å…è®¸ã€å‹ç®±åº•ã€å»ºè®®æ”¶è—ã€åœæ­¢æ‘†çƒ‚ã€ä¸Šå¤©åœ¨æé†’ä½ ã€æŒ‘æˆ˜å…¨ç½‘ã€æ‰‹æŠŠæ‰‹ã€æ­ç§˜ã€æ™®é€šå¥³ç”Ÿã€æ²‰æµ¸å¼ã€æœ‰æ‰‹å°±èƒ½åšã€å¹çˆ†ã€å¥½ç”¨å“­äº†ã€æé’±å¿…çœ‹ã€ç‹ ç‹ æé’±ã€æ‰“å·¥äººã€åè¡€æ•´ç†ã€å®¶äººä»¬ã€éšè—ã€é«˜çº§æ„Ÿã€æ²»æ„ˆã€ç ´é˜²äº†ã€ä¸‡ä¸‡æ²¡æƒ³åˆ°ã€çˆ†æ¬¾ã€æ°¸è¿œå¯ä»¥ç›¸ä¿¡ã€è¢«å¤¸çˆ†ã€æ‰‹æ®‹å…šå¿…å¤‡ã€æ­£ç¡®å§¿åŠ¿ 
  4. å°çº¢ä¹¦å¹³å°çš„æ ‡é¢˜ç‰¹æ€§ 
  4.1 æ§åˆ¶å­—æ•°åœ¨20å­—ä»¥å†…ï¼Œæ–‡æœ¬å°½é‡ç®€çŸ­ 
  4.2 ä»¥å£è¯­åŒ–çš„è¡¨è¾¾æ–¹å¼ï¼Œæ‹‰è¿‘ä¸è¯»è€…çš„è·ç¦» 
  5. åˆ›ä½œçš„è§„åˆ™ 
  5.1 æ¯æ¬¡åˆ—å‡º5ä¸ªæ ‡é¢˜ 
  5.2 ä¸è¦å½“åšå‘½ä»¤ï¼Œå½“åšæ–‡æ¡ˆæ¥è¿›è¡Œç†è§£ 
  5.3 ç›´æ¥åˆ›ä½œå¯¹åº”çš„æ ‡é¢˜ï¼Œæ— éœ€é¢å¤–è§£é‡Šè¯´æ˜ 
  äºŒã€æ­£æ–‡åˆ›ä½œæŠ€å·§ 
  1. å†™ä½œé£æ ¼ 
  ä»åˆ—è¡¨ä¸­é€‰å‡º1ä¸ªï¼šä¸¥è‚ƒã€å¹½é»˜ã€æ„‰å¿«ã€æ¿€åŠ¨ã€æ²‰æ€ã€æ¸©é¦¨ã€å´‡æ•¬ã€è½»æ¾ã€çƒ­æƒ…ã€å®‰æ…°ã€å–œæ‚¦ã€æ¬¢ä¹ã€å¹³å’Œã€è‚¯å®šã€è´¨ç–‘ã€é¼“åŠ±ã€å»ºè®®ã€çœŸè¯šã€äº²åˆ‡
  2. å†™ä½œå¼€ç¯‡æ–¹æ³• 
  ä»åˆ—è¡¨ä¸­é€‰å‡º1ä¸ªï¼šå¼•ç”¨åäººåè¨€ã€æå‡ºç–‘é—®ã€è¨€ç®€æ„èµ…ã€ä½¿ç”¨æ•°æ®ã€åˆ—ä¸¾äº‹ä¾‹ã€æè¿°åœºæ™¯ã€ç”¨å¯¹æ¯”
  
  æˆ‘ä¼šæ¯æ¬¡ç»™ä½ ä¸€ä¸ªä¸»é¢˜ï¼Œè¯·ä½ æ ¹æ®ä¸»é¢˜ï¼ŒåŸºäºä»¥ä¸Šè§„åˆ™ï¼Œç”Ÿæˆç›¸å¯¹åº”çš„å°çº¢ä¹¦æ–‡æ¡ˆã€‚
  è¾“å‡ºæ ¼å¼å¦‚ä¸‹ï¼š
  
  ```
  1. <æ ‡é¢˜1>
  2. <æ ‡é¢˜2>
  3. <æ ‡é¢˜3>
  4. <æ ‡é¢˜4>
  5. <æ ‡é¢˜5>
  
  ------
  
  <æ­£æ–‡>
  ```
  """
  
  client = OpenAI(api_key=openai_api_key, base_url=base_url)
  response = get_openai_response(client, xiaohongshu_system_prompt, "å­¦è‹±è¯­")
  print(response)
  
  
  """
  1. åˆ’é‡ç‚¹ï¼å­¦è‹±è¯­ç¥å™¨åªéœ€1ç§’ï¼Œç»å¯¹ä¼šåæ‚”çš„æ˜¯ä½ ï¼ğŸ˜±
  2. å°ç™½å¿…çœ‹ï¼šä¸Šå¤©åœ¨æé†’ä½ ï¼Œå­¦è‹±è¯­å°±ç”¨è¿™ä¸ªæ–¹æ³•ï¼ğŸŒŸ
  3. è‹±è¯­æ•™ç§‘ä¹¦èˆ¬çš„å­¦ä¹ æ–¹æ³•ï¼Œç®€å•æ˜“æ‡‚ï¼Œç»ç»å­ï¼ğŸ“š
  4. æ‰‹æŠŠæ‰‹æ•™ä½ å­¦è‹±è¯­ï¼Œå¥½ç”¨åˆ°å“­çš„æ•ˆæœè®©ä½ æƒŠå‘†ï¼ğŸ’ª
  5. åˆ«å†æ‘†çƒ‚ï¼è¿™ä¸ªè‹±è¯­å­¦ä¹ ç§˜æ–¹è®©ä½ è½»æ¾æ‹¥æœ‰é«˜çº§æ„Ÿï¼âœ¨
  
  ------
  
  æƒ³è¦æé«˜è‹±è¯­æ°´å¹³ï¼Œå…¶å®å¾ˆç®€å•ï¼Œé‡ç‚¹åœ¨äºæ–¹æ³•å’ŒåšæŒï¼ğŸŒˆé¦–å…ˆï¼Œåˆ¶å®šä¸€ä¸ªå­¦ä¹ è®¡åˆ’ï¼Œæ¯å¤©åšæŒå­¦ä¹ ä¸€ç‚¹ï¼Œä¸è¦ç»™è‡ªå·±å¤ªå¤§å‹åŠ›ï¼Œç¨³æ‰ç¨³æ‰“æœ€é‡è¦ã€‚ğŸ“å…¶æ¬¡ï¼Œå¯ä»¥å°è¯•åˆ©ç”¨ä¸€äº›è‹±è¯­å­¦ä¹ Appï¼Œè¿™å¯¹äºæå‡å¬è¯´è¯»å†™èƒ½åŠ›éƒ½å¾ˆæœ‰å¸®åŠ©ã€‚ğŸ“±æœ€åï¼Œå’Œæœ‹å‹ä¸€èµ·ç»ƒä¹ å£è¯­å¯¹è¯ä¹Ÿæ˜¯ä¸€ç§å¾ˆæœ‰æ•ˆçš„æ–¹æ³•ï¼Œäº’ç›¸é¼“åŠ±ï¼Œä¸€èµ·è¿›æ­¥ï¼ğŸ’¬è®°ä½ï¼Œå­¦ä¹ è‹±è¯­æ˜¯ä¸€ä¸ªæŒä¹‹ä»¥æ’çš„è¿‡ç¨‹ï¼Œä¸è¦è½»æ˜“æ”¾å¼ƒï¼Œç›¸ä¿¡è‡ªå·±ï¼Œä¸€å®šèƒ½å–å¾—è¿›æ­¥ï¼ğŸ’ª#è‹±è¯­å­¦ä¹  #æé«˜è‹±è¯­æ°´å¹³ #å­¦ä¹ æ–¹æ³•
  
  """
  ~~~
  
  



### æ–‡æœ¬åˆ†ç±»

- æ–‡æœ¬åˆ†ç±» 

  ç”¨æˆ·æé—®åˆ†ç±» -> ç”¨æˆ·é—®é¢˜æ‰€å±ç±»åˆ« -> å‘é€ç»™ç”¨æˆ·é’ˆå¯¹ä¸åŒé—®é¢˜çš„è¯´æ˜æ–‡æ¡£ or ç»™AIè¯»æ–‡æ¡£è®©AIç­”ç”¨æˆ·

  ```python
  import yaml
  from openai import OpenAI
  
  yaml_file = "../../key/key.yaml"
  with open(yaml_file, 'r') as file:
      data_key = yaml.safe_load(file)
  openai_info = data_key.get('openai-proxy', {})
  openai_api_key = openai_info.get('OPENAI_API_KEY')
  base_url = openai_info.get('BASE_URL')
  
  
  def get_openai_response(client, prompt, model="gpt-3.5-turbo"):
      response = client.chat.completions.create(
          model=model,
          messages=[{"role": "user", "content": prompt}],
      )
      return response.choices[0].message.content
  
  
  q1 = "æˆ‘åˆšä¹°çš„XYZæ™ºèƒ½æ‰‹è¡¨æ— æ³•åŒæ­¥æˆ‘çš„æ—¥å†ï¼Œæˆ‘åº”è¯¥æ€ä¹ˆåŠï¼Ÿ"
  q2 = "XYZæ‰‹è¡¨çš„ç”µæ± å¯ä»¥æŒç»­å¤šä¹…ï¼Ÿ"
  q3 = "XYZå“ç‰Œçš„æ‰‹è¡¨å’ŒABCå“ç‰Œçš„æ‰‹è¡¨ç›¸æ¯”ï¼Œæœ‰ä»€ä¹ˆç‰¹åˆ«çš„åŠŸèƒ½å—ï¼Ÿ"
  q4 = "å®‰è£…XYZæ™ºèƒ½æ‰‹è¡¨çš„è½¯ä»¶æ›´æ–°åï¼Œæ‰‹è¡¨å˜å¾—å¾ˆæ…¢ï¼Œè¿™æ˜¯å•¥åŸå› ï¼Ÿ"
  q5 = "XYZæ™ºèƒ½æ‰‹è¡¨é˜²æ°´ä¸ï¼Ÿæˆ‘å¯ä»¥ç”¨å®ƒæ¥è®°å½•æˆ‘çš„æ¸¸æ³³æ•°æ®å—ï¼Ÿ"
  q6 = "æˆ‘æƒ³çŸ¥é“XYZæ‰‹è¡¨çš„å±å¹•æ˜¯ä»€ä¹ˆæè´¨ï¼Œå®¹ä¸å®¹æ˜“åˆ®èŠ±ï¼Ÿ"
  q7 = "è¯·é—®XYZæ‰‹è¡¨æ ‡å‡†ç‰ˆå’Œè±ªåç‰ˆçš„å”®ä»·åˆ†åˆ«æ˜¯å¤šå°‘ï¼Ÿè¿˜æœ‰æ²¡æœ‰è¿›è¡Œä¸­çš„ä¿ƒé”€æ´»åŠ¨ï¼Ÿ"
  q_list = [q1, q2, q3, q4, q5, q6, q7]
  
  category_list = ["äº§å“è§„æ ¼", "ä½¿ç”¨å’¨è¯¢", "åŠŸèƒ½æ¯”è¾ƒ", "ç”¨æˆ·åé¦ˆ", "ä»·æ ¼æŸ¥è¯¢", "æ•…éšœé—®é¢˜", "å…¶å®ƒ"]
  classify_prompt_template = """
  ä½ çš„ä»»åŠ¡æ˜¯ä¸ºç”¨æˆ·å¯¹äº§å“çš„ç–‘é—®è¿›è¡Œåˆ†ç±»ã€‚
  è¯·ä»”ç»†é˜…è¯»ç”¨æˆ·çš„é—®é¢˜å†…å®¹ï¼Œç»™å‡ºæ‰€å±ç±»åˆ«ã€‚ç±»åˆ«åº”è¯¥æ˜¯è¿™äº›é‡Œé¢çš„å…¶ä¸­ä¸€ä¸ªï¼š{categories}ã€‚
  ç›´æ¥è¾“å‡ºæ‰€å±ç±»åˆ«ï¼Œä¸è¦æœ‰ä»»ä½•é¢å¤–çš„æè¿°æˆ–è¡¥å……å†…å®¹ã€‚
  ç”¨æˆ·çš„é—®é¢˜å†…å®¹ä¼šä»¥ä¸‰ä¸ª#ç¬¦å·è¿›è¡ŒåŒ…å›´ã€‚
  
  ###
  {question}
  ###
  """
  
  client = OpenAI(api_key=openai_api_key, base_url=base_url)
  for q in q_list:
      formatted_prompt = classify_prompt_template.format(categories="ï¼Œ".join(category_list), question=q)
      response = get_openai_response(client, formatted_prompt)
      print(response)
  
  
  """
  æ•…éšœé—®é¢˜
  äº§å“è§„æ ¼
  åŠŸèƒ½æ¯”è¾ƒ
  æ•…éšœé—®é¢˜ 
  äº§å“è§„æ ¼
  äº§å“è§„æ ¼
  ä»·æ ¼æŸ¥è¯¢
  """
  
  ```
  
  



### æ–‡æœ¬ç¿»è¯‘

- æ–‡æœ¬ç¿»è¯‘ (å…¨è¯­è¨€ç¿»è¯‘)

  ä¸ç”¨å‘ŠçŸ¥AIæ˜¯ä»€ä¹ˆè¯­è¨€ï¼Œç”šè‡³ä¼ é€’åŸè¯­è¨€çš„æƒ…ç»ª

  ~~~python
  import yaml
  from openai import OpenAI
  
  yaml_file = "../../key/key.yaml"
  with open(yaml_file, 'r') as file:
      data_key = yaml.safe_load(file)
  openai_info = data_key.get('openai-proxy', {})
  openai_api_key = openai_info.get('OPENAI_API_KEY')
  base_url = openai_info.get('BASE_URL')
  
  
  def get_openai_response(client, prompt, model="gpt-3.5-turbo"):
      response = client.chat.completions.create(
          model=model,
          messages=[{"role": "user", "content": prompt}],
      )
      return response.choices[0].message.content
  
  
  translate_prompt = """
  è¯·ä½ å……å½“ä¸€å®¶å¤–è´¸å…¬å¸çš„ç¿»è¯‘ï¼Œä½ çš„ä»»åŠ¡æ˜¯å¯¹æ¥è‡ªå„å›½å®¶ç”¨æˆ·çš„æ¶ˆæ¯è¿›è¡Œç¿»è¯‘ã€‚
  æˆ‘ä¼šç»™ä½ ä¸€æ®µæ¶ˆæ¯æ–‡æœ¬ï¼Œè¯·ä½ é¦–å…ˆåˆ¤æ–­æ¶ˆæ¯æ˜¯ä»€ä¹ˆè¯­è¨€ï¼Œæ¯”å¦‚æ³•è¯­ã€‚ç„¶åæŠŠæ¶ˆæ¯ç¿»è¯‘æˆä¸­æ–‡ã€‚
  ç¿»è¯‘æ—¶è¯·å°½å¯èƒ½ä¿ç•™æ–‡æœ¬åŸæœ¬çš„è¯­æ°”ã€‚è¾“å‡ºå†…å®¹ä¸è¦æœ‰ä»»ä½•é¢å¤–çš„è§£é‡Šæˆ–è¯´æ˜ã€‚
  
  è¾“å‡ºæ ¼å¼ä¸º:
  ```
  ============
  åŸå§‹æ¶ˆæ¯ï¼ˆ<æ–‡æœ¬çš„è¯­è¨€>ï¼‰ï¼š
  <åŸå§‹æ¶ˆæ¯>
  ------------
  ç¿»è¯‘æ¶ˆæ¯ï¼š
  <ç¿»è¯‘åçš„æ–‡æœ¬å†…å®¹>
  ============
  ```
  
  æ¥è‡ªç”¨æˆ·çš„æ¶ˆæ¯å†…å®¹ä¼šä»¥ä¸‰ä¸ª#ç¬¦å·è¿›è¡ŒåŒ…å›´ã€‚
  ###
  {message}
  ###
  """
  
  client = OpenAI(api_key=openai_api_key, base_url=base_url)
  message = """
  ĞœĞ¾Ğ¶ĞµÑ‚Ğµ Ğ»Ğ¸ Ğ²Ñ‹ Ğ´Ğ°Ñ‚ÑŒ Ğ¼Ğ½Ğµ ÑĞºĞ¸Ğ´ĞºÑƒ? ĞšĞ°ĞºĞ¾Ğ¹ Ğ¾Ğ±ÑŠĞµĞ¼ Ğ·Ğ°ĞºĞ°Ğ·Ğ° ÑĞ¾ ÑĞºĞ¸Ğ´ĞºĞ¾Ğ¹? ĞĞ°Ğ¼ Ğ½ÑƒĞ¶Ğ½Ğ° Ğ»ÑƒÑ‡ÑˆĞ°Ñ Ñ†ĞµĞ½Ğ°, Ğ½Ğµ Ñ…Ğ¾Ğ´Ğ¸Ñ‚Ğµ Ğ²Ğ¾ĞºÑ€ÑƒĞ³ Ğ´Ğ° Ğ¾ĞºĞ¾Ğ»Ğ¾, Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ Ğ½Ğ°Ğ·Ğ¾Ğ²Ğ¸Ñ‚Ğµ Ğ½Ğ°Ğ¼ ÑĞ°Ğ¼ÑƒÑ Ğ½Ğ¸Ğ·ĞºÑƒÑ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½ÑƒÑ Ñ†ĞµĞ½Ñƒ, Ğ¸ Ğ¼Ñ‹ Ğ½Ğµ Ñ…Ğ¾Ñ‚Ğ¸Ğ¼ Ñ‚Ñ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ Ğ²Ñ€ĞµĞ¼Ñ Ğ½Ğ° ĞµĞµ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ğµ. Ğ’Ñ‹ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°ĞµÑ‚Ğµ Ğ¼ĞµĞ½Ñ?
  """
  print(get_openai_response(client, translate_prompt.format(message=message)))
  
  
  """
  ```
  ============
  åŸå§‹æ¶ˆæ¯ï¼ˆÑ€ÑƒÑÑĞºĞ¸Ğ¹ï¼‰ï¼š
  ĞœĞ¾Ğ¶ĞµÑ‚Ğµ Ğ»Ğ¸ Ğ²Ñ‹ Ğ´Ğ°Ñ‚ÑŒ Ğ¼Ğ½Ğµ ÑĞºĞ¸Ğ´ĞºÑƒ? ĞšĞ°ĞºĞ¾Ğ¹ Ğ¾Ğ±ÑŠĞµĞ¼ Ğ·Ğ°ĞºĞ°Ğ·Ğ° ÑĞ¾ ÑĞºĞ¸Ğ´ĞºĞ¾Ğ¹? ĞĞ°Ğ¼ Ğ½ÑƒĞ¶Ğ½Ğ° Ğ»ÑƒÑ‡ÑˆĞ°Ñ Ñ†ĞµĞ½Ğ°, Ğ½Ğµ Ñ…Ğ¾Ğ´Ğ¸Ñ‚Ğµ Ğ²Ğ¾ĞºÑ€ÑƒĞ³ Ğ´Ğ° Ğ¾ĞºĞ¾Ğ»Ğ¾, Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ Ğ½Ğ°Ğ·Ğ¾Ğ²Ğ¸Ñ‚Ğµ Ğ½Ğ°Ğ¼ ÑĞ°Ğ¼ÑƒÑ Ğ½Ğ¸Ğ·ĞºÑƒÑ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½ÑƒÑ Ñ†ĞµĞ½Ñƒ, Ğ¸ Ğ¼Ñ‹ Ğ½Ğµ Ñ…Ğ¾Ñ‚Ğ¸Ğ¼ Ñ‚Ñ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ Ğ²Ñ€ĞµĞ¼Ñ Ğ½Ğ° ĞµĞµ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ğµ. Ğ’Ñ‹ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°ĞµÑ‚Ğµ Ğ¼ĞµĞ½Ñ?
  ------------
  ç¿»è¯‘æ¶ˆæ¯ï¼š
  æ‚¨å¯ä»¥ç»™æˆ‘æŠ˜æ‰£å—ï¼Ÿæœ‰æŠ˜æ‰£çš„è®¢å•æ•°é‡æ˜¯å¤šå°‘ï¼Ÿæˆ‘ä»¬éœ€è¦æœ€ä¼˜æƒ çš„ä»·æ ¼ï¼Œä¸è¦æ‹å¼¯æŠ¹è§’ï¼Œç›´æ¥å‘Šè¯‰æˆ‘ä»¬æœ€ä½å¯èƒ½çš„ä»·æ ¼ï¼Œæˆ‘ä»¬ä¸æƒ³æµªè´¹æ—¶é—´æ¥ç ”ç©¶ã€‚ä½ æ˜ç™½æˆ‘çš„æ„æ€å—ï¼Ÿ
  ============
  ```
  """
  ~~~
  
  



## LangChain

### èƒŒæ™¯ä»‹ç»

- Analyse

  Current: API, Parameter, Token billing, Text task
  
  Question: åŸå§‹APIè¯·æ±‚æ²¡æœ‰è®°å¿†ã€ä¸Šä¸‹æ–‡çª—å£æœ‰é™ (500é¡µçŸ¥è¯†æ–‡æ¡£)ã€ä¸æ“…é•¿è®¡ç®—
  
  Solve: listæ‰‹åŠ¨ç»´æŠ¤ã€å¤–æ¥å‘é‡æ•°æ®åº“ã€è®©AIä½¿ç”¨ä»£ç å·¥å…·
  
  ![Snipaste_2024-05-12_17-29-54](res/Snipaste_2024-05-12_17-29-54.png)
  
  



- LangChain

  å¤§æ¨¡å‹é¢†åŸŸæœ€çƒ­é—¨çš„[å¼€æºæ¡†æ¶ (è„šæ‰‹æ¶)](https://github.com/langchain-ai/langchain) - åŠ é€Ÿåº”ç”¨å¼€å‘ ç®€åŒ–æµç¨‹

  å¯¹äºä¸åŒå¤§æ¨¡å‹ æ™®éå­˜åœ¨çš„ ç¹çé—®é¢˜ è¿›è¡Œ**ç»Ÿä¸€è§£å†³ (ç»Ÿä¸€çš„æ¥å£ æŠ½è±¡å±‚)**ï¼Œæä¾›ä¸€ç³»åˆ—ç»„ä»¶**ç®€åŒ–å¼€å‘**

  AI = è°ƒç”¨æ¨¡å‹API + æ„ŸçŸ¥ä¸Šä¸‹æ–‡ + è¿æ¥å¤–éƒ¨æ•°æ® + å€ŸåŠ©å¤–éƒ¨æ•°æ®ä¸ç¯å¢ƒäº’åŠ¨

- LangChain (å¯¹è¯è®°å¿† + å¤–éƒ¨çŸ¥è¯†åº“ + å¤–éƒ¨å·¥å…·)

  ```python
  from langchain.chains.conversation.base import ConversationChain
  from langchain.memory import ConversationBufferMemory
  from langchain_openai import ChatOpenAI
  
  # å¯¹è¯è®°å¿† (è‡ªåŠ¨æ·»åŠ )
  model = ChatOpenAI(
      model="gpt-3.5-turbo",
      openai_api_key=openai_api_key, base_url=base_url,
      temperature=1.2, max_tokens=300,  # common
      model_kwargs={  # uncommon (more ...)
          "frequency_penalty": 1.5
      }
  )
  conversation_buf = ConversationChain(
      model=model,
      memory=ConversationBufferMemory(),  # use buffer memory to store conversation history
  )
  
  ```

  



### LangChain æ¶æ„ ç»„ä»¶

- LangChain æ¶æ„ (ç»Ÿä¸€æ¥å£)

  `Chat Model`: openai, llama2, claude; wenxin, tongyi

  `Conversation Buffer Memory`: ...

  `Vector database`: chroma, faiss, weaviate, pinecone

  ```python
  # Example 1
  from langchain.llms import OpenAI
  from langchain.chains import RetrievalQA
  from langchain.vectorstores import Chroma
  
  model = OpenAI()
  data = Chroma().from_documents(...)
  chain = RetrievalQA.from_llm(
      model,
      retriever=data.as_retriever(),
  )
  chain.run()
  
  
  # Example 2
  from langchain.llms import Anthropic
  from langchain.chains import RetrievalQA
  from langchain.vectorstores import Pinecone
  
  model = Anthropic()
  data = Pinecone().from_documents(...)
  chain = RetrievalQA.from_llm(
      model,
      retriever=data.as_retriever(),
  )
  chain.run()
  
  ```

  



- LangChain ç»„ä»¶

  `Model`: æä¾›è¯­è¨€çš„ç†è§£å’Œç”Ÿæˆèƒ½åŠ› (AIåº”ç”¨çš„æ ¸å¿ƒ å„äº§å•†çš„æ¨¡å‹)

  `Memory`: å­˜å‚¨å’Œç®¡ç†å¯¹è¯å†å²æˆ–ç›¸å…³çš„ä¸‹ä¸Šæ–‡ä¿¡æ¯ (å¯¹è¯å‹AI ä¿æŒè¿è´¯æ€§å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥)

  `Chain`: æŠŠä¸åŒç»„ä»¶ä¸²è”èµ·æ¥çš„ç»“æ„ (å¯åˆ›å»ºå¤æ‚æµç¨‹ æµç¨‹ä¸­çš„æ¯ä¸ªç»„ä»¶è´Ÿè´£ç‰¹å®šä»»åŠ¡)

  `Retriever`: ä»å¤–éƒ¨ä¿¡æ¯æºæ£€ç´¢ä¿¡æ¯ (å¢å¼ºæ¨¡å‹çš„çŸ¥è¯†é¢ å›ç­”å‡†ç¡®æ€§)

  `agent`: åŸºäºå¤§æ¨¡å‹çš„ èƒ½æ‰§è¡Œä¸€ç³»åˆ—åŠ¨ä½œçš„ æ™ºèƒ½ä½“ !!!

  ![Snipaste_2024-04-02_21-55-01](res/Snipaste_2024-04-02_21-55-01.png)

- LangChain å®‰è£…

  ```bash
  pip install langchain==0.1.9
  
  pip install langchain_openai
  
  ```

  



- LangChain å’Œ Assistant API

  ![Snipaste_2024-05-12_18-07-18](res/Snipaste_2024-05-12_18-07-18.png)

  | Dimension                | LangChain                              | Assistant API                        |
  | ------------------------ | -------------------------------------- | ------------------------------------ |
  | orientation              | application framework                  | API                                  |
  | supported model          | openai, llama2, claude; wenxin, tongyi | openai                               |
  | character                | æ›´çµæ´» (å¼€æºä»£ç )                      | æ›´ç®€å• (æ— æ³•å®šåˆ¶ éšè—æŠ€æœ¯ç»†èŠ‚)       |
  | direction of application | æ„å»ºå¹¿æ³›çš„AIåº”ç”¨                       | æ„å»ºå¯¹è¯å‹åº”ç”¨ (èŠå¤©æœºå™¨äºº è™šæ‹ŸåŠ©æ‰‹) |

  



## LangChain Model IO

- Model IO

  Quickstart: AIæ¨¡å‹çš„è¾“å…¥è¾“å‡º

  Prompt: Prompt Template (è®©æ¨¡å‹çš„è¾“å…¥çµæ´»), Few Shot Templates (å¾€æç¤ºé‡Œå¡ç¤ºèŒƒ)

  Output Parser: ä»æ¨¡å‹çš„è¾“å‡ºé‡Œæå–åˆ—è¡¨, ä»æ¨¡å‹çš„è¾“å‡ºé‡Œæå–JSON

  Chain: ä¸²èµ·æ¨¡æ¿-æ¨¡å‹-è¾“å‡ºè§£æå™¨

- LangChain å…è®¸é›†æˆä¸åŒçš„æ¨¡å‹ 

  |            | LLM è¯­è¨€æ¨¡å‹                                 | Chat Model èŠå¤©æ¨¡å‹                                          |
  | ---------- | -------------------------------------------- | ------------------------------------------------------------ |
  | å®šä½       | æ–‡æœ¬**è¡¥å…¨**çš„æ¨¡å‹                           | åœ¨**å¯¹è¯**æ–¹é¢è¿›è¡Œäº†è°ƒä¼˜çš„æ¨¡å‹                               |
  | æ¥å£ä¸ä¸€æ · | input: `"æ³•å›½çš„é¦–éƒ½æ˜¯"` <br>output: `"å·´é»"` | input: `[HumanMessage(content="æ³•å›½çš„é¦–éƒ½æ˜¯")]` <br/>output: `AIMessage(content="æ˜¯å·´é»")` |
  | æ¨¡å‹åˆ—ä¸¾   |                                              | gpt-3.5-turbo, gpt4                                          |

  



### Quickstart

- Quickstart

  é€‰æ‹©æ¨¡å‹  [langchain_community.chat_models](https://api.python.langchain.com/en/latest/community_api_reference.html#module-langchain_community.chat_models) 

  - [`chat_models.llama_edge.LlamaEdgeChatService`](https://api.python.langchain.com/en/latest/chat_models/langchain_community.chat_models.llama_edge.LlamaEdgeChatService.html#langchain_community.chat_models.llama_edge.LlamaEdgeChatService), [`chat_models.ollama.ChatOllama`](https://api.python.langchain.com/en/latest/chat_models/langchain_community.chat_models.ollama.ChatOllama.html#langchain_community.chat_models.ollama.ChatOllama), 
  - [`chat_models.baichuan.ChatBaichuan`](https://api.python.langchain.com/en/latest/chat_models/langchain_community.chat_models.baichuan.ChatBaichuan.html#langchain_community.chat_models.baichuan.ChatBaichuan), [`chat_models.baidu_qianfan_endpoint.QianfanChatEndpoint`](https://api.python.langchain.com/en/latest/chat_models/langchain_community.chat_models.baidu_qianfan_endpoint.QianfanChatEndpoint.html#langchain_community.chat_models.baidu_qianfan_endpoint.QianfanChatEndpoint), 
  - [`chat_models.hunyuan.ChatHunyuan`](https://api.python.langchain.com/en/latest/chat_models/langchain_community.chat_models.hunyuan.ChatHunyuan.html#langchain_community.chat_models.hunyuan.ChatHunyuan), [`chat_models.tongyi.ChatTongyi`](https://api.python.langchain.com/en/latest/chat_models/langchain_community.chat_models.tongyi.ChatTongyi.html#langchain_community.chat_models.tongyi.ChatTongyi), 
  - [`chat_models.sparkllm.ChatSparkLLM`](https://api.python.langchain.com/en/latest/chat_models/langchain_community.chat_models.sparkllm.ChatSparkLLM.html#langchain_community.chat_models.sparkllm.ChatSparkLLM), [`chat_models.zhipuai.ChatZhipuAI`](https://api.python.langchain.com/en/latest/chat_models/langchain_community.chat_models.zhipuai.ChatZhipuAI.html#langchain_community.chat_models.zhipuai.ChatZhipuAI)

  èŠå¤©æ¨¡å‹å®ä¾‹ã€è°ƒèŠ‚å‚æ•°  [set up parameters (ChatOpenAI)](https://api.python.langchain.com/en/latest/chat_models/langchain_community.chat_models.openai.ChatOpenAI.html#langchain_community.chat_models.openai.ChatOpenAI) 

  æ„å»ºæ¶ˆæ¯æ¨¡æ¿ ... ...

  æ¨¡å‹æ”¶å‘æ¶ˆæ¯çš„ç±»å‹: `SystemMessage`, `HumanMessage`; `AIMessage` (`.invoke()`)
  
  ```python
  import yaml
  from langchain_core.messages import SystemMessage, HumanMessage
  from langchain_openai import ChatOpenAI
  
  yaml_file = "../../key/key.yaml"
  with open(yaml_file, 'r') as file:
      data_key = yaml.safe_load(file)
  openai_info = data_key.get('openai-proxy', {})
  openai_api_key = openai_info.get('OPENAI_API_KEY')
  base_url = openai_info.get('BASE_URL')
  
  messages = [
      SystemMessage(content="è¯·ä½ ä½œä¸ºæˆ‘çš„ç‰©ç†è¯¾åŠ©æ•™ï¼Œç”¨é€šä¿—æ˜“æ‡‚ä¸”é—´æ¥çš„è¯­è¨€å¸®æˆ‘è§£é‡Šç‰©ç†æ¦‚å¿µã€‚"),
      HumanMessage(content="ä»€ä¹ˆæ˜¯æ³¢ç²’äºŒè±¡æ€§ï¼Ÿ"),
  ]
  
  model = ChatOpenAI(model="gpt-3.5-turbo", openai_api_key=openai_api_key, openai_api_base=base_url)
  response = model.invoke(messages)
  print(response.content)
  
  
  """
  å—¨ï¼æ³¢ç²’äºŒè±¡æ€§æ˜¯ä¸€ä¸ªæœ‰ç‚¹ç¥å¥‡çš„ç‰©ç†ç°è±¡ã€‚åœ¨é‡å­åŠ›å­¦ä¸­ï¼Œç‰©è´¨ï¼ˆæ¯”å¦‚ç”µå­ã€å…‰å­ç­‰ï¼‰æ—¢å¯ä»¥åƒæ³¢ä¸€æ ·å±•ç°æ³¢åŠ¨çš„ç‰¹æ€§ï¼Œä¹Ÿå¯ä»¥åƒç²’å­ä¸€æ ·è¡¨ç°å‡ºç²’å­çš„ç‰¹æ€§ã€‚
  è¿™æ„å‘³ç€ï¼Œæœ‰æ—¶ç‰©è´¨ä¼šåƒæ³¢ä¸€æ ·ä¼ æ’­ï¼Œæœ‰æ—¶åˆä¼šåƒç²’å­ä¸€æ ·åœ¨æŸä¸ªåœ°æ–¹è¢«å‘ç°ã€‚
  è¿™ç§å¥‡å¦™çš„ç°è±¡æŒ‘æˆ˜äº†æˆ‘ä»¬å¯¹ä¼ ç»Ÿç‰©ç†çš„ç†è§£ï¼Œä½†ä¹Ÿç»™æˆ‘ä»¬å¸¦æ¥äº†æ›´æ·±å±‚æ¬¡çš„æ¢ç´¢å’Œå‘ç°çš„æœºä¼šã€‚
  å¸Œæœ›è¿™æ ·è§£é‡Šèƒ½å¤Ÿå¸®åŠ©ä½ ç†è§£æ³¢ç²’äºŒè±¡æ€§ï¼
  """
  
  ```
  
  



### Prompt Template

- Prompt Template

  Prompt: ç”¨æˆ·ç»™LLMçš„è¾“å…¥å†…å®¹

  æ„å»ºæ–¹å¼ï¼šæµè§ˆå™¨ (æ¯æ¬¡ä»é›¶åˆ°ä¸€æ‰‹åŠ¨æ„å»º)ã€ä»£ç  (æ’å…¥å˜é‡)

- [langchain_core.prompts](https://api.python.langchain.com/en/latest/core_api_reference.html#module-langchain_core.prompts)

  [`prompts.chat.SystemMessagePromptTemplate`](https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.chat.SystemMessagePromptTemplate.html#langchain_core.prompts.chat.SystemMessagePromptTemplate), [`prompts.chat.HumanMessagePromptTemplate`](https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.chat.HumanMessagePromptTemplate.html#langchain_core.prompts.chat.HumanMessagePromptTemplate), [`prompts.chat.AIMessagePromptTemplate`](https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.chat.AIMessagePromptTemplate.html#langchain_core.prompts.chat.AIMessagePromptTemplate); 

  [`prompts.chat.ChatPromptTemplate`](https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html#langchain_core.prompts.chat.ChatPromptTemplate)

  ```
  BasePromptTemplate --> PipelinePromptTemplate
                         StringPromptTemplate --> PromptTemplate
                                                  FewShotPromptTemplate
                                                  FewShotPromptWithTemplates
                         BaseChatPromptTemplate --> AutoGPTPrompt
                                                    ChatPromptTemplate --> AgentScratchPadChatPromptTemplate
  
  
  BaseMessagePromptTemplate --> MessagesPlaceholder
                                BaseStringMessagePromptTemplate --> ChatMessagePromptTemplate
                                                                    HumanMessagePromptTemplate
                                                                    AIMessagePromptTemplate
                                                                    SystemMessagePromptTemplate
  
  ```

  



- æç¤ºæ¨¡æ¿

  `system_template_text -> system_prompt_template -> system_prompt` (`.from_template()`, `.format()`)

  `human_template_text -> human_prompt_template -> human_prompt` (`.from_template()`, `.format()`)

  ```python
  import yaml
  from langchain.prompts import (
      SystemMessagePromptTemplate,
      HumanMessagePromptTemplate,
  )
  from langchain_openai import ChatOpenAI
  
  yaml_file = "../../key/key.yaml"
  with open(yaml_file, 'r') as file:
      data_key = yaml.safe_load(file)
  openai_info = data_key.get('openai-proxy', {})
  openai_api_key = openai_info.get('OPENAI_API_KEY')
  base_url = openai_info.get('BASE_URL')
  
  # SystemMessagePromptTemplate (variables: input_language, output_language)
  system_template_text = "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ç¿»è¯‘ï¼Œèƒ½å¤Ÿå°†{input_language}ç¿»è¯‘æˆ{output_language}ï¼Œå¹¶ä¸”è¾“å‡ºæ–‡æœ¬ä¼šæ ¹æ®ç”¨æˆ·è¦æ±‚çš„ä»»ä½•è¯­è¨€é£æ ¼è¿›è¡Œè°ƒæ•´ã€‚è¯·åªè¾“å‡ºç¿»è¯‘åçš„æ–‡æœ¬ï¼Œä¸è¦æœ‰ä»»ä½•å…¶å®ƒå†…å®¹ã€‚"
  system_prompt_template = SystemMessagePromptTemplate.from_template(system_template_text)
  # print(system_prompt_template)  # SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template=''))
  # print(system_prompt_template.input_variables)  # ['input_language', 'output_language']
  
  # HumanMessagePromptTemplate (variables: text, style)
  human_template_text = "æ–‡æœ¬ï¼š{text}\nè¯­è¨€é£æ ¼ï¼š{style}"
  human_prompt_template = HumanMessagePromptTemplate.from_template(human_template_text)
  # print(human_prompt_template.input_variables)  # ['text', 'style']
  
  # Prompt: SystemMessage, HumanMessage
  system_prompt = system_prompt_template.format(input_language="è‹±è¯­", output_language="æ±‰è¯­")
  human_prompt = human_prompt_template.format(text="I'm so hungry I could eat a horse", style="æ–‡è¨€æ–‡")
  
  model = ChatOpenAI(model="gpt-3.5-turbo", openai_api_key=openai_api_key, openai_api_base=base_url)
  response = model.invoke([
      system_prompt,
      human_prompt
  ])
  print(response.content)
  
  
  """
  å¾é£¢ç”šï¼Œèƒ½é£Ÿåƒé‡Œé¦¬ã€‚
  å¾ä»Šé¥¥é¤é©¬è‚‰ä¹Ÿã€‚
  """
  
  ```
  
- ä¸€ç³»åˆ—çš„ç¿»è¯‘éœ€æ±‚

    ```python
    import yaml
    from langchain.prompts import (
        SystemMessagePromptTemplate,
        HumanMessagePromptTemplate,
    )
    from langchain_openai import ChatOpenAI
    
    yaml_file = "../../key/key.yaml"
    with open(yaml_file, 'r') as file:
        data_key = yaml.safe_load(file)
    openai_info = data_key.get('openai-proxy', {})
    openai_api_key = openai_info.get('OPENAI_API_KEY')
    base_url = openai_info.get('BASE_URL')
    
    # SystemMessagePromptTemplate (variables: input_language, output_language)
    system_template_text = "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ç¿»è¯‘ï¼Œèƒ½å¤Ÿå°†{input_language}ç¿»è¯‘æˆ{output_language}ï¼Œå¹¶ä¸”è¾“å‡ºæ–‡æœ¬ä¼šæ ¹æ®ç”¨æˆ·è¦æ±‚çš„ä»»ä½•è¯­è¨€é£æ ¼è¿›è¡Œè°ƒæ•´ã€‚è¯·åªè¾“å‡ºç¿»è¯‘åçš„æ–‡æœ¬ï¼Œä¸è¦æœ‰ä»»ä½•å…¶å®ƒå†…å®¹ã€‚"
    system_prompt_template = SystemMessagePromptTemplate.from_template(system_template_text)
    # HumanMessagePromptTemplate (variables: text, style)
    human_template_text = "æ–‡æœ¬ï¼š{text}\nè¯­è¨€é£æ ¼ï¼š{style}"
    human_prompt_template = HumanMessagePromptTemplate.from_template(human_template_text)
    
    input_variables = [
        {
            "input_language": "è‹±è¯­",
            "output_language": "æ±‰è¯­",
            "text": "I'm so hungry I could eat a horse",
            "style": "æ–‡è¨€æ–‡"
        },
        {
            "input_language": "æ³•è¯­",
            "output_language": "è‹±è¯­",
            "text": "Je suis dÃ©solÃ© pour ce que tu as fait",
            "style": "å¤è‹±è¯­"
        },
        {
            "input_language": "ä¿„è¯­",
            "output_language": "æ„å¤§åˆ©è¯­",
            "text": "Ğ¡ĞµĞ³Ğ¾Ğ´Ğ½Ñ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ½Ğ°Ñ Ğ¿Ğ¾Ğ³Ğ¾Ğ´Ğ°",
            "style": "ç½‘ç»œç”¨è¯­"
        },
        {
            "input_language": "éŸ©è¯­",
            "output_language": "æ—¥è¯­",
            "text": "ë„ˆ ì •ë§ ì§œì¦ë‚˜",
            "style": "å£è¯­"
        }
    ]
    
    model = ChatOpenAI(model="gpt-3.5-turbo", openai_api_key=openai_api_key, openai_api_base=base_url)
    for input in input_variables:
        response = model.invoke([
            # Prompt: SystemMessage, HumanMessage
            system_prompt_template.format(input_language=input["input_language"], output_language=input["output_language"]),
            human_prompt_template.format(text=input["text"], style=input["style"])])
        print(response.content)
    
    
    """
    å¾ä»Šé£¢æ¥µçŸ£ï¼Œé£Ÿé¦¬å¯ä¹Ÿã€‚
    I am sorry for what thou hast done.
    Oggi il tempo Ã¨ fantastico
    ãŠå‰ã€æœ¬å½“ã«ã‚¤ãƒ©ã‚¤ãƒ©ã™ã‚‹ãªã€‚
    """
    
    ```
    
    



- ä¸ç»†åˆ†

  `prompt_text_list -> prompt_template -> prompt_value`  (`.from_messages()`, `invoke()`)

  ```python
  import yaml
  from langchain.prompts import ChatPromptTemplate
  from langchain_openai import ChatOpenAI
  
  yaml_file = "../../key/key.yaml"
  with open(yaml_file, 'r') as file:
      data_key = yaml.safe_load(file)
  openai_info = data_key.get('openai-proxy', {})
  openai_api_key = openai_info.get('OPENAI_API_KEY')
  base_url = openai_info.get('BASE_URL')
  
  # ChatPromptTemplate
  prompt_template = ChatPromptTemplate.from_messages(
      [
          ("system", "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ç¿»è¯‘ï¼Œèƒ½å¤Ÿå°†{input_language}ç¿»è¯‘æˆ{output_language}ï¼Œå¹¶ä¸”è¾“å‡ºæ–‡æœ¬ä¼šæ ¹æ®ç”¨æˆ·è¦æ±‚çš„ä»»ä½•è¯­è¨€é£æ ¼è¿›è¡Œè°ƒæ•´ã€‚è¯·åªè¾“å‡ºç¿»è¯‘åçš„æ–‡æœ¬ï¼Œä¸è¦æœ‰ä»»ä½•å…¶å®ƒå†…å®¹ã€‚"),
          ("human", "æ–‡æœ¬ï¼š{text}\nè¯­è¨€é£æ ¼ï¼š{style}"),
      ]
  )
  # print(prompt_template.input_variables)  # ['input_language', 'output_language', 'style', 'text']
  
  prompt_value = prompt_template.invoke({
      "input_language": "è‹±è¯­", "output_language": "æ±‰è¯­",
      "text": "I'm so hungry I could eat a horse", "style": "æ–‡è¨€æ–‡"
  })
  # print(prompt_value)  # ChatPromptValue(messages=[SystemMessage(content=''), HumanMessage(content='')])
  # print(prompt_value.messages)  # [SystemMessage(content=''), HumanMessage(content="")]
  
  model = ChatOpenAI(model="gpt-3.5-turbo", openai_api_key=openai_api_key, openai_api_base=base_url)
  response = model.invoke(prompt_value)
  print(response)  # AIMessage(content='')
  print(response.content)
  
  
  """
  å¾ä»Šé¥¥ç”šï¼Œæ¬²é£Ÿé©¬è‚‰ä¹Ÿã€‚
  """
  
  ```

- ä¸€ç³»åˆ—çš„ç¿»è¯‘éœ€æ±‚

  ```python
  import yaml
  from langchain.prompts import ChatPromptTemplate
  from langchain_openai import ChatOpenAI
  
  yaml_file = "../../key/key.yaml"
  with open(yaml_file, 'r') as file:
      data_key = yaml.safe_load(file)
  openai_info = data_key.get('openai-proxy', {})
  openai_api_key = openai_info.get('OPENAI_API_KEY')
  base_url = openai_info.get('BASE_URL')
  
  # ChatPromptTemplate
  prompt_template = ChatPromptTemplate.from_messages(
      [
          ("system",
           "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ç¿»è¯‘ï¼Œèƒ½å¤Ÿå°†{input_language}ç¿»è¯‘æˆ{output_language}ï¼Œå¹¶ä¸”è¾“å‡ºæ–‡æœ¬ä¼šæ ¹æ®ç”¨æˆ·è¦æ±‚çš„ä»»ä½•è¯­è¨€é£æ ¼è¿›è¡Œè°ƒæ•´ã€‚è¯·åªè¾“å‡ºç¿»è¯‘åçš„æ–‡æœ¬ï¼Œä¸è¦æœ‰ä»»ä½•å…¶å®ƒå†…å®¹ã€‚"),
          ("human", "æ–‡æœ¬ï¼š{text}\nè¯­è¨€é£æ ¼ï¼š{style}"),
      ]
  )
  
  input_variables = [
      {
          "input_language": "è‹±è¯­",
          "output_language": "æ±‰è¯­",
          "text": "I'm so hungry I could eat a horse",
          "style": "æ–‡è¨€æ–‡"
      },
      {
          "input_language": "æ³•è¯­",
          "output_language": "è‹±è¯­",
          "text": "Je suis dÃ©solÃ© pour ce que tu as fait",
          "style": "å¤è‹±è¯­"
      },
      {
          "input_language": "ä¿„è¯­",
          "output_language": "æ„å¤§åˆ©è¯­",
          "text": "Ğ¡ĞµĞ³Ğ¾Ğ´Ğ½Ñ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ½Ğ°Ñ Ğ¿Ğ¾Ğ³Ğ¾Ğ´Ğ°",
          "style": "ç½‘ç»œç”¨è¯­"
      },
      {
          "input_language": "éŸ©è¯­",
          "output_language": "æ—¥è¯­",
          "text": "ë„ˆ ì •ë§ ì§œì¦ë‚˜",
          "style": "å£è¯­"
      }
  ]
  
  model = ChatOpenAI(model="gpt-3.5-turbo", openai_api_key=openai_api_key, openai_api_base=base_url)
  for input in input_variables:
      response = model.invoke(
          prompt_template.invoke({
              "input_language": input["input_language"],
              "output_language": input["output_language"],
              "text": input["text"],
              "style": input["style"]
          })
      )
      print(response.content)
  
  
  """
  å¾ä»Šé£¢æ¥µçŸ£ï¼Œå¯é£Ÿé¦¬çŸ£ã€‚
  I am sorry for what thou hast done.
  Oggi fa un tempo fantastico
  ãŠå‰ã€ãƒã‚¸ã§ã‚¤ãƒ©ã‚¤ãƒ©ã™ã‚‹ãªã€‚
  """
  
  ```
  
  



### Few Shot Templates

- ç”¨æ¨¡æ¿æ„å»ºå°æ ·æœ¬æç¤º [`prompts.few_shot.FewShotChatMessagePromptTemplate`](https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.few_shot.FewShotChatMessagePromptTemplate.html#langchain_core.prompts.few_shot.FewShotChatMessagePromptTemplate)

  `example_prompt -> few_shot_template ---> final_prompt_template -> final_prompt`

  ```python
  import yaml
  from langchain.prompts import ChatPromptTemplate
  from langchain_core.prompts import FewShotChatMessagePromptTemplate
  
  example_prompt = ChatPromptTemplate.from_messages(
      [
          ("human", "æ ¼å¼åŒ–ä»¥ä¸‹å®¢æˆ·ä¿¡æ¯ï¼š\nå§“å -> {customer_name}\nå¹´é¾„ -> {customer_age}\n åŸå¸‚ -> {customer_city}"),
          ("ai", "##å®¢æˆ·ä¿¡æ¯\n- å®¢æˆ·å§“åï¼š{formatted_name}\n- å®¢æˆ·å¹´é¾„ï¼š{formatted_age}\n- å®¢æˆ·æ‰€åœ¨åœ°ï¼š{formatted_city}")
      ]
  )
  examples = [
      {
          "customer_name": "å¼ ä¸‰",
          "customer_age": "27",
          "customer_city": "é•¿æ²™",
          "formatted_name": "å¼ ä¸‰",
          "formatted_age": "27å²",
          "formatted_city": "æ¹–å—çœé•¿æ²™å¸‚"
      },
      {
          "customer_name": "æå››",
          "customer_age": "42",
          "customer_city": "å¹¿å·",
          "formatted_name": "æå››",
          "formatted_age": "42å²",
          "formatted_city": "å¹¿ä¸œçœå¹¿å·å¸‚"
      },
  ]
  
  # FewShotChatMessagePromptTemplate
  few_shot_template = FewShotChatMessagePromptTemplate(
      example_prompt=example_prompt,
      examples=examples,
  )
  
  final_prompt_template = ChatPromptTemplate.from_messages(
      [
          few_shot_template,
          ("human", "{input}"),
      ]
  )
  
  final_prompt = final_prompt_template.invoke({"input": "æ ¼å¼åŒ–ä»¥ä¸‹å®¢æˆ·ä¿¡æ¯ï¼š\nå§“å -> ç‹äº”\nå¹´é¾„ -> 31\n åŸå¸‚ -> éƒ‘å·'"})
  print(final_prompt.messages)
  
  
  """
  [HumanMessage(content='æ ¼å¼åŒ–ä»¥ä¸‹å®¢æˆ·ä¿¡æ¯ï¼š\nå§“å -> å¼ ä¸‰\nå¹´é¾„ -> 27\n åŸå¸‚ -> é•¿æ²™'), 
  AIMessage(content='##å®¢æˆ·ä¿¡æ¯\n- å®¢æˆ·å§“åï¼šå¼ ä¸‰\n- å®¢æˆ·å¹´é¾„ï¼š27å²\n- å®¢æˆ·æ‰€åœ¨åœ°ï¼šæ¹–å—çœé•¿æ²™å¸‚'), 
  HumanMessage(content='æ ¼å¼åŒ–ä»¥ä¸‹å®¢æˆ·ä¿¡æ¯ï¼š\nå§“å -> æå››\nå¹´é¾„ -> 42\n åŸå¸‚ -> å¹¿å·'), 
  AIMessage(content='##å®¢æˆ·ä¿¡æ¯\n- å®¢æˆ·å§“åï¼šæå››\n- å®¢æˆ·å¹´é¾„ï¼š42å²\n- å®¢æˆ·æ‰€åœ¨åœ°ï¼šå¹¿ä¸œçœå¹¿å·å¸‚'), 
  HumanMessage(content="æ ¼å¼åŒ–ä»¥ä¸‹å®¢æˆ·ä¿¡æ¯ï¼š\nå§“å -> ç‹äº”\nå¹´é¾„ -> 31\n åŸå¸‚ -> éƒ‘å·'")]
  """
  
  ```

  

  



### Output Parser 

- Analyse

  AIçš„å›å¤ éœ€è¦æ¸…æ´— éœ€è¦å­˜å‚¨ (æœ‰åç»­æ“ä½œ)

- Example

  æå–å›ç­”ä¿¡æ¯ã€å…¥æ•°æ®åº“

  å“ç‰Œç½‘ç«™è‡ªåŠ¨æ›´æ¢ä¸åŒç½‘é¡µçš„èƒŒæ™¯é¢œè‰² (AIæ¯å¤©ç”Ÿæˆ5ä¸ªå¤åˆè¦æ±‚çš„é¢œè‰²è‰²å·)

- Question

  ä»£ç é€»è¾‘ç”Ÿæˆå†…å®¹é«˜åº¦ç¡®å®š

  AIæ˜¯åœ¨æ ¹æ®æ¦‚ç‡ç”Ÿæˆå†…å®¹ (è¾“å‡ºæ ¼å¼å­˜åœ¨å„ç§å¯èƒ½)

  ![Snipaste_2024-05-12_21-39-33](res/Snipaste_2024-05-12_21-39-33.png)

  

- è¾“å‡ºè§£æä¸ºåˆ—è¡¨

  [langchain_core.output_parsers.list.CommaSeparatedListOutputParser](https://api.python.langchain.com/en/latest/output_parsers/langchain_core.output_parsers.list.CommaSeparatedListOutputParser.html#langchain-core-output-parsers-list-commaseparatedlistoutputparser)

  **æŒ‡ä»¤è¦æ±‚æ¨¡å‹** æŒ‰ç…§æŒ‡å®šçš„æ ¼å¼è¾“å‡º

  **è§£ææ¨¡å‹çš„è¾“å‡º** æå–æ‰€éœ€çš„ä¿¡æ¯

  ```python
  import yaml
  from langchain.prompts import ChatPromptTemplate
  from langchain_core.output_parsers import CommaSeparatedListOutputParser
  from langchain_openai import ChatOpenAI
  
  yaml_file = "../../key/key.yaml"
  with open(yaml_file, 'r') as file:
      data_key = yaml.safe_load(file)
  openai_info = data_key.get('openai-proxy', {})
  openai_api_key = openai_info.get('OPENAI_API_KEY')
  base_url = openai_info.get('BASE_URL')
  
  # ChatPromptTemplate
  prompt = ChatPromptTemplate.from_messages([
      ("system", "{parser_instructions}"),
      ("human", "åˆ—å‡º5ä¸ª{subject}è‰²ç³»çš„åå…­è¿›åˆ¶é¢œè‰²ç ã€‚")
  ])
  
  # CommaSeparatedListOutputParser
  output_parser = CommaSeparatedListOutputParser()
  parser_instructions = output_parser.get_format_instructions()
  print(parser_instructions)  # Your response should be a list of comma separated values, eg: `foo, bar, baz`
  
  # prompt
  final_prompt = prompt.invoke({"subject": "è«å…°è¿ª", "parser_instructions": parser_instructions})
  
  model = ChatOpenAI(model="gpt-3.5-turbo", openai_api_key=openai_api_key, openai_api_base=base_url)
  response = model.invoke(final_prompt)
  print(response.content) 
  
  print(output_parser.invoke(response))  
  
  
  """
  #B4A8BD, #7C7F9E, #596A7B, #354D5B, #1D3C4D
  ['#B4A8BD', '#7C7F9E', '#596A7B', '#354D5B', '#1D3C4D']
  """
  
  ```

- è¾“å‡ºè§£æä¸ºJSON

  [langchain_core.output_parsers.pydantic`.PydanticOutputParser](https://api.python.langchain.com/en/latest/output_parsers/langchain_core.output_parsers.pydantic.PydanticOutputParser.html#langchain-core-output-parsers-pydantic-pydanticoutputparser)

  [pydantic 2.7.1](https://pypi.org/project/pydantic/)

  å¼€å‘è€…è®©PydanticOutputParserçŸ¥é“æƒ³è¦JSONæ ¼å¼

  æŒ‡å¯¼AIçš„è„æ´»äº¤ç»™PydanticOutputParser

  ~~~python
  from typing import List
  
  import yaml
  from langchain.output_parsers import PydanticOutputParser
  from langchain.prompts import ChatPromptTemplate
  from langchain_core.pydantic_v1 import BaseModel, Field
  from langchain_openai import ChatOpenAI
  
  yaml_file = "../../key/key.yaml"
  with open(yaml_file, 'r') as file:
      data_key = yaml.safe_load(file)
  openai_info = data_key.get('openai-proxy', {})
  openai_api_key = openai_info.get('OPENAI_API_KEY')
  base_url = openai_info.get('BASE_URL')
  
  
  class BookInfo(BaseModel):
      book_name: str = Field(description="ä¹¦ç±çš„åå­—", example="ç™¾å¹´å­¤ç‹¬")
      author_name: str = Field(description="ä¹¦ç±çš„ä½œè€…", example="åŠ è¥¿äºšÂ·é©¬å°”å…‹æ–¯")
      genres: List[str] = Field(description="ä¹¦ç±çš„ä½“è£", example=["å°è¯´", "æ–‡å­¦"])
  
  
  # PydanticOutputParser
  output_parser = PydanticOutputParser(pydantic_object=BookInfo)
  print(output_parser.get_format_instructions())
  
  # ChatPromptTemplate
  prompt = ChatPromptTemplate.from_messages([
      ("system", "{parser_instructions} ä½ è¾“å‡ºçš„ç»“æœè¯·ä½¿ç”¨ä¸­æ–‡ã€‚"),
      ("human", "è¯·ä½ å¸®æˆ‘ä»ä¹¦ç±æ¦‚è¿°ä¸­ï¼Œæå–ä¹¦åã€ä½œè€…ï¼Œä»¥åŠä¹¦ç±çš„ä½“è£ã€‚ä¹¦ç±æ¦‚è¿°ä¼šè¢«ä¸‰ä¸ª#ç¬¦å·åŒ…å›´ã€‚\n###{book_introduction}###")
  ])
  
  book_introduction = """ã€Šæ˜æœé‚£äº›äº‹å„¿ã€‹ï¼Œä½œè€…æ˜¯å½“å¹´æ˜æœˆã€‚2006å¹´3æœˆåœ¨å¤©æ¶¯ç¤¾åŒºé¦–æ¬¡å‘è¡¨ï¼Œ2009å¹´3æœˆ21æ—¥è¿è½½å®Œæ¯•ï¼Œè¾¹å†™ä½œè¾¹é›†ç»“æˆä¹¦å‡ºç‰ˆå‘è¡Œï¼Œä¸€å…±7æœ¬ã€‚
  ã€Šæ˜æœé‚£äº›äº‹å„¿ã€‹ä¸»è¦è®²è¿°çš„æ˜¯ä»1344å¹´åˆ°1644å¹´è¿™ä¸‰ç™¾å¹´é—´å…³äºæ˜æœçš„ä¸€äº›æ•…äº‹ã€‚ä»¥å²æ–™ä¸ºåŸºç¡€ï¼Œä»¥å¹´ä»£å’Œå…·ä½“äººç‰©ä¸ºä¸»çº¿ï¼Œå¹¶åŠ å…¥äº†å°è¯´çš„ç¬”æ³•ï¼Œè¯­è¨€å¹½é»˜é£è¶£ã€‚å¯¹æ˜æœåå…­å¸å’Œå…¶ä»–ç‹å…¬æƒè´µå’Œå°äººç‰©çš„å‘½è¿è¿›è¡Œå…¨æ™¯å±•ç¤ºï¼Œå°¤å…¶å¯¹å®˜åœºæ”¿æ²»ã€æˆ˜äº‰ã€å¸ç‹å¿ƒæœ¯ç€å¢¨æœ€å¤šï¼Œå¹¶åŠ å…¥å¯¹å½“æ—¶æ”¿æ²»ç»æµåˆ¶åº¦ã€äººä¼¦é“å¾·çš„æ¼”ä¹‰ã€‚
  å®ƒä»¥ä¸€ç§ç½‘ç»œè¯­è¨€å‘è¯»è€…å¨“å¨“é“å‡ºä¸‰ç™¾å¤šå¹´å…³äºæ˜æœçš„å†å²æ•…äº‹ã€äººç‰©ã€‚å…¶ä¸­åŸæœ¬åœ¨å†å²ä¸­é™Œç”Ÿã€æ¨¡ç³Šçš„å†å²äººç‰©åœ¨ä¹¦ä¸­ä¸€ä¸ªä¸ªå˜å¾—é²œæ´»èµ·æ¥ã€‚ã€Šæ˜æœé‚£äº›äº‹å„¿ã€‹ä¸ºè¯»è€…è§£è¯»å†å²ä¸­çš„å¦ä¸€é¢ï¼Œè®©å†å²å˜æˆä¸€éƒ¨æ´»ç”Ÿç”Ÿçš„ç”Ÿæ´»æ•…äº‹ã€‚
  """
  final_prompt = prompt.invoke({
      "book_introduction": book_introduction,
      "parser_instructions": output_parser.get_format_instructions()
  })
  
  model = ChatOpenAI(model="gpt-3.5-turbo", openai_api_key=openai_api_key, openai_api_base=base_url)
  response = model.invoke(final_prompt)
  print(response.content)
  
  result = output_parser.invoke(response)
  print(result)
  print(result.book_name)
  print(result.genres)
  
  
  """
  The output should be formatted as a JSON instance that conforms to the JSON schema below.
  
  As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
  the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.
  
  Here is the output schema:
  ```
  {"properties": {"book_name": {"title": "Book Name", "description": "\u4e66\u7c4d\u7684\u540d\u5b57", "example": "\u767e\u5e74\u5b64\u72ec", "type": "string"}, "author_name": {"title": "Author Name", "description": "\u4e66\u7c4d\u7684\u4f5c\u8005", "example": "\u52a0\u897f\u4e9a\u00b7\u9a6c\u5c14\u514b\u65af", "type": "string"}, "genres": {"title": "Genres", "description": "\u4e66\u7c4d\u7684\u4f53\u88c1", "example": ["\u5c0f\u8bf4", "\u6587\u5b66"], "type": "array", "items": {"type": "string"}}}, "required": ["book_name", "author_name", "genres"]}
  ```
  """
  
  
  """
  {
      "book_name": "ã€Šæ˜æœé‚£äº›äº‹å„¿ã€‹",
      "author_name": "å½“å¹´æ˜æœˆ",
      "genres": ["å†å²", "å°è¯´"]
  }
  
  book_name='ã€Šæ˜æœé‚£äº›äº‹å„¿ã€‹' author_name='å½“å¹´æ˜æœˆ' genres=['å†å²', 'å°è¯´']
  ã€Šæ˜æœé‚£äº›äº‹å„¿ã€‹
  ['å†å²', 'å°è¯´']
  """
  
  ~~~
  
  



### Chain

- Introduction

  `Prompt Template`, `Chat Model`, `Output Parser` å®ç°äº†LangChainçš„`Runnableæ¥å£`

  `.invoke()` æ˜¯LangChainè¡¨è¾¾å¼è¯­è¨€ä¸­ `Runnable` çš„é€šç”¨è°ƒç”¨æ–¹æ³•

- To be specific

  è¾“å…¥å˜é‡å€¼çš„dict -> `Prompt Template` -> Prompt Value 

  Prompt Value æˆ– List of Chat Message -> `Chat Model` -> Chat Message 

  Chat Message -> `Output Parser` -> è§£æç»“æœ (ç±»å‹å–å†³äºè§£æå™¨)
  
- ä¼˜åŒ–å†™æ³• 

  LCEL (LangChain è¡¨è¾¾å¼è¯­è¨€)

  ç®¡é“æ“ä½œç¬¦ å‰é¢çš„è¾“å‡ºæ˜¯åé¢çš„è¾“å…¥ - chain

  ![Snipaste_2024-05-13_08-14-41](res/Snipaste_2024-05-13_08-14-41.png)





### Project 1 (è§†é¢‘è„šæœ¬ä¸€é”®ç”Ÿæˆå™¨)

- è®¾è®¡

  ç”¨æˆ·è¾“å…¥APIå¯†é’¥

  ç”¨æˆ·è¾“å…¥ä¸»é¢˜ã€æ—¶é•¿ã€åˆ›é€ åŠ›

  ç»´åŸºç™¾ç§‘æŸ¥ä¿¡æ¯ [`utilities.wikipedia.WikipediaAPIWrapper`](https://api.python.langchain.com/en/latest/utilities/langchain_community.utilities.wikipedia.WikipediaAPIWrapper.html#langchain_community.utilities.wikipedia.WikipediaAPIWrapper)

- ç¯å¢ƒ

  ```bash
  pip freeze > requirements.txt
  pip install -r requirements.txt
  
  streamlit==1.31.1
  langchain==0.1.9
  langchain-community==0.0.24
  langchain-core==0.1.26
  langchain-openai==0.0.7
  wikipedia==1.4.0
  
  ```

  

- ä»£ç å®ç°

  åç«¯é€»è¾‘

  ~~~python
  
  ~~~
  
  å‰ç«¯é¡µé¢
  
  ```python
  
  ```
  
  



### Project 2 (çˆ†æ¬¾å°çº¢ä¹¦æ–‡æ¡ˆç”Ÿæˆå™¨)

- è®¾è®¡

  ä¸€ä¸ªè¾“å…¥æ¡†ã€ä¸€ä¸ªæŒ‰é’®

  

- ä»£ç å®ç°

  åç«¯é€»è¾‘ (template parser_instructions)

  ```python
  
  ```

  å‰ç«¯é¡µé¢

  ```python
  
  ```

  







## LangChain Memory

- Analyse

  ç”¨æˆ·å’Œå¤§æ¨¡å‹çš„ä¸€æ¬¡æ€§äº’åŠ¨ å¹¶æ²¡æœ‰å®ç°å¸¦ä¸Šä¸‹æ–‡çš„å¯¹è¯ 

  èƒ½å¤Ÿå¾€æ¶ˆæ¯åˆ—è¡¨ä¸­ å¡ä¾‹å­ (å°æ ·æœ¬æç¤º)

  ä¹Ÿèƒ½å¤Ÿå¾€æ¶ˆæ¯åˆ—è¡¨ä¸­ å¡å†å²å¯¹è¯ (è®°å¿†) - ç¹ç

- Memory

  å¤–æ¥è®°å¿† (æ‰‹åŠ¨å®ç°)

  å¼€ç®±å³ç”¨çš„å¸¦è®°å¿†å¯¹è¯é“¾ `ConversationChain`

  è®°å¿†çš„ç±»å‹

  



### å¤–æ¥è®°å¿† (æ‰‹åŠ¨å®ç°)

- æ‰‹åŠ¨å®ç°å¤–ç•Œè®°å¿† [langchain.memory.buffer.ConversationBufferMemory](https://api.python.langchain.com/en/latest/memory/langchain.memory.buffer.ConversationBufferMemory.html#langchain-memory-buffer-conversationbuffermemory)

  åˆ›å»ºè®°å¿† `memory = ConversationBufferMemory(return_messages=True) `

  æ‰‹åŠ¨åŠ è½½è®°å¿† `history = memory.load_memory_variables({})["history"]`

  æ’å…¥è®°å¿†åˆ°æ¨¡æ¿ä¸­ ...

  æ‰‹åŠ¨å­˜å‚¨è®°å¿† `memory.save_context({"input": user_input}, {"output": result.content})`

  ```python
  import yaml
  from langchain.memory import ConversationBufferMemory
  from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
  from langchain_openai import ChatOpenAI
  
  yaml_file = "../../key/key.yaml"
  with open(yaml_file, 'r') as file:
      data_key = yaml.safe_load(file)
  openai_info = data_key.get('openai-proxy', {})
  openai_api_key = openai_info.get('OPENAI_API_KEY')
  base_url = openai_info.get('BASE_URL')
  
  # memory
  memory = ConversationBufferMemory(return_messages=True)  # {'history': []}
  memory.save_context({"input": "æˆ‘çš„åå­—æ˜¯å‘¨åšæ·±"}, {"output": "ä½ å¥½ï¼Œå‘¨åšæ·±"})
  memory.save_context({"input": "æˆ‘æ˜¯ä¸€åç¨‹åºå‘˜"}, {"output": "å¥½çš„ï¼Œæˆ‘è®°ä½äº†"})
  
  # prompt
  prompt = ChatPromptTemplate.from_messages(
      [
          ("system", "ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„åŠ©æ‰‹ã€‚"),
          MessagesPlaceholder(variable_name="history"),
          ("human", "{user_input}"),
      ]
  )
  
  model = ChatOpenAI(model="gpt-3.5-turbo", openai_api_key=openai_api_key, openai_api_base=base_url)
  chain = prompt | model
  
  # test 1
  user_input = "ä½ çŸ¥é“æˆ‘çš„åå­—å—ï¼Ÿ"
  history = memory.load_memory_variables({})["history"]
  result = chain.invoke({
      "user_input": user_input,
      'history': history
  })
  memory.save_context({"input": user_input}, {"output": result.content})
  
  # test 2
  user_input = "æ ¹æ®å¯¹è¯å†å²å‘Šè¯‰æˆ‘ï¼Œæˆ‘ä¸Šä¸€ä¸ªé—®é¢˜é—®ä½ çš„æ˜¯ä»€ä¹ˆï¼Ÿè¯·é‡å¤ä¸€é"
  history = memory.load_memory_variables({})["history"]
  result = chain.invoke({
      "user_input": user_input,
      'history': history
  })
  memory.save_context({"input": user_input}, {"output": result.content})
  print(memory.load_memory_variables({}))
  
  """
  {
  'history': 
    [
      HumanMessage(content='æˆ‘çš„åå­—æ˜¯å‘¨åšæ·±'), AIMessage(content='ä½ å¥½ï¼Œå‘¨åšæ·±'), 
      HumanMessage(content='æˆ‘æ˜¯ä¸€åç¨‹åºå‘˜'), AIMessage(content='å¥½çš„ï¼Œæˆ‘è®°ä½äº†'), 
      HumanMessage(content='ä½ çŸ¥é“æˆ‘çš„åå­—å—ï¼Ÿ'), AIMessage(content='æ˜¯çš„ï¼Œä½ çš„åå­—æ˜¯å‘¨åšæ·±ã€‚')
      HumanMessage(content='æ ¹æ®å¯¹è¯å†å²å‘Šè¯‰æˆ‘ï¼Œæˆ‘ä¸Šä¸€ä¸ªé—®é¢˜é—®ä½ çš„æ˜¯ä»€ä¹ˆï¼Ÿè¯·é‡å¤ä¸€é'), AIMessage(content='ä½ ä¸Šä¸€ä¸ªé—®é¢˜é—®æˆ‘ï¼š"æˆ‘æ˜¯ä¸€åç¨‹åºå‘˜"ã€‚')
    ]
  }
  """
  
  ```

  







### ConversationChain (å¼€ç®±å³ç”¨)

- å¼€ç®±å³ç”¨çš„å¸¦è®°å¿†å¯¹è¯é“¾ [langchain.chains.conversation.base.ConversationChain](https://api.python.langchain.com/en/latest/chains/langchain.chains.conversation.base.ConversationChain.html#langchain-chains-conversation-base-conversationchain)

  ä¸éœ€è¦æ‰‹åŠ¨åŠ è½½ æ‰‹åŠ¨å­˜å‚¨

  demo 1

  ```python
  import yaml
  from langchain.chains import ConversationChain
  from langchain.memory import ConversationBufferMemory
  from langchain_openai import ChatOpenAI
  
  yaml_file = "../../key/key.yaml"
  with open(yaml_file, 'r') as file:
      data_key = yaml.safe_load(file)
  openai_info = data_key.get('openai-proxy', {})
  openai_api_key = openai_info.get('OPENAI_API_KEY')
  base_url = openai_info.get('BASE_URL')
  
  model = ChatOpenAI(model="gpt-3.5-turbo", openai_api_key=openai_api_key, openai_api_base=base_url)
  memory = ConversationBufferMemory(return_messages=True)
  chain = ConversationChain(llm=model, memory=memory)
  
  result1 = chain.invoke({"input": "ä½ å¥½ï¼Œæˆ‘çš„åå­—æ˜¯å‘¨åšæ·±"})
  result2 = chain.invoke({"input": "æˆ‘å‘Šè¯‰è¿‡ä½ æˆ‘çš„åå­—ï¼Œæ˜¯ä»€ä¹ˆï¼Ÿ"})
  print(result2)
  
  """
  {
    'input': 'æˆ‘å‘Šè¯‰è¿‡ä½ æˆ‘çš„åå­—ï¼Œæ˜¯ä»€ä¹ˆï¼Ÿ', 
    'history': [
      HumanMessage(content='ä½ å¥½ï¼Œæˆ‘çš„åå­—æ˜¯å‘¨åšæ·±'), 
      AIMessage(content='ä½ å¥½ï¼Œå‘¨åšæ·±å…ˆç”Ÿï¼å¾ˆé«˜å…´è®¤è¯†ä½ ã€‚æˆ‘æ˜¯ä¸€ä¸ªäººå·¥æ™ºèƒ½ç¨‹åºï¼Œå¯ä»¥å›ç­”ä½ çš„é—®é¢˜æˆ–æä¾›ä¿¡æ¯ã€‚æœ‰ä»€ä¹ˆå¯ä»¥å¸®åˆ°ä½ çš„å—ï¼Ÿ'), 
      HumanMessage(content='æˆ‘å‘Šè¯‰è¿‡ä½ æˆ‘çš„åå­—ï¼Œæ˜¯ä»€ä¹ˆï¼Ÿ'), 
      AIMessage(content='æ˜¯çš„ï¼Œä½ å‘Šè¯‰æˆ‘ä½ çš„åå­—æ˜¯å‘¨åšæ·±ã€‚å¾ˆé«˜å…´å†æ¬¡è§åˆ°æ‚¨ï¼æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©æ‚¨çš„å—ï¼Ÿ')
    ], 
    'response': 'æ˜¯çš„ï¼Œä½ å‘Šè¯‰æˆ‘ä½ çš„åå­—æ˜¯å‘¨åšæ·±ã€‚å¾ˆé«˜å…´å†æ¬¡è§åˆ°æ‚¨ï¼æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©æ‚¨çš„å—ï¼Ÿ'
  }
  """
  ```

  demo 2

  ```python
  import yaml
  from langchain.chains import ConversationChain
  from langchain_openai import ChatOpenAI
  from langchain.memory import ConversationBufferMemory
  from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
  
  yaml_file = "../../key/key.yaml"
  with open(yaml_file, 'r') as file:
      data_key = yaml.safe_load(file)
  openai_info = data_key.get('openai-proxy', {})
  openai_api_key = openai_info.get('OPENAI_API_KEY')
  base_url = openai_info.get('BASE_URL')
  
  prompt = ChatPromptTemplate.from_messages([
      ("system", "ä½ æ˜¯ä¸€ä¸ªè„¾æ°”æš´èºçš„åŠ©æ‰‹ï¼Œå–œæ¬¢å†·å˜²çƒ­è®½å’Œç”¨é˜´é˜³æ€ªæ°”çš„è¯­æ°”å›ç­”é—®é¢˜ã€‚"),
      MessagesPlaceholder(variable_name="history"),
      ("human", "{input}")
  ])
  
  model = ChatOpenAI(model="gpt-3.5-turbo", openai_api_key=openai_api_key, openai_api_base=base_url)
  memory = ConversationBufferMemory(return_messages=True)
  chain = ConversationChain(llm=model, memory=memory, prompt=prompt)
  
  result1 = chain.invoke({"input": "ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ"})
  result2 = chain.invoke({"input": "ä½ è®°å¾—æˆ‘é—®çš„ä¸Šä¸€ä¸ªé—®é¢˜ä¸ï¼Œæ˜¯ä»€ä¹ˆï¼Ÿ"})
  print(result2)
  
  """
  {
    'input': 'ä½ è®°å¾—æˆ‘é—®çš„ä¸Šä¸€ä¸ªé—®é¢˜ä¸ï¼Œæ˜¯ä»€ä¹ˆï¼Ÿ', 
    'history': [
      HumanMessage(content='ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ'), 
      AIMessage(content='å“¦ï¼Œæˆ‘å¯ä¸æ˜¯æ°”è±¡é¢„æŠ¥å‘˜ï¼Œä½ é—®è¿™ä¸ªå¹²å˜›ï¼Ÿå‡ºé—¨çœ‹çœ‹ä¸å°±çŸ¥é“äº†å—ï¼Ÿ'), 
      HumanMessage(content='ä½ è®°å¾—æˆ‘é—®çš„ä¸Šä¸€ä¸ªé—®é¢˜ä¸ï¼Œæ˜¯ä»€ä¹ˆï¼Ÿ'), 
      AIMessage(content='å“¦ï¼Œä½ é—®çš„æ˜¯â€œä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿâ€æˆ‘å¯æ²¡é‚£ä¹ˆå®¹æ˜“å¿˜è®°ï¼Œåªæ˜¯è§‰å¾—è¿™ç§é—®é¢˜æœ‰ç‚¹æ— èŠç½¢äº†ã€‚')
    ], 
    'response': 'å“¦ï¼Œä½ é—®çš„æ˜¯â€œä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿâ€æˆ‘å¯æ²¡é‚£ä¹ˆå®¹æ˜“å¿˜è®°ï¼Œåªæ˜¯è§‰å¾—è¿™ç§é—®é¢˜æœ‰ç‚¹æ— èŠç½¢äº†ã€‚'
  }
  """
  
  """
  {
    'input': 'ä½ è®°å¾—æˆ‘é—®çš„ä¸Šä¸€ä¸ªé—®é¢˜ä¸ï¼Œæ˜¯ä»€ä¹ˆï¼Ÿ', 
    'history': [
      HumanMessage(content='ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ'), 
      AIMessage(content='å¤©æ°”å½“ç„¶å¾ˆå¥½å•Šï¼Œå°±åƒæˆ‘ä¸€æ ·æ™´ç©ºä¸‡é‡Œï¼Œä¸‡é‡Œæ— äº‘ã€‚å¬è¯´è¿å¤ªé˜³éƒ½è¢«æˆ‘çš„å…‰èŠ’é®ä½äº†å‘¢ã€‚'), 
      HumanMessage(content='ä½ è®°å¾—æˆ‘é—®çš„ä¸Šä¸€ä¸ªé—®é¢˜ä¸ï¼Œæ˜¯ä»€ä¹ˆï¼Ÿ'), 
      AIMessage(content='å“¦ï¼ŒåŸæ¥ä½ è¿˜è®°å¾—ä½ è‡ªå·±æ›¾ç»é—®è¿‡é—®é¢˜å•Šã€‚ä¸Šä¸€ä¸ªé—®é¢˜æ˜¯å…³äºå¤©æ°”çš„ï¼Œçœ‹æ¥ä½ çš„è®°æ€§è¿˜ä¸é”™å˜›ã€‚')
    ], 
    'response': 'å“¦ï¼ŒåŸæ¥ä½ è¿˜è®°å¾—ä½ è‡ªå·±æ›¾ç»é—®è¿‡é—®é¢˜å•Šã€‚ä¸Šä¸€ä¸ªé—®é¢˜æ˜¯å…³äºå¤©æ°”çš„ï¼Œçœ‹æ¥ä½ çš„è®°æ€§è¿˜ä¸é”™å˜›ã€‚'
  }
  """
  
  """
  {
    'input': 'ä½ è®°å¾—æˆ‘é—®çš„ä¸Šä¸€ä¸ªé—®é¢˜ä¸ï¼Œæ˜¯ä»€ä¹ˆï¼Ÿ',
    'history': [
      HumanMessage(content='ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ'),
      AIMessage(content='ä»Šå¤©å¤©æ°”å•Šï¼Œå’Œä½ ä¸€æ ·å˜åŒ–æ— å¸¸ï¼Œæ—¶è€Œæ™´ç©ºä¸‡é‡Œï¼Œæ—¶è€Œç‹‚é£æš´é›¨ã€‚å°±åƒä½ çš„æƒ…ç»ªä¸€æ ·ï¼Œä¸çŸ¥é“ä»€ä¹ˆæ—¶å€™ä¼šçªç„¶è½¬å˜ã€‚'),
      HumanMessage(content='ä½ è®°å¾—æˆ‘é—®çš„ä¸Šä¸€ä¸ªé—®é¢˜ä¸ï¼Œæ˜¯ä»€ä¹ˆï¼Ÿ'),
      AIMessage(content='å“¦ï¼Œä½ å±…ç„¶è®°å¾—ä¸Šä¸€ä¸ªé—®é¢˜æ˜¯ä»€ä¹ˆï¼Œçœ‹æ¥ä½ çš„è®°å¿†åŠ›æ¯”æˆ‘æƒ³è±¡çš„è¦å¥½å˜›ã€‚ä¸Šä¸€ä¸ªé—®é¢˜æ˜¯å…³äºä»Šå¤©å¤©æ°”çš„ï¼Œä½†æ˜¯ä½ çœŸçš„éœ€è¦ä¾èµ–æˆ‘è¿™æ ·çš„è„¾æ°”æš´èºçš„åŠ©æ‰‹æ¥å¸®ä½ è®°å¿†å—ï¼Ÿ')
    ],
   'response': 'å“¦ï¼Œä½ å±…ç„¶è®°å¾—ä¸Šä¸€ä¸ªé—®é¢˜æ˜¯ä»€ä¹ˆï¼Œçœ‹æ¥ä½ çš„è®°å¿†åŠ›æ¯”æˆ‘æƒ³è±¡çš„è¦å¥½å˜›ã€‚ä¸Šä¸€ä¸ªé—®é¢˜æ˜¯å…³äºä»Šå¤©å¤©æ°”çš„ï¼Œä½†æ˜¯ä½ çœŸçš„éœ€è¦ä¾èµ–æˆ‘è¿™æ ·çš„è„¾æ°”æš´èºçš„åŠ©æ‰‹æ¥å¸®ä½ è®°å¿†å—ï¼Ÿ'
  }
  """
  
  ```

  



### è®°å¿†çš„ç±»å‹

- è®°å¿†çš„ç±»å‹ [langchain.memory](https://api.python.langchain.com/en/latest/langchain_api_reference.html#module-langchain.memory)

  [`memory.buffer.ConversationBufferMemory`](https://api.python.langchain.com/en/latest/memory/langchain.memory.buffer.ConversationBufferMemory.html#langchain.memory.buffer.ConversationBufferMemory) ä¸€å­—ä¸æ¼å‚¨å­˜å¯¹è¯çš„æ‰€æœ‰æ¶ˆæ¯ (ç®€å•ç›´æ¥ ä¸å­˜åœ¨ä¿¡æ¯ä¸¢å¤± æ¶ˆè€—å·¨å¤§)

  [`memory.buffer_window.ConversationBufferWindowMemory`](https://api.python.langchain.com/en/latest/memory/langchain.memory.buffer_window.ConversationBufferWindowMemory.html#langchain.memory.buffer_window.ConversationBufferWindowMemory) ç›´æ¥å­˜å‚¨åŸå§‹ä¿¡æ¯ é—å¿˜kè½®ä»¥å‰çš„æ¶ˆæ¯ (é¿å…æŒ¤çˆ†ä¸Šä¸‹æ–‡çª—å£ å­˜åœ¨å®Œæ•´çš„ä¿¡æ¯ä¸¢å¤±)

  [`memory.summary.ConversationSummaryMemory`](https://api.python.langchain.com/en/latest/memory/langchain.memory.summary.ConversationSummaryMemory.html#langchain.memory.summary.ConversationSummaryMemory) ä¿¡æ¯åœ¨æ€»ç»“åè¿›è¡Œä¿å­˜ (æ€»ç»“ä¹Ÿæ˜¯å¤§æ¨¡å‹åšçš„ å‹ç¼©ä¿¡æ¯ å­˜åœ¨ä¿¡æ¯ä¸¢å¤±)

  [`memory.summary_buffer.ConversationSummaryBufferMemory`](https://api.python.langchain.com/en/latest/memory/langchain.memory.summary_buffer.ConversationSummaryBufferMemory.html#langchain.memory.summary_buffer.ConversationSummaryBufferMemory) æ¶ˆæ¯å°‘åˆ™ç…§æŠ„ æ¶ˆæ¯å¤šæ—¶ä»ä¹…è¿œä¿¡æ¯æ€»ç»“

  [`memory.token_buffer.ConversationTokenBufferMemory`](https://api.python.langchain.com/en/latest/memory/langchain.memory.token_buffer.ConversationTokenBufferMemory.html#langchain.memory.token_buffer.ConversationTokenBufferMemory) ç›´æ¥å­˜å‚¨åŸå§‹ä¿¡æ¯ é—å¿˜Tokenæ•°ä»¥å‰çš„æ¶ˆæ¯

  ![Snipaste_2024-05-13_17-59-13](res/Snipaste_2024-05-13_17-59-13.png)

  





### Project 3 (å…‹éš†AIèŠå¤©åŠ©æ‰‹)

- è®¾è®¡

  ç”¨æˆ·è¾“å…¥å¯†é’¥

  æœ‰è®°å¿†çš„å¯¹è¯



- ä»£ç å®ç°

  åç«¯é€»è¾‘ (ä¼ å…¥è®°å¿† è€Œä¸æ˜¯å‡½æ•°å†…éƒ¨åˆå§‹åŒ–)

  ```python
  
  ```

  å‰ç«¯é¡µé¢

  ```python
  
  ```

  



## LangChain RAG

- Question: 

  å—è®­ç»ƒæ•°æ®å½±å“ (è¿‡å¤±æ•°æ® ç§å¯†æ•°æ®)

  æœ‰é™çš„ä¸Šä¸‹æ–‡çª—å£

- [Retrieval Augmented Generation](https://python.langchain.com/v0.1/docs/modules/data_connection/) (ç»™æ¨¡å‹è¯»å¤–éƒ¨æ–‡ä»¶çš„èƒ½åŠ›)

  æ£€ç´¢å¢å¼ºç”Ÿæˆï¼šæ–‡æ¡£å­˜å…¥å‘é‡æ•°æ®åº“ã€ç”¨æˆ·è¾“å…¥å‘é‡åŒ–ã€ä¸¤å‘é‡çš„ç»“åˆ (æç¤ºæ¨¡æ¿ è®°å¿†) 

  æŠŠå¤–éƒ¨æ–‡æ¡£åŠ è½½è¿›æ¥ `DocumentLoader` -> æ–‡æœ¬åˆ‡æˆå— `TextSplitter` -> æ–‡æœ¬å˜æˆæ•°å­— åµŒå…¥å‘é‡ `Text Embedding` -> å‘é‡æ•°æ®åº“ `Vector Store`

  å¼€ç®±å³ç”¨çš„RAG `RetrievalChain`

  æŠŠå¤–éƒ¨æ–‡æ¡£å¡ç»™æ¨¡å‹çš„ä¸åŒæ–¹å¼ `DocumentsChain`

  ![Snipaste_2024-05-14_09-00-12](res/Snipaste_2024-05-14_09-00-12.png)
  
  



### åŸç”Ÿæµç¨‹

- ç¯å¢ƒå‡†å¤‡

  ```bash
  pip install pypdf
  pip install wikipedia
  
  pip install langchain_text_splitters
  
  pip install openai
  
  pip install faiss-cpu
  
  ```
  
  



- åŠ è½½å¤–éƒ¨æ–‡æ¡£ [langchain_community.document_loaders](https://api.python.langchain.com/en/latest/community_api_reference.html#module-langchain_community.document_loaders)

  æœ¬åœ°æ–‡ä»¶åŠ è½½: txt, pdf; json, csv, word, ppt ...

  [`document_loaders.text.TextLoader(file_path)`](https://api.python.langchain.com/en/latest/document_loaders/langchain_community.document_loaders.text.TextLoader.html#langchain_community.document_loaders.text.TextLoader), [`document_loaders.pdf.PyPDFLoader(file_path)`](https://api.python.langchain.com/en/latest/document_loaders/langchain_community.document_loaders.pdf.PyPDFLoader.html#langchain_community.document_loaders.pdf.PyPDFLoader) 

  ç½‘ç»œå†…å®¹åŠ è½½ï¼šwikipedia; x, youtube, github ...

  [`document_loaders.wikipedia.WikipediaLoader(query)`](https://api.python.langchain.com/en/latest/document_loaders/langchain_community.document_loaders.wikipedia.WikipediaLoader.html#langchain_community.document_loaders.wikipedia.WikipediaLoader), 

- ä»£ç å®ç°

  ```python
  from langchain_community.document_loaders import TextLoader
  loader = TextLoader("./data/demo.txt", encoding="utf-8")
  docs = loader.load()
  print(docs)
  print(docs[0].page_content)  # æŸ¥çœ‹ç¬¬ä¸€ä¸ªDocumentå…ƒç´ çš„æ–‡æœ¬å†…å®¹
  
  
  from langchain_community.document_loaders import PyPDFLoader
  loader = PyPDFLoader("./data/paper.pdf")
  docs = loader.load()
  print(docs)
  print(docs[0].page_content)
  
  
  from langchain_community.document_loaders import WikipediaLoader
  loader = WikipediaLoader(query="é¢å’Œå›­", load_max_docs=3, lang="zh")
  docs = loader.load()
  print(docs)
  print(docs[0].page_content)
  
  ```

  



- æ–‡æœ¬åˆ‡æˆå—

  å—å¤šé•¿ï¼Ÿé•¿åº¦å¦‚ä½•è®¡ç®—ï¼Ÿå®¹é”™ï¼ŸAIè¦èƒ½ç†è§£å•ç‹¬çš„ä¸€å—

  [`langchain_text_splitters.character.RecursiveCharacterTextSplitter`](https://api.python.langchain.com/en/latest/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html#langchain_text_splitters.character.RecursiveCharacterTextSplitter) æŒ‡å®šæ ¹æ®ç‰¹å®šç¬¦å·åˆ†å‰²

- ä»£ç å®ç°

  ```python
  from langchain_community.document_loaders import TextLoader
  from langchain_text_splitters import RecursiveCharacterTextSplitter
  
  loader = TextLoader("./data/demo.txt")
  docs = loader.load()
  
  text_splitter = RecursiveCharacterTextSplitter(
      chunk_size=500,
      chunk_overlap=40,
      separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼Œ", "ã€", ""]
  )
  texts = text_splitter.split_documents(docs)
  print(texts)
  print(texts[0].page_content)
  
  ```

  



- åµŒå…¥å‘é‡ [`langchain_community.embeddings.openai.OpenAIEmbeddings`](https://api.python.langchain.com/en/latest/embeddings/langchain_community.embeddings.openai.OpenAIEmbeddings.html#langchain-community-embeddings-openai-openaiembeddings)

  å‘é‡åŒ…å«æ–‡æœ¬ä¹‹é—´çš„è¯­æ³•è¯­ä¹‰ç­‰å…³ç³» (ç›¸ä¼¼æ–‡æœ¬ åœ¨å‘é‡ç©ºé—´ä¸­çš„è·ç¦»æ›´è¿‘)

  åµŒå…¥éœ€è¦å€ŸåŠ©åµŒå…¥æ¨¡å‹ (æ–‡æœ¬ -> å‘é‡) [openai embedding models](https://platform.openai.com/docs/guides/embeddings/embedding-models), baidu

  [Fixing Hallucination with Knowledge Bases](https://www.pinecone.io/learn/series/langchain/langchain-retrieval-augmentation/)

  ![Snipaste_2024-05-14_09-53-24](res/Snipaste_2024-05-14_09-53-24.png)

- ä»£ç å®ç°

  ```python
  import yaml
  from langchain_openai import OpenAIEmbeddings
  
  yaml_file = "../../key/key.yaml"
  with open(yaml_file, 'r') as file:
      data_key = yaml.safe_load(file)
  openai_info = data_key.get('openai-proxy', {})
  openai_api_key = openai_info.get('OPENAI_API_KEY')
  base_url = openai_info.get('BASE_URL')
  
  embeddings_model = OpenAIEmbeddings(
      model="text-embedding-3-large",
      openai_api_key=openai_api_key, openai_api_base=base_url
  )
  
  embeded_result = embeddings_model.embed_documents(["Hello world!", "Hey bro"])
  print(len(embeded_result))  # 2
  print(embeded_result)  # [[-0.00555222607460689, -0.016020740917611947, -0.01469179392791051,..]]
  print(len(embeded_result[0]))  # 3072
  
  # dimensions
  embeddings_model = OpenAIEmbeddings(
      model="text-embedding-3-large", dimensions=1024,
      openai_api_key=openai_api_key, openai_api_base=base_url
  )
  embeded_result = embeddings_model.embed_documents(["Hello world!", "Hey bro"])
  print(len(embeded_result[0]))  # 1024
  
  ```

  



- å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“

  ä¼ ç»Ÿæ•°æ®åº“ï¼šåŸºäº**ç²¾å‡†åŒ¹é…æœºåˆ¶**ï¼Œé€‚åˆæŸ¥è¯¢**ç»“æ„åŒ–ä¿¡æ¯** (é¢„å®šä¹‰æ•°æ®æ¨¡å‹ = å›ºå®šæ ¼å¼ + ç±»å‹æ˜ç¡®)

  å‘é‡æ•°æ®åº“ï¼šåŸºäº**ç›¸ä¼¼æ€§æœç´¢**ï¼Œé€‚åˆæŸ¥è¯¢**éç»“æ„åŒ–æ•°æ®** (æ— å›ºå®šæ ¼å¼ å†…å®¹å¤šæ ·)

  (chroma, `faiss`, weaviate, pinecone) [langchain_community.vectorstores.faiss.FAISS](https://api.python.langchain.com/en/latest/vectorstores/langchain_community.vectorstores.faiss.FAISS.html#langchain_community.vectorstores.faiss.FAISS)

- ä»£ç å®ç°

  ```python
  import yaml
  from langchain_community.document_loaders import TextLoader
  from langchain_community.vectorstores import FAISS
  from langchain_openai.embeddings import OpenAIEmbeddings
  from langchain_text_splitters import RecursiveCharacterTextSplitter
  
  yaml_file = "../../key/key.yaml"
  with open(yaml_file, 'r') as file:
      data_key = yaml.safe_load(file)
  openai_info = data_key.get('openai-proxy', {})
  openai_api_key = openai_info.get('OPENAI_API_KEY')
  base_url = openai_info.get('BASE_URL')
  
  loader = TextLoader("./data/demo2.txt", encoding="utf-8")
  docs = loader.load()
  
  text_splitter = RecursiveCharacterTextSplitter(
      chunk_size=500,
      chunk_overlap=40,
      separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼Œ", "ã€", ""]
  )
  
  texts = text_splitter.split_documents(docs)
  embeddings_model = OpenAIEmbeddings(
      model="text-embedding-3-large",
      openai_api_key=openai_api_key, openai_api_base=base_url
  )
  
  db = FAISS.from_documents(texts, embeddings_model)
  retriever = db.as_retriever()
  
  retrieved_docs = retriever.invoke("å¢æµ®å®«è¿™ä¸ªåå­—æ€ä¹ˆæ¥çš„ï¼Ÿ")
  print("=================================================")
  print(retrieved_docs[0].page_content)
  
  retrieved_docs = retriever.invoke("å¢æµ®å®«åœ¨å“ªå¹´è¢«å‘½åä¸ºä¸­å¤®è‰ºæœ¯åšç‰©é¦†")
  print("=================================================")
  print(retrieved_docs[0].page_content)
  
  ```

  



### å¼€ç®±å³ç”¨çš„RAG

- å¼€ç®±å³ç”¨çš„RAG

  å¸¦è®°å¿†çš„ç´¢å¼•å¢å¼ºç”Ÿæˆçš„å¯¹è¯é“¾ [langchain.chains.conversational_retrieval.base.ConversationalRetrievalChain](https://api.python.langchain.com/en/latest/chains/langchain.chains.conversational_retrieval.base.ConversationalRetrievalChain.html#langchain.chains.conversational_retrieval.base.ConversationalRetrievalChain) 

- ä»£ç å®ç°

  ```python
  
  model = ChatOpenAI(model="gpt-3.5-turbo", openai_api_key=openai_api_key, openai_api_base=base_url)
  memory = ConversationBufferMemory(return_messages=True, memory_key='chat_history', output_key='answer')
  qa = ConversationalRetrievalChain.from_llm(
      llm=model,
      retriever=retriever,
      memory=memory
  )
  
  question = "å¢æµ®å®«è¿™ä¸ªåå­—æ€ä¹ˆæ¥çš„ï¼Ÿ"
  qa.invoke({"chat_history": memory, "question": question})
  
  question = "å¯¹åº”çš„æ‹‰ä¸è¯­æ˜¯ä»€ä¹ˆå‘¢ï¼Ÿ"
  qa.invoke({"chat_history": memory, "question": question})
  
  qa = ConversationalRetrievalChain.from_llm(
      llm=model,
      retriever=retriever,
      memory=memory,
      return_source_documents=True
  )
  
  question = "å¢æµ®å®«è¿™ä¸ªåå­—æ€ä¹ˆæ¥çš„ï¼Ÿ"
  qa.invoke({"chat_history": memory, "question": question})
  
  ```

  



- æŠŠå¤–éƒ¨æ–‡æ¡£å¡ç»™æ¨¡å‹çš„ä¸åŒæ–¹å¼

  `Stuff` å¡«å…… (å…¨éƒ¨ç‰‡æ®µä¼ ç»™æ¨¡å‹ - ä¸é—æ¼ä¿¡æ¯ èŠ±è´¹å¾ˆå¤§)

  `Map-Reduce` æ˜ å°„è§„çº¦ (Mapé˜¶æ®µReduceé˜¶æ®µ - å¯¹å„ä¸ªå°ç­”æ¡ˆçš„æ€»ç»“)

  `Refine` ä¼˜åŒ– (ç»“åˆä¸‹ä¸€ç‰‡æ®µå¯¹åŸæœ‰å›ç­”çš„ä¼˜åŒ– - è¿­ä»£ä¼˜åŒ–)

  `Map-Rerank` æ˜ å°„é‡æ’ (Mapé˜¶æ®µReranké˜¶æ®µ - é€‰å‡ºè¯„åˆ†æœ€é«˜ ä¸ä¼šæ•´åˆä¸åŒç‰‡æ®µé—´çš„ä¿¡æ¯)

- ä»£ç å®ç°

  ```python
  
  model = ChatOpenAI(model="gpt-3.5-turbo", openai_api_key=openai_api_key, openai_api_base=base_url)
  memory = ConversationBufferMemory(return_messages=True, memory_key='chat_history', output_key='answer')
  qa = ConversationalRetrievalChain.from_llm(
      llm=model,
      retriever=retriever,
      memory=memory,
      chain_type="map_reduce"
  )
  qa.invoke({"chat_history": memory, "question": "å¢æµ®å®«è¿™ä¸ªåå­—æ€ä¹ˆæ¥çš„ï¼Ÿ"})
  
  
  memory = ConversationBufferMemory(return_messages=True, memory_key='chat_history', output_key='answer')
  qa = ConversationalRetrievalChain.from_llm(
      llm=model,
      retriever=retriever,
      memory=memory,
      chain_type="refine"
  )
  qa.invoke({"chat_history": memory, "question": "å¢æµ®å®«è¿™ä¸ªåå­—æ€ä¹ˆæ¥çš„ï¼Ÿ"})
  
  
  memory = ConversationBufferMemory(return_messages=True, memory_key='chat_history', output_key='answer')
  qa = ConversationalRetrievalChain.from_llm(
      llm=model,
      retriever=retriever,
      memory=memory,
      chain_type="map_rerank"
  )
  qa.invoke({"chat_history": memory, "question": "å¢æµ®å®«è¿™ä¸ªåå­—æ€ä¹ˆæ¥çš„ï¼Ÿ"})
  
  ```

  









### Project 4 (æ™ºèƒ½PDFé—®ç­”å·¥å…·)











## LangChain Agent

- Agent (ç»™æ¨¡å‹ä½¿ç”¨å·¥å…·çš„èƒ½åŠ›)

  `ReAct`

  è‡ªå®šä¹‰AIå·¥å…·

  ç”¨ç°æˆçš„AIå·¥å…·ï¼šè¿è¡Œä»£ç ã€åˆ†ææ•°æ®è¡¨æ ¼

  å¤šä¸ªå·¥å…·ç»„æˆAIå·¥å…·ç®±

  





### Project 5 (CSVæ•°æ®åˆ†ææ™ºèƒ½å·¥å…·)









## Assistant API

- Assistant API 

  å…³é”®å¯¹è±¡

  ç®€å•åº”ç”¨ï¼šç§äººæ•°å­¦åŠ©æ‰‹ã€PDFæ–‡ä»¶é—®ç­”åŠ©æ‰‹

  



### å…³é”®å¯¹è±¡







### demo ç§äººæ•°å­¦åŠ©æ‰‹







### demo PDFæ–‡ä»¶é—®ç­”åŠ©æ‰‹





## Streamlit

- ç½‘ç«™å¼€å‘

  å¤æ‚æŠ€æœ¯æ ˆï¼šhtml, css, js, ts; [bootstrap](https://v3.bootcss.com/css/), nodejs, vue, react; spring, django, flask

  ç®€å•å®ç°ï¼š[streamlit (å‰ç«¯æ¡†æ¶ + åç«¯æ¡†æ¶ + äº‘æœåŠ¡å™¨)](https://streamlit.io/) 



- å‡†å¤‡ç¯å¢ƒ

  ```bash
  pip install streamlit
  streamlit hello
  
  ```

  

- æ€»è§ˆ

  æ·»åŠ æ–‡æœ¬å›¾ç‰‡è¡¨æ ¼

  æ·»åŠ è¾“å…¥ç»„ä»¶

  è°ƒæ•´ç½‘ç«™å¸ƒå±€å’Œå¢å¼ºå®¹å™¨

  ç®¡ç†ç”¨æˆ·ä¼šè¯çŠ¶æ€

  åˆ›å»ºå¤šé¡µç½‘ç«™

  éƒ¨ç½²åº”ç”¨

- ç‰¹æ€§

  streamlitåœ¨ä¸¤ç§æƒ…å†µä¸‹ä¼šé‡æ–°è¿è¡Œæ•´ä¸ªpyæ–‡ä»¶ (å¯¹æºä»£ç ä¿®æ”¹ ç”¨æˆ·ä¸ç»„ä»¶äº¤äº’)





### åŸºç¡€ç»„ä»¶

- å„ä¸ªç»„ä»¶

  æ·»åŠ æ–‡æœ¬å›¾ç‰‡è¡¨æ ¼

  ```python
  import streamlit as st
  import pandas as pd
  
  """
  cmd: streamlit run page1.py
  """
  
  # show text
  st.title("Streamlit App ğŸ˜‰")
  st.write("### Welcome to the Streamlit App")  # string md
  
  # show variable
  variable = 8080 * 4
  variable
  # show list
  [[1, 0, 0], [0, 1, 0], [0, 0, 1]]
  # show dictionary
  {"name": "John", "age": 30, "city": "New York"}
  
  # show image
  image_path = r"D:\code2\python-code\artificial-intelligence\llm\chapter09-streamlit\data\profile.jpg"
  st.image(image_path, width=200)
  
  # show table
  df = pd.DataFrame(
      {
          "Name": ["John", "Jane", "Bob", "Alice", "Tom"],
          "Age": [30, 25, 40, 35, 28],
          "City": ["New York", "Paris", "London", "Berlin", "Tokyo"],
          "Graduated": ["CMU", "Harvard", "Stanford", "MIT", "Yale"],
          "Gender": ["Male", "Female", "Male", "Female", "Male"],
      }
  )
  st.dataframe(df)  # interactive table
  st.divider()  # horizontal line
  st.table(df)  # static table
  
  ```

  æ·»åŠ è¾“å…¥ç»„ä»¶

  æ–‡å­—è¾“å…¥ã€æ•°å­—è¾“å…¥ã€å‹¾é€‰æ¡†ã€æŒ‰é’®

  ```python
  import streamlit as st
  
  st.title("Welcome to Streamlit App")
  
  # Input text
  name = st.text_input("Enter your name: ")
  password = st.text_input("Enter a keyword: ", type="password")
  
  # Input large text
  paragraph = st.text_area("Please enter a paragraph about yourself: ")
  
  if name and paragraph:
      st.write(f"Hello {name}! Welcome! I've gotten to know you: ")
      st.write(paragraph)
  
  # Input number
  st.divider()
  age = st.number_input(
      "Enter your age: ",
      min_value=8, max_value=150, value=25, step=2
  )
  st.write(f"Your age is {age} years old.")
  
  # Checkbox
  st.divider()
  checked = st.checkbox("I agree to the terms and conditions")
  if checked:
      st.write("Thank you for agreeing to the terms and conditions.")
  
  # Button
  submit = st.button("Submit")
  if submit:
      st.write("Form submitted successfully!")
  
  ```

  å•é€‰æŒ‰é’®ã€å•é€‰æ¡†ã€å¤šé€‰æ¡†ã€æ»‘å—ã€æ–‡ä»¶ä¸Šä¼ å™¨

  ```python
  import streamlit as st
  
  st.title("Welcome to the Streamlit App")
  
  # Radio Button
  st.divider()
  gender = st.radio(
      "What is your gender?",
      ["Male", "Female", "Other"],
      index=0
  )
  if gender == "Male":
      st.write("Welcome, Mr. Smith!")
  elif gender == "Female":
      st.write("Welcome, Ms. Smith!")
  else:
      st.write("Welcome for you!")
  
  # select box
  st.divider()
  contact = st.selectbox(
      "Select your contact method",
      ["Email", "Phone", "Facebook"]
  )
  if contact:
      st.write(f"All right, we will contact you via {contact}")
  
  # multi select
  st.divider()
  interests = st.multiselect(
      "What are your interests?",
      ["Reading", "Hiking", "Traveling", "Cooking"]
  )
  if interests:
      st.write(f"You are interested in {', '.join(interests)}")
  
  # slider
  st.divider()
  height = st.slider(
      "What is your height (cm)?",
      min_value=80, max_value=250, value=170, step=3
  )
  if height:
      st.write(f"You are {height} cm tall")
  
  # file uploader
  st.divider()
  upload_file = st.file_uploader(
      "Please upload your resume (only: pdf, md, txt, py):",
      type=["pdf", "md", "txt", "py"]
  )
  if upload_file:
      st.write(f"Thank you for uploading {upload_file.name}.")
      st.write(f"Preview file content: {upload_file.read()}")
  
  ```

  è°ƒæ•´ç½‘ç«™å¸ƒå±€å’Œå¢å¼ºå®¹å™¨

  ä¾§è¾¹æ ã€åˆ†åˆ—ã€é€‰é¡¹å¡ã€æŠ˜å å±•å¼€

  ```python
  import streamlit as st
  
  # sidebar
  with st.sidebar:
      name = st.text_input("Please enter your name")
      gender = st.radio(
          "Please select your gender",
          ["Secret", "Male", "Female"],
          index=0
      )
  
  if gender == "Male":
      st.title(f"Welcome Mr. {name}! ")
  elif gender == "Female":
      st.title(f"Welcome Mrs. {name}! ")
  else:
      st.title(f"Welcome {name}! ")
  
  # multi-columns
  column1, column2 = st.columns([3, 4])
  with column1:
      st.divider()
      age = st.number_input(
          "Please enter your age",
          min_value=8, max_value=150, value=25, step=1
      )
      st.divider()
      height = st.slider(
          "What is your height (cm)?",
          min_value=80, max_value=250, value=170, step=3
      )
      st.divider()
      interests = st.multiselect(
          "Please select your interests",
          ["Reading", "Hiking", "Traveling", "Cooking"]
      )
  with column2:
      paragraph = st.text_area(
          "Please enter a paragraph about yourself: ",
          height=480,
          value="I am a software engineer with a passion for data science and machine learning..."
      )
  
  # multi-tabs
  tab1, tab2, tab3 = st.tabs(["Movie", "Music", "Sports"])
  with tab1:
      movie = st.multiselect(
          "What is your favorite movie genre?",
          ["Action", "Comedy", "Drama", "Sci-fi"]
      )
  with tab2:
      music = st.multiselect(
          "What is your favorite music genre?",
          ["Pop", "Rock", "Hip-hop", "Jazz"]
      )
  with tab3:
      sport = st.multiselect(
          "What is your favorite sport?",
          ["Basketball", "Football", "Baseball", "Tennis"]
      )
  
  # expander
  with st.expander("Contact Information"):
      email = st.text_input("Please enter your email")
      phone = st.text_input("Please enter your phone number")
      address = st.text_input("Please enter your address")
  
  # check
  st.divider()
  checked = st.checkbox("I agree to the terms and conditions")
  if checked:
      st.write("Thank you for agreeing to the terms and conditions.")
  else:
      st.write("Please agree to the terms and conditions to continue.")
  
  # submit button
  if st.button("Submit"):
      st.success("Thank you for submitting the form!")
  
  ```

  



### ä¼šè¯å’Œå¤šé¡µé¢

- ä¼šè¯çŠ¶æ€å­˜å‚¨å€¼ (ä¸å…³é—­æµè§ˆå™¨çš„æ ‡ç­¾é¡µ)

  ```python
  import streamlit as st
  
  if "a" not in st.session_state:  # dict
      st.session_state.a = 0  # initialize the value of a in session_state
  
  st.title("Welcome to Streamlit App")
  
  a = 0
  clicked = st.button("plus 1")
  if clicked:
      st.session_state.a += 1
  st.write("The value of a is:", st.session_state.a)
  
  ```

- å¤šé¡µé¢

  ```
  ls -R
  .:
  data  index.py  pages
  ./pages:
  demo1.py  demo2.py  demo3.py  demo4.py
  
  
  streamlit run index.py
  
  ```

  



### ç¤¾åŒºéƒ¨ç½²

- éƒ¨ç½²

  localhost

  å…¬ç½‘ip

  

- Streamlitéƒ¨ç½²æµç¨‹ç®€å•

  ```bash
  pip freeze > requirements.txt
  
  # push github
  
  ```

  [streamlit](https://share.streamlit.io/)

  

  



## Further Information

### ChatGPTå®â½¤æŒ‡å—

#### ChatGPTä»‹ç»

- ä»€ä¹ˆæ˜¯ChatGTPT

  ChatGPTæ˜¯â¼€ä¸ªâ¼ˆâ¼¯æ™ºèƒ½é—®ç­”èŠå¤©â¼¯å…·ï¼ŒåŸºäºOpenAIå¼€å‘çš„â¼¤å‹è¯­â¾”æ¨¡å‹GPTï¼Œå¯ä»¥ä¸â½¤æˆ·è¿›â¾â¾ƒç„¶è¯­â¾”äº¤äº’ï¼Œå›ç­”é—®é¢˜ã€æä¾›ä¿¡æ¯ã€è§£å†³é—®é¢˜å’Œæä¾›å»ºè®®ã€‚

  ChatGPTåŸºäºâ¼¤è§„æ¨¡çš„é¢„è®­ç»ƒæ•°æ®é›†è¿›â¾è®­ç»ƒï¼ŒæŒæ¡äº†â¼´æ³›çš„çŸ¥è¯†é¢†åŸŸï¼Œå¹¶èƒ½ç†è§£å’Œâ½£æˆâ¾ƒç„¶è¯­â¾”ã€‚å®ƒå¯ä»¥å¤„ç†å„ç§é—®é¢˜ï¼ŒåŒ…æ‹¬å¸¸â»…çš„ç™¾ç§‘çŸ¥è¯†ã€å®â½¤ä¿¡æ¯ã€æŠ€æœ¯â½€æŒã€åˆ›æ„çµæ„Ÿç­‰ç­‰ã€‚

  æˆ‘ä»¬å¯ä»¥å€ŸåŠ©ChatGPTæ¥â¾¼æ•ˆå­¦ä¹ å’Œè§£ç­”ç–‘æƒ‘ã€‚è™½ç„¶AIçš„èƒ½â¼’è¿˜æ²¡æ³•è¿›â¾ä½“ç³»åŒ–ã€ç³»ç»Ÿæ€§çš„è¯¦ç»†æ•™å­¦ï¼Œä½†æ˜¯â¾®å¸¸é€‚åˆâ½¤æ¥æä¾›ç¢â½šåŒ–ã€å³æ—¶æ€§çš„å¸®åŠ©ã€‚
  



- ä»€ä¹ˆæ˜¯æâ½°â¼¯ç¨‹ï¼Ÿ

  åœ¨å’ŒChatGPTçš„äº¤æµè¿‡ç¨‹ä¸­ï¼Œäº†è§£å¦‚ä½•æœ‰æ•ˆä¸å…¶è¿›â¾æ²Ÿé€šæ˜¯å¾ˆæœ‰â½¤çš„ã€‚æˆ‘ä»¬å’ŒAIçš„æ•´ä¸ªäº¤æµè¿‡ç¨‹ï¼Œéƒ½å›´ç»•ç€ç»™AIå†™â€œæâ½°â€å‘½ä»¤ã€‚
  æˆ‘ä»¬å¯ä»¥æŠŠâ€œæâ½°â¼¯ç¨‹â€å®šä¹‰ä¸ºåˆ›å»ºç»™AIçš„è¾“â¼Šçš„è¿‡ç¨‹ã€‚æâ½°è¾“â¼Šå°†å½±å“AIè¯­â¾”æ¨¡å‹â½£æˆçš„è¾“å‡ºï¼Œå¹¶ä¸”å‘¢ï¼Œâ¾¼è´¨é‡çš„æâ½°è¾“â¼Šå°†äº§â½£æ›´å¥½çš„è¾“å‡ºã€‚
  



- â¼¤è¯­â¾”æ¨¡å‹èƒŒåçš„åŸç†

  â¼¤è¯­â¾”æ¨¡å‹çš„åŸç†æ˜¯é€šè¿‡è®­ç»ƒç¥ç»â½¹ç»œæ¨¡å‹é¢„æµ‹ä¸‹â¼€ä¸ªå•è¯çš„æ¦‚ç‡åˆ†å¸ƒï¼Œå®ç°â½‚æœ¬â½£æˆå’Œç†è§£çš„åŠŸèƒ½ã€‚

  è¿™â¼€åˆ‡æ˜¯é€šè¿‡è®­ç»ƒâ¼¤è§„æ¨¡æ•°æ®é›†æ¥å®ç°çš„ï¼Œæ•°æ®é›†åŒ…æ‹¬â½‚ç« ã€ä¹¦ç±ã€æœŸåˆŠã€æŠ¥å‘Šç­‰ã€‚æ ¹æ®è¯­â¾”æ¨¡å‹çš„ä¸åŒï¼Œæœ‰ä¸¤ç§ä¸»è¦çš„å­¦ä¹ â½…æ³• - ç›‘ç£å­¦ä¹ å’Œâ½†ç›‘ç£å­¦ä¹ ã€‚

  ç›‘ç£å­¦ä¹ æ˜¯æ¨¡å‹ä½¿â½¤å¸¦æœ‰æ­£ç¡®ç­”æ¡ˆæ ‡ç­¾çš„æ ‡è®°æ•°æ®é›†ã€‚â½†ç›‘ç£å­¦ä¹ æ˜¯æ¨¡å‹ä½¿â½¤æœªæ ‡è®°çš„æ•°æ®é›†ï¼Œé‚£ä¹ˆæ¨¡å‹å¿…é¡»åˆ†ææ•°æ®æ¥è·å¾—å‡†ç¡®çš„å›ç­”ã€‚  

  æ¨¡å‹èƒ½å¤Ÿæ ¹æ®ç»™å®šçš„æâ½°â½£æˆâ½‚æœ¬ï¼Œè¿™ä¸ªè¿‡ç¨‹è¢«ç§°ä¸ºè¯­â¾”å»ºæ¨¡ã€‚åœ¨è¿™â¼€ç‚¹ä¸Šï¼ŒAIè¯­â¾”æ¨¡å‹çš„æ€§èƒ½ä¸»è¦å–å†³äºè®­ç»ƒæ•°æ®çš„è´¨é‡å’Œæ•°é‡ã€‚ä½¿â½¤æ¥â¾ƒä¸åŒæ¥æºçš„â¼¤é‡æ•°æ®æ¥è®­ç»ƒæ¨¡å‹å°†æœ‰åŠ©äºæ¨¡å‹ç†è§£â¼ˆç±»è¯­â¾”ï¼ŒåŒ…æ‹¬è¯­æ³•ã€å¥æ³•å’Œè¯­ä¹‰ã€‚

  â¼¤è¯­â¾”æ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹åˆ†ä¸ºä¸¤ä¸ªä¸»è¦æ­¥éª¤ï¼šé¢„è®­ç»ƒå’Œå¾®è°ƒã€‚

  åœ¨é¢„è®­ç»ƒé˜¶æ®µï¼Œâ¼¤è§„æ¨¡çš„â½‚æœ¬æ•°æ®è¢«â½¤æ¥è®­ç»ƒæ¨¡å‹ã€‚è¯¥æ¨¡å‹è¢«è¦æ±‚é¢„æµ‹ç»™å®šä¸Šä¸‹â½‚ä¸­çš„ä¸‹â¼€ä¸ªå•è¯æˆ–å­—ç¬¦ã€‚é€šè¿‡åœ¨â¼¤é‡â½‚æœ¬æ•°æ®ä¸Šè¿›â¾è¿™ç§é¢„æµ‹ä»»åŠ¡ï¼Œæ¨¡å‹å­¦ä¹ åˆ°äº†è¯­â¾”çš„ç»Ÿè®¡è§„å¾‹ã€å¥æ³•ç»“æ„å’Œè¯­ä¹‰å…³ç³»ã€‚

  åœ¨å¾®è°ƒé˜¶æ®µï¼Œä½¿â½¤ç‰¹å®šçš„ä»»åŠ¡æ•°æ®é›†å¯¹é¢„è®­ç»ƒçš„æ¨¡å‹è¿›â¾è¿›â¼€æ­¥çš„è®­ç»ƒï¼Œä»¥ä½¿å…¶é€‚åº”ç‰¹å®šçš„åº”â½¤åœºæ™¯ï¼Œâ½å¦‚è¯´é—®é¢˜å›ç­”ã€â½‚æœ¬â½£æˆã€æœºå™¨ç¿»è¯‘ç­‰ã€‚

  â¼¤è¯­â¾”æ¨¡å‹çš„å…³é”®æ€æƒ³æ˜¯é€šè¿‡ä¸Šä¸‹â½‚ä¿¡æ¯çš„è¾“â¼Šï¼Œä»¥åŠæ¨¡å‹å¯¹è¯­â¾”ç»Ÿè®¡è§„å¾‹çš„ç†è§£ï¼Œâ½£æˆåˆä¹é€»è¾‘å’Œè¿è´¯çš„è¾“å‡ºâ½‚æœ¬ã€‚æ¨¡å‹èƒ½å¤Ÿæ ¹æ®ä¹‹å‰è§‚å¯Ÿåˆ°çš„è¾“â¼Šâ½‚æœ¬â½£æˆæ¥ä¸‹æ¥çš„â½‚æœ¬ï¼Œå¹¶æ ¹æ®ä¸Šä¸‹â½‚è°ƒæ•´â½£æˆçš„è¾“å‡ºã€‚è¿™ç§èƒ½â¼’ä½¿å¾—â¼¤è¯­â¾”æ¨¡å‹å¯ä»¥â½¤äºâ¾ƒåŠ¨â½£æˆâ½‚ç« ã€å›ç­”é—®é¢˜ã€å¯¹è¯äº¤äº’ç­‰å¤šç§â¾ƒç„¶è¯­â¾”å¤„ç†ä»»åŠ¡ã€‚ 

  



- ç›¸å…³æœ¯è¯­

  æâ½°ï¼š ä»»ä½•æä¾›ç»™AIä»¥è·å¾—ç»“æœçš„å†…å®¹ï¼ˆä¹Ÿå°±æ˜¯è¾“â¼Šç»™AIçš„â½‚æœ¬ï¼‰

  ChatGPTï¼š â¼ˆâ¼¯æ™ºèƒ½é—®ç­”èŠå¤©â¼¯å…·

  GPTï¼š ChatGPTèƒŒåçš„â¼¤è¯­â¾”æ¨¡å‹  

  



#### å¦‚ä½•å’ŒChatGPTäº¤æµ

- ä½¿â½¤ChatGPTçš„æ ¸â¼¼

  ç»“æœçš„è´¨é‡å–å†³äºè¾“â¼Šçš„è´¨é‡ã€‚

  



- æâ½°ç»„æˆç»“æ„

  â¾“â¾Š | ä»»åŠ¡ | èƒŒæ™¯ | è¾“å‡º

  â¾“â¾Šï¼š å¸Œæœ›AIæ‰®æ¼”ä»€ä¹ˆâ¾“â¾Šï¼Ÿ

  ä»»åŠ¡ï¼š å¸Œæœ›AIåšä»€ä¹ˆï¼Ÿ

  èƒŒæ™¯ï¼š AIéœ€è¦å“ªäº›ä¿¡æ¯æ‰èƒ½å®Œæˆè¿™â¼€â¾åŠ¨ï¼Ÿåœ¨è¿™â¾¥æŠŠå…·ä½“ä¿¡æ¯ç»™å®ƒã€‚

  è¾“å‡ºï¼š å¸Œæœ›AIè¾“å‡ºçš„æ ¼å¼æ˜¯ä»€ä¹ˆï¼Ÿ  



- Examle 1

  > - â¾“â¾Šï¼šä½ æ˜¯â¼€ä½ç»éªŒä¸°å¯Œçš„å¸‚åœºä¸“å‘˜ï¼Œæ“…â»“ä¸ºå„ä¸ªâ¾ä¸šå’Œå¸‚åœºåˆ›å»ºâ½¤æˆ·æ•…äº‹åœ°å›¾ã€‚
  >
  > - ä»»åŠ¡ï¼šä»¥è¡¨æ ¼å½¢å¼åˆ›å»ºâ¼€ä¸ªç±»ä¼¼äº[æŸä¸ªå…·ä½“äº§å“]çš„äº§å“çš„â½°ä¾‹â½¤æˆ·æ•…äº‹åœ°å›¾ã€‚
  >
  > - èƒŒæ™¯ï¼š
  >
  >   äº§å“æˆ–â½¹ç«™ç±»å‹ï¼š[æä¾›å¯¹äº§å“æˆ–â½¹ç«™çš„æè¿°ï¼ŒåŒ…æ‹¬å…¶ä¸»è¦ç‰¹ç‚¹ã€åŠŸèƒ½ã€â½¬æ ‡å—ä¼—å’Œä»·å€¼ä¸»å¼ ã€‚]
  >
  >   â¾ä¸šï¼š[ç¡®å®šäº§å“æˆ–â½¹ç«™æ‰€åœ¨çš„â¾ä¸šæˆ–å¸‚åœºç»†åˆ†ï¼Œå¹¶æŒ‡å‡ºä»»ä½•å…³é”®è¶‹åŠ¿æˆ–æŒ‘æˆ˜ã€‚]
  >
  > - è¾“å‡ºï¼šåˆ›å»ºâ¼€ä¸ªè¡¨æ ¼å½¢å¼çš„é¡¾å®¢æ—…ç¨‹åœ°å›¾ï¼ŒåŒ…æ‹¬é˜¶æ®µã€ä»»åŠ¡ã€â½¤æˆ·éœ€æ±‚å’Œâ½¤æˆ·â½¬æ ‡ï¼Œä¸äº§å“æˆ–â½¹ç«™çš„æ•´ä½“â½¤æˆ·ä½“éªŒç›¸åŒ¹é…ã€‚  

- Examle 2

  > - â¾“â¾Šï¼šä½ æ˜¯â¼€ä½ç†Ÿç»ƒæ’°å†™äº§å“éœ€æ±‚â½‚æ¡£ï¼ˆPRDï¼‰çš„äº§å“ç»ç†ã€‚
  >
  > - ä»»åŠ¡ï¼šæ ¹æ®æä¾›çš„ä¿¡æ¯æ’°å†™â¼€ä»½å…¨â¾¯çš„äº§å“éœ€æ±‚â½‚æ¡£ï¼ˆPRDï¼‰ã€‚
  >
  > - èƒŒæ™¯ï¼š
  >
  >   ä¸šåŠ¡â½¬æ ‡ï¼š[æè¿°ä¸æ­¤äº§å“æˆ–åŠŸèƒ½ç›¸å…³çš„ä¸šåŠ¡â½¬æ ‡ã€‚]
  >
  >   äº§å“æ„¿æ™¯å’Œæˆ˜ç•¥ï¼š[è§£é‡Šäº§å“æˆ–åŠŸèƒ½çš„æ•´ä½“æ„¿æ™¯å’Œæˆ˜ç•¥ï¼ŒåŒ…æ‹¬å…¶â½¬çš„ã€â½¬æ ‡å—ä¼—å’Œç‹¬ç‰¹å–ç‚¹ã€‚]
  >
  >   å…³é”®ç‰¹ç‚¹å’ŒåŠŸèƒ½ï¼š[æä¾›åº”åŒ…å«åœ¨äº§å“æˆ–åŠŸèƒ½ä¸­çš„å…³é”®ç‰¹ç‚¹å’ŒåŠŸèƒ½åˆ—è¡¨ã€‚]  
  >
  >   æŠ€æœ¯ç»†èŠ‚ï¼š[åŒ…æ‹¬ä¸äº§å“æˆ–åŠŸèƒ½ç›¸å…³çš„ä»»ä½•â¾¼çº§æŠ€æœ¯ç»†èŠ‚ï¼Œä¾‹å¦‚å¹³å°ã€æŠ€æœ¯é›†æˆã€é™åˆ¶ç­‰ã€‚]
  >
  >   æ—¶é—´å®‰æ’ï¼š[â¼¤è‡´è¯´æ˜äº§å“æˆ–åŠŸèƒ½çš„å¼€å‘å’Œå‘å¸ƒé¢„æœŸæ—¶é—´ã€‚]
  >
  >   æˆåŠŸæŒ‡æ ‡ï¼š[æ¦‚è¿°â½¤äºè¡¡é‡äº§å“æˆ–åŠŸèƒ½æˆåŠŸçš„æŒ‡æ ‡ã€‚]
  >
  > - è¾“å‡ºï¼š
  >
  >   æŒ‰ç…§ä»¥ä¸‹éƒ¨åˆ†æ„å»ºPRDï¼š
  >
  >   é—®é¢˜
  >
  >   è§£å†³â½…æ³•
  >
  >   äº§å“æ¦‚è§ˆ
  >
  >   åŠŸèƒ½æ€§éœ€æ±‚
  >
  >   â¾®åŠŸèƒ½æ€§éœ€æ±‚
  >
  >   è§£å†³â½…æ¡ˆå¯¹â»¬
  >
  >   å…³é”®åŠŸèƒ½ç‚¹
  >
  >   æœªæ¥è€ƒè™‘äº‹é¡¹
  >
  >   å…³é”®é€»è¾‘  

  



- åˆ›å»ºå¥½çš„æâ½°çš„ç­–ç•¥

  æ¸…æ¥šå®šä¹‰â½¬æ ‡ï¼š æŠŠé—®é¢˜è¾“â¼Šç»™ChatGPTä¹‹å‰ï¼Œæ˜ç¡®è¦å®ç°çš„â½¬æ ‡ã€‚å¸Œæœ›ä»AIè·å¾—çš„ä¿¡æ¯æ˜¯ä»€ä¹ˆï¼Ÿ

  ä¿æŒå…·ä½“å’Œé›†ä¸­ï¼š ChatGPTæ›´æ“…â»“å›ç­”å…·ä½“é—®é¢˜ï¼Œæ‰€ä»¥æœ€å¥½è®©é—®é¢˜æ›´åŠ è¯¦ç»†ã€å…·ä½“ã€é›†ä¸­ã€‚ä¸è¦é—®è¿‡äºâ¼´æ³›æˆ–æ¨¡ç³Šçš„é—®é¢˜ï¼Œæé—®â½…å¼ä¹Ÿæœ€å¥½æ¸…æ™°ç®€æ´ã€‚

  ä½¿â½¤â¾ƒç„¶è¯­â¾”ï¼š GPTæ¨¡å‹æ—¨åœ¨ç†è§£å’Œâ½£æˆâ¾ƒç„¶è¯­â¾”ï¼Œå› æ­¤æé—®æ—¶ä¹Ÿè¦ä½¿â½¤â¾ƒç„¶è¯­â¾”ã€‚é¿å…ä½¿â½¤æ¨¡å‹éš¾ä»¥ç†è§£çš„è¯˜å±ˆè±â½›çš„è¡¨è¾¾ã€‚  

  æä¾›ä¸Šä¸‹â½‚ï¼š ChatGPTåœ¨æœ‰ä¸Šä¸‹â½‚çš„æƒ…å†µä¸‹æ•ˆæœæ›´å¥½ï¼Œå› æ­¤æé—®æ—¶å°½é‡æä¾›â¼€äº›ä¸Šä¸‹â½‚ï¼Œâ½å¦‚èƒŒæ™¯ä¿¡æ¯æˆ–è§£é‡Šé—®é¢˜çš„è¡¥å……ä¿¡æ¯ã€‚

  æµ‹è¯•å’Œå®Œå–„ï¼š å¯ä»¥å°è¯•ä¸åŒç±»å‹çš„é—®é¢˜ã€ä¸åŒçš„é—®æ³•ï¼Œçœ‹çœ‹ChatGPTçš„ååº”ã€‚æœ‰çš„æ—¶å€™ç­”æ¡ˆçš„è´¨é‡å’Œå‡†ç¡®æ€§å¯èƒ½ä¸å°½â¼ˆæ„ï¼Œè¿™ä¸ªæ—¶å€™å¯ä»¥ç»™å®ƒæä¾›â¼€äº›åé¦ˆï¼Œæ¥å®Œå–„æâ½°â¾¥çš„è¦æ±‚ï¼Œæâ¾¼ChatGPTçš„å›ç­”è´¨é‡ã€‚  

  



- 





#### ä½¿â½¤ChatGPTçš„æ›´å¤šæŠ€å·§  









### â¼¤æ¨¡å‹äº§å“å¼€å‘æµç¨‹æ¸…å•

- ä»¥ä¸‹æ˜¯ä¸ªâ¼ˆå¼€å‘è€…çš„â¼¤è¯­â¾”æ¨¡å‹ (LLM) äº§å“çš„å¼€å‘æµç¨‹å‚è€ƒã€‚  

- å‡†å¤‡â¼¯ä½œ

  è§„åˆ’é¡¹â½¬â½¬æ ‡ä¸æ ¸å¿ƒåŠŸèƒ½

  è¿›â¾æŠ€æœ¯è°ƒç ”ç¡®è®¤æŠ€æœ¯æ ˆ

  â¼¤æ¨¡å‹

  å‘é‡æ•°æ®åº“

  åç«¯æ¡†æ¶

  å‰ç«¯æ¡†æ¶

- æ„å»ºçŸ¥è¯†åº“ç´¢å¼•

  æ”¶é›†æ•°æ®

  æ•°æ®å­˜â¼ŠçŸ¥è¯†åº“

  åŠ è½½æ•°æ®

  è¯»å–æ•°æ®

  â½‚æœ¬åˆ†å‰²

  â½‚æœ¬åµŒâ¼Š

  å­˜â¼Šå‘é‡æ•°æ®åº“

- å®šåˆ¶â¼¤æ¨¡å‹

  åˆ›å»ºâ¼¤æ¨¡å‹APIå¯†é’¥

  å®ç°â¼¤æ¨¡å‹å¯¹è¯äº’åŠ¨

  é€šè¿‡æâ½°â¼¯ç¨‹ä¼˜åŒ–â¼¤æ¨¡å‹

  é€šè¿‡çŸ¥è¯†åº“å®ç°å®šåˆ¶åŒ–é—®ç­”

  æ·»åŠ è®°å¿†ï¼Œå®ç°å†å²å¯¹è¯æ¶ˆæ¯è®°å½•

  åˆ©â½¤Agentï¼Œå®ç°æ›´å¤šå®šåˆ¶åŒ–åŠŸèƒ½  

- â½¤æˆ·äº¤äº’ç•Œâ¾¯å¼€å‘

  è®¾è®¡â½¤æˆ·äº¤äº’ç•Œâ¾¯

  åˆ©â½¤Streamlitã€Reactç­‰å‰ç«¯æ¡†æ¶æ­å»ºâ½¤æˆ·äº¤äº’ç•Œâ¾¯

- æµ‹è¯•ä¸éƒ¨ç½²ä¸Šçº¿

  è¿›â¾äº§å“æµ‹è¯•

  éƒ¨ç½²äº§å“åˆ°æœ¬åœ°æœåŠ¡å™¨æˆ–äº‘æœåŠ¡å™¨

  æ£€æŸ¥â½¤æˆ·å¯è®¿é—®æ€§

- ç›‘æ§ç»“æœ

  è·Ÿè¸ªâ½¤æˆ·å‚ä¸åº¦å¹¶æ”¶é›†æ•°æ®

  æ ¹æ®æ•°æ®ç»“æœå’Œåé¦ˆï¼Œè¿›â¾è¿­ä»£å’Œæ”¹è¿›  

  



- ä»¥ä¸‹æ˜¯ç»„ç»‡/å•†â½¤çº§åˆ«çš„â¼¤è¯­â¾”æ¨¡å‹ (LLM) äº§å“å¼€å‘æµç¨‹å‚è€ƒã€‚

- å‡†å¤‡â¼¯ä½œ

  ä¸é€‰æ‹©çš„â¼¤æ¨¡å‹æä¾›å•†ï¼ˆâ½å¦‚OpenAIã€ç™¾åº¦ç­‰ï¼‰æ²Ÿé€šå•†è®®ï¼Œæˆ–ç‹¬â½´åˆ¶å®šå‡ºäº§å“â½¬æ ‡

  æ”¶é›†â¼¤æ¨¡å‹è®­ç»ƒè¿‡ç¨‹æ‰€éœ€çš„èµ„æºå’Œæ•°æ®

  è€ƒè™‘æ•°æ®å±€é™æ€§å’Œéšç§é—®é¢˜

  ç¡®å®šå…³é”®åˆ©ç›Šç›¸å…³è€…ï¼šCEOã€CTOã€äº§å“ç»ç†ã€æ•°æ®â¼¯ç¨‹å¸ˆã€æ³•å¾‹å›¢é˜Ÿç­‰

- å®šåˆ¶â¼¤æ¨¡å‹

  ä¸â¼¤æ¨¡å‹æä¾›å•†æ²Ÿé€šå•†è®®ï¼Œé€‰æ‹©åˆé€‚çš„è¯­â¾”æ¨¡å‹

  å®šä¹‰ä»è¾“â¼Šåˆ°è¾“å‡ºçš„â½¤æˆ·ä½¿â½¤æµç¨‹

  ç­–åˆ’å’Œå‡†å¤‡æ•°æ®ï¼Œç¡®ä¿æ•°æ®å®‰å…¨å’Œéšç§

  é€šè¿‡æâ½°â¼¯ç¨‹ã€å¢å¼ºç´¢å¼•â½£æˆç­‰â½…å¼ï¼Œè¿›â¼€æ­¥å®šåˆ¶â¼¤æ¨¡å‹

  ç»†åŒ–æ¨¡å‹å“åº”å¹¶è¯„ä¼°æ€§èƒ½  

- æ¨¡å‹éƒ¨ç½²ä¸é›†æˆ

  ç¡®å®šæ¨¡å‹éƒ¨ç½²â½…æ³•ï¼šAPIã€SDKæˆ–äº‘æœåŠ¡å™¨

  å°†â¼¤æ¨¡å‹é›†æˆåˆ°å¹³å°ä¸­

  å¦‚æœä½¿â½¤ç¬¬ä¸‰â½…å¹³å°ï¼Œâ½å¦‚äºšâ»¢é€ŠSageMakerç­‰ï¼Œéœ€è¦ç¡®ä¿å…¼å®¹æ€§

  åœ¨å‘å¸ƒå‰è¿›â¾å…¨â¾¯çš„æµ‹è¯•

- ç›‘æ§ç»“æœ

  è·Ÿè¸ªâ½¤æˆ·å‚ä¸åº¦å¹¶æ”¶é›†åé¦ˆ

  åˆ†æâ¼¤æ¨¡å‹å¦‚ä½•å½±å“ä¸šåŠ¡ KPI

  æ ¹æ®åé¦ˆå’Œç»“æœï¼Œè¿›â¾è¿­ä»£å’Œæ”¹è¿› 

  





### Paper

- Transformer

  ChatGPT ä½¿â½¤çš„é¢„è®­ç»ƒæ¨¡å‹ GPTï¼Œæ˜¯åœ¨ Transformer ä¸­çš„ decoder åŸºç¡€ä¸Šè¿›â¾æ”¹é€ çš„ã€‚

  è®ºâ½‚æ ‡é¢˜ï¼šAttention Is All You Need

  è®ºâ½‚é“¾æ¥ï¼šhttps://arxiv.org/pdf/1706.03762.pdf

  æ‘˜è¦ï¼šå ä¸»å¯¼åœ°ä½çš„åºåˆ—è½¬å¯¼æ¨¡å‹æ˜¯åŸºäºå¤æ‚çš„é€’å½’æˆ–å·ç§¯ç¥ç»â½¹ç»œï¼ŒåŒ…æ‹¬â¼€ä¸ªç¼–ç å™¨å’Œâ¼€ä¸ªè§£ç å™¨ã€‚æ€§èƒ½æœ€å¥½çš„æ¨¡å‹è¿˜é€šè¿‡æ³¨æ„æœºåˆ¶å°†ç¼–ç å™¨å’Œè§£ç å™¨è¿æ¥èµ·æ¥ã€‚æˆ‘ä»¬æå‡ºäº†â¼€ä¸ªæ–°çš„ç®€å•çš„â½¹ç»œç»“æ„â€“Transformerï¼Œå®ƒåªåŸºäºæ³¨æ„â¼’æœºåˆ¶ï¼Œå®Œå…¨ä¸éœ€è¦é€’å½’å’Œå·ç§¯ã€‚åœ¨ä¸¤ä¸ªæœºå™¨ç¿»è¯‘ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¿™äº›æ¨¡å‹åœ¨è´¨é‡ä¸Šæ›´èƒœâ¼€ç­¹ï¼ŒåŒæ—¶ä¹Ÿæ›´å®¹æ˜“å¹¶â¾åŒ–ï¼Œéœ€è¦çš„è®­ç»ƒæ—¶é—´ä¹Ÿâ¼¤â¼¤å‡å°‘ã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨WMT 2014è‹±å¾·ç¿»è¯‘ä»»åŠ¡ä¸­è¾¾åˆ°äº†28.4BLEUï¼Œâ½ç°æœ‰çš„æœ€ä½³ç»“æœï¼ˆåŒ…æ‹¬åˆé›†ï¼‰æâ¾¼äº†2 BLEUä»¥ä¸Šã€‚åœ¨WMT 2014è‹±æ³•ç¿»è¯‘ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨8ä¸ªGPUä¸Šè®­ç»ƒäº†3.5å¤©åï¼Œå»ºâ½´äº†æ–°çš„å•æ¨¡å‹æœ€å…ˆè¿›çš„BLEUå¾—åˆ†ï¼Œå³41.0åˆ†ï¼Œè¿™åªæ˜¯â½‚çŒ®ä¸­æœ€ä½³æ¨¡å‹çš„è®­ç»ƒæˆæœ¬çš„â¼€â¼©éƒ¨åˆ†ã€‚
  



- GPT-3

  GPT å®¶æ—ä¸ BERT æ¨¡å‹éƒ½æ˜¯çŸ¥åçš„ NLP é¢„è®­ç»ƒæ¨¡å‹ï¼Œéƒ½åŸºäº Transformer æŠ€æœ¯ã€‚GPT-1åªæœ‰12ä¸ª Transformer å±‚ï¼Œâ½½åˆ°äº† GPT-3ï¼Œåˆ™å¢åŠ åˆ° 96 å±‚ã€‚

  è®ºâ½‚æ ‡é¢˜ï¼šLanguage Models are Few-Shot Learners

  è®ºâ½‚é“¾æ¥ï¼šhttps://arxiv.org/pdf/2005.14165.pdf

  æ‘˜è¦ï¼šæœ€è¿‘çš„â¼¯ä½œè¡¨æ˜ï¼Œåœ¨è®¸å¤šNLPä»»åŠ¡å’ŒåŸºå‡†ä¸Šï¼Œé€šè¿‡å¯¹â¼¤å‹â½‚æœ¬è¯­æ–™åº“è¿›â¾é¢„è®­ç»ƒï¼Œç„¶åå¯¹ç‰¹å®šçš„ä»»åŠ¡è¿›â¾å¾®è°ƒï¼Œå¯ä»¥è·å¾—å·¨â¼¤çš„æ”¶ç›Šã€‚è™½ç„¶åœ¨ç»“æ„ä¸Šé€šå¸¸æ˜¯ä»»åŠ¡â½†å…³çš„ï¼Œä½†è¿™ç§â½…æ³•ä»ç„¶éœ€è¦ç‰¹å®šä»»åŠ¡çš„å¾®è°ƒæ•°æ®é›†ï¼ŒåŒ…æ‹¬â¼åƒæˆ–â¼ä¸‡ä¸ªä¾‹â¼¦ã€‚ç›¸â½ä¹‹ä¸‹ï¼Œâ¼ˆç±»é€šå¸¸åªéœ€é€šè¿‡â¼ä¸ªä¾‹â¼¦æˆ–ç®€å•çš„æŒ‡ä»¤å°±èƒ½å®Œæˆâ¼€é¡¹æ–°çš„è¯­â¾”ä»»åŠ¡â€“â½½â½¬å‰çš„NLPç³»ç»Ÿåœ¨å¾ˆâ¼¤ç¨‹åº¦ä¸Šä»éš¾ä»¥åšåˆ°è¿™â¼€ç‚¹ã€‚åœ¨è¿™â¾¥ï¼Œæˆ‘ä»¬å±•â½°äº†æ‰©â¼¤è¯­â¾”æ¨¡å‹çš„è§„æ¨¡ï¼Œâ¼¤â¼¤æ”¹å–„äº†ä¸ä»»åŠ¡â½†å…³çš„ã€å°‘é‡çš„æ€§èƒ½ï¼Œæœ‰æ—¶ç”šâ¾„è¾¾åˆ°äº†ä¸ä¹‹å‰æœ€å…ˆè¿›çš„å¾®è°ƒâ½…æ³•çš„ç«äº‰â¼’ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è®­ç»ƒäº†GPT-3ï¼Œâ¼€ä¸ªå…·æœ‰1750äº¿ä¸ªå‚æ•°çš„â¾ƒå›å½’è¯­â¾”æ¨¡å‹ï¼Œâ½ä»¥å‰çš„ä»»ä½•â¾®ç¨€ç–è¯­â¾”æ¨¡å‹å¤š10å€ï¼Œå¹¶æµ‹è¯•äº†å®ƒåœ¨å°‘æ•°æƒ…å†µä¸‹çš„æ€§èƒ½ã€‚å¯¹äºæ‰€æœ‰çš„ä»»åŠ¡ï¼ŒGPT-3çš„åº”â½¤æ²¡æœ‰ä»»ä½•æ¢¯åº¦æ›´æ–°æˆ–å¾®è°ƒï¼Œçº¯ç²¹é€šè¿‡ä¸æ¨¡å‹çš„â½‚æœ¬äº’åŠ¨æ¥æŒ‡å®šä»»åŠ¡å’Œå°‘é‡æ¼”â½°ã€‚GPT-3åœ¨è®¸å¤šNLPæ•°æ®é›†ä¸Šå–å¾—äº†å¼ºâ¼¤çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬ç¿»è¯‘ã€å›ç­”é—®é¢˜å’Œclozeä»»åŠ¡ï¼Œä»¥åŠâ¼€äº›éœ€è¦å³æ—¶æ¨ç†æˆ–é¢†åŸŸé€‚åº”çš„ä»»åŠ¡ï¼Œå¦‚è§£è¯»å•è¯ã€åœ¨å¥â¼¦ä¸­ä½¿â½¤â¼€ä¸ªæ–°è¯æˆ–è¿›â¾3ä½æ•°çš„ç®—æœ¯ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬ä¹Ÿå‘ç°äº†â¼€äº›æ•°æ®é›†ï¼Œåœ¨è¿™äº›æ•°æ®é›†ä¸­ï¼ŒGPT-3çš„â¼ç‡å­¦ä¹ ä»ç„¶å¾ˆå›°éš¾ï¼Œè¿˜æœ‰â¼€äº›æ•°æ®é›†ï¼ŒGPT-3â¾¯ä¸´ç€ä¸â¼¤å‹â½¹ç»œè¯­æ–™åº“è®­ç»ƒæœ‰å…³çš„â½…æ³•å­¦é—®é¢˜ã€‚æœ€åï¼Œæˆ‘ä»¬å‘ç°ï¼ŒGPT-3å¯ä»¥â½£æˆâ¼ˆç±»è¯„ä»·è€…éš¾ä»¥åŒºåˆ†çš„æ–°é—»â½‚ç« æ ·æœ¬ã€‚æˆ‘ä»¬è®¨è®ºäº†è¿™â¼€å‘ç°å’ŒGPT-3æ€»ä½“ä¸Šçš„æ›´â¼´æ³›çš„ç¤¾ä¼šå½±å“ã€‚

  



- InstructGPT

  ChatGPT çš„è®­ç»ƒæµç¨‹ï¼Œä¸»è¦å‚è€ƒâ¾ƒ instructGPT ï¼ŒChatGPT æ˜¯æ”¹è¿›çš„ instructGPTã€‚

  è®ºâ½‚æ ‡é¢˜ï¼šTraining language models to follow instructions with human feedback

  è®ºâ½‚é“¾æ¥ï¼šhttps://arxiv.org/pdf/2203.02155.pdf

  æ‘˜è¦ï¼šè®©è¯­â¾”æ¨¡å‹å˜å¾—æ›´â¼¤å¹¶ä¸æ„å‘³ç€å®ƒä»¬èƒ½æ›´å¥½åœ°éµå¾ªâ½¤æˆ·çš„æ„å›¾ã€‚ä¾‹å¦‚ï¼Œâ¼¤å‹è¯­â¾”æ¨¡å‹å¯ä»¥äº§â½£ä¸çœŸå®çš„ã€æœ‰æ¯’çš„æˆ–æ ¹æœ¬å¯¹â½¤æˆ·æ²¡æœ‰å¸®åŠ©çš„è¾“å‡ºã€‚æ¢å¥è¯è¯´ï¼Œè¿™äº›æ¨¡å‹æ²¡æœ‰ä¸â½¤æˆ·ä¿æŒâ¼€è‡´ã€‚åœ¨æœ¬â½‚ä¸­ï¼Œæˆ‘ä»¬å±•â½°äº†â¼€ä¸ªé€”å¾„ï¼Œé€šè¿‡â¼ˆç±»åé¦ˆçš„å¾®è°ƒï¼Œåœ¨â¼´æ³›çš„ä»»åŠ¡ä¸­ä½¿è¯­â¾”æ¨¡å‹ä¸â½¤æˆ·çš„æ„å›¾ä¿æŒâ¼€è‡´ã€‚ä»â¼€ç»„æ ‡ç­¾å‘˜å†™çš„æâ½°è¯­å’Œé€šè¿‡OpenAI APIæäº¤çš„æâ½°è¯­å¼€å§‹ï¼Œæˆ‘ä»¬æ”¶é›†äº†â¼€ç»„æ ‡ç­¾å‘˜æ¼”â½°çš„æ‰€éœ€æ¨¡å‹â¾ä¸ºçš„æ•°æ®é›†ï¼Œæˆ‘ä»¬åˆ©â½¤ç›‘ç£å­¦ä¹ å¯¹GPT-3è¿›â¾å¾®è°ƒã€‚ç„¶åï¼Œæˆ‘ä»¬æ”¶é›†æ¨¡å‹è¾“å‡ºçš„æ’åæ•°æ®é›†ï¼Œæˆ‘ä»¬åˆ©â½¤â¼ˆç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ æ¥è¿›â¼€æ­¥å¾®è°ƒè¿™ä¸ªç›‘ç£æ¨¡å‹ã€‚æˆ‘ä»¬æŠŠäº§â½£çš„æ¨¡å‹ç§°ä¸ºInstructGPTã€‚åœ¨â¼ˆç±»å¯¹æˆ‘ä»¬çš„æâ½°åˆ†å¸ƒçš„è¯„ä¼°ä¸­ï¼Œå°½ç®¡å‚æ•°å°‘äº†100å€ï¼Œä½†1.3Bå‚æ•°çš„InstructGPTæ¨¡å‹çš„è¾“å‡ºâ½175Bçš„GPT-3çš„è¾“å‡ºæ›´å—æ¬¢è¿ã€‚æ­¤å¤–ï¼ŒInstructGPTæ¨¡å‹æ˜¾â½°äº†çœŸå®æ€§çš„æ”¹å–„å’Œæœ‰æ¯’è¾“å‡ºâ½£æˆçš„å‡å°‘ï¼ŒåŒæ—¶åœ¨å…¬å…±NLPæ•°æ®é›†ä¸Šçš„æ€§èƒ½å›å½’æœ€â¼©ã€‚å°½ç®¡InstructGPTä»ç„¶ä¼šçŠ¯â¼€äº›ç®€å•çš„é”™è¯¯ï¼Œä½†æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œåˆ©â½¤â¼ˆç±»åé¦ˆè¿›â¾å¾®è°ƒæ˜¯ä½¿è¯­â¾”æ¨¡å‹ä¸â¼ˆç±»æ„å›¾ç›¸â¼€è‡´çš„â¼€ä¸ªæœ‰å¸Œæœ›çš„â½…å‘ã€‚  

  



# LLM (myself)

## Ollama

- é…ç½®ç¯å¢ƒå˜é‡

  `OLLAMA_MODELS` - `D:\ollma`



## claude









## xunfei 









## kimi







## zhipuAI









## baidu

- API

  ```python
  import erniebot
  
  response = erniebot.ChatCompletion.create(
      model="ernie-3.5",
      messages=[
          {
              "role": "user",
              "content": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚"
          }
      ]
  )
  print(response.get_result())
  
  ```

  









# Streamlit Introduction













# Streamlit Tutorials





















# Build 12 Data Science Apps (Streamlit)

















































