{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "yaml_file = \"../../key/key.yaml\"\n",
    "with open(yaml_file, 'r') as file:\n",
    "    data_key = yaml.safe_load(file)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T02:30:55.578008Z",
     "start_time": "2024-04-03T02:30:55.562404600Z"
    }
   },
   "id": "1cb4b6a35e4e34c7"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-03T01:18:38.702212700Z",
     "start_time": "2024-04-03T01:18:26.897043600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"云在青天水在瓶\"是明朝嘉靖皇帝的一句名言，这句话的字面意思是描述云在天空中，水在瓶子里，这是事物的本来面貌。然而，这句话的深层含义则是嘉靖皇帝对于国家治理的一种理念。\n",
      "\n",
      "嘉靖皇帝把国事分成了不同的“摊子”交给他的大臣们负责。他认为，就像云在天空，水在瓶子，每个人都有自己的职责和使命，各司其职，各负其责。这种理念与法家的帝王术有相似之处，即通过分散权力，达到治理国家的目的。\n",
      "\n",
      "同时，这句话也反映了嘉靖皇帝的一种人生哲学。他认为，人们应该像云在天空，水在瓶子一样，认清自己的本分，理解自己的处境，从而更好地适应生活。\n",
      "\n",
      "总的来说，\"云在青天水在瓶\"这句话既表达了嘉靖皇帝的国家治理理念，也揭示了他的人生哲学。\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from langchain_community.llms import SparkLLM\n",
    "\n",
    "spark_info = data_key.get(\"spark\", {})\n",
    "os.environ[\"IFLYTEK_SPARK_APP_ID\"] = spark_info.get('APPID')\n",
    "os.environ[\"IFLYTEK_SPARK_API_SECRET\"] = spark_info.get('APISecret')\n",
    "os.environ[\"IFLYTEK_SPARK_API_KEY\"] = spark_info.get('APIKey')\n",
    "\n",
    "llm_spark = SparkLLM()\n",
    "res = llm_spark.invoke(\"嘉靖有一句话：云在青天水在瓶，你觉得是什么意思?\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[12], line 22\u001B[0m\n\u001B[0;32m     12\u001B[0m chat \u001B[38;5;241m=\u001B[39m ChatZhipuAI(\n\u001B[0;32m     13\u001B[0m     api_key\u001B[38;5;241m=\u001B[39mzhipuai_api_key,\n\u001B[0;32m     14\u001B[0m     model\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mglm-4\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     15\u001B[0m     temperature\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.5\u001B[39m,\n\u001B[0;32m     16\u001B[0m )\n\u001B[0;32m     17\u001B[0m messages \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m     18\u001B[0m     AIMessage(content\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mHi.\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[0;32m     19\u001B[0m     SystemMessage(content\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYour role is a poet.\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[0;32m     20\u001B[0m     HumanMessage(content\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWrite a short poem about AI in four lines.\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[0;32m     21\u001B[0m ]\n\u001B[1;32m---> 22\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[43mchat\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     23\u001B[0m \u001B[38;5;28mprint\u001B[39m(response\u001B[38;5;241m.\u001B[39mcontent)  \u001B[38;5;66;03m# Displays the AI-generated poem\u001B[39;00m\n",
      "File \u001B[1;32mD:\\systemEnvironment\\Anaconda3\\envs\\llm\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:145\u001B[0m, in \u001B[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    143\u001B[0m     warned \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m    144\u001B[0m     emit_warning()\n\u001B[1;32m--> 145\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m wrapped(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\systemEnvironment\\Anaconda3\\envs\\llm\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:798\u001B[0m, in \u001B[0;36mBaseChatModel.__call__\u001B[1;34m(self, messages, stop, callbacks, **kwargs)\u001B[0m\n\u001B[0;32m    790\u001B[0m \u001B[38;5;129m@deprecated\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m0.1.7\u001B[39m\u001B[38;5;124m\"\u001B[39m, alternative\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minvoke\u001B[39m\u001B[38;5;124m\"\u001B[39m, removal\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m0.2.0\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    791\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\n\u001B[0;32m    792\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    796\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[0;32m    797\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m BaseMessage:\n\u001B[1;32m--> 798\u001B[0m     generation \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgenerate(\n\u001B[0;32m    799\u001B[0m         [messages], stop\u001B[38;5;241m=\u001B[39mstop, callbacks\u001B[38;5;241m=\u001B[39mcallbacks, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m    800\u001B[0m     )\u001B[38;5;241m.\u001B[39mgenerations[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m    801\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(generation, ChatGeneration):\n\u001B[0;32m    802\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m generation\u001B[38;5;241m.\u001B[39mmessage\n",
      "File \u001B[1;32mD:\\systemEnvironment\\Anaconda3\\envs\\llm\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:415\u001B[0m, in \u001B[0;36mBaseChatModel.generate\u001B[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001B[0m\n\u001B[0;32m    413\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m run_managers:\n\u001B[0;32m    414\u001B[0m             run_managers[i]\u001B[38;5;241m.\u001B[39mon_llm_error(e, response\u001B[38;5;241m=\u001B[39mLLMResult(generations\u001B[38;5;241m=\u001B[39m[]))\n\u001B[1;32m--> 415\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[0;32m    416\u001B[0m flattened_outputs \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m    417\u001B[0m     LLMResult(generations\u001B[38;5;241m=\u001B[39m[res\u001B[38;5;241m.\u001B[39mgenerations], llm_output\u001B[38;5;241m=\u001B[39mres\u001B[38;5;241m.\u001B[39mllm_output)  \u001B[38;5;66;03m# type: ignore[list-item]\u001B[39;00m\n\u001B[0;32m    418\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m res \u001B[38;5;129;01min\u001B[39;00m results\n\u001B[0;32m    419\u001B[0m ]\n\u001B[0;32m    420\u001B[0m llm_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_combine_llm_outputs([res\u001B[38;5;241m.\u001B[39mllm_output \u001B[38;5;28;01mfor\u001B[39;00m res \u001B[38;5;129;01min\u001B[39;00m results])\n",
      "File \u001B[1;32mD:\\systemEnvironment\\Anaconda3\\envs\\llm\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:405\u001B[0m, in \u001B[0;36mBaseChatModel.generate\u001B[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001B[0m\n\u001B[0;32m    402\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(messages):\n\u001B[0;32m    403\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    404\u001B[0m         results\u001B[38;5;241m.\u001B[39mappend(\n\u001B[1;32m--> 405\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate_with_cache(\n\u001B[0;32m    406\u001B[0m                 m,\n\u001B[0;32m    407\u001B[0m                 stop\u001B[38;5;241m=\u001B[39mstop,\n\u001B[0;32m    408\u001B[0m                 run_manager\u001B[38;5;241m=\u001B[39mrun_managers[i] \u001B[38;5;28;01mif\u001B[39;00m run_managers \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    409\u001B[0m                 \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m    410\u001B[0m             )\n\u001B[0;32m    411\u001B[0m         )\n\u001B[0;32m    412\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    413\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m run_managers:\n",
      "File \u001B[1;32mD:\\systemEnvironment\\Anaconda3\\envs\\llm\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:624\u001B[0m, in \u001B[0;36mBaseChatModel._generate_with_cache\u001B[1;34m(self, messages, stop, run_manager, **kwargs)\u001B[0m\n\u001B[0;32m    622\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    623\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m inspect\u001B[38;5;241m.\u001B[39msignature(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate)\u001B[38;5;241m.\u001B[39mparameters\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_manager\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m--> 624\u001B[0m         result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate(\n\u001B[0;32m    625\u001B[0m             messages, stop\u001B[38;5;241m=\u001B[39mstop, run_manager\u001B[38;5;241m=\u001B[39mrun_manager, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m    626\u001B[0m         )\n\u001B[0;32m    627\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    628\u001B[0m         result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate(messages, stop\u001B[38;5;241m=\u001B[39mstop, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\systemEnvironment\\Anaconda3\\envs\\llm\\lib\\site-packages\\langchain_community\\chat_models\\zhipuai.py:267\u001B[0m, in \u001B[0;36mChatZhipuAI._generate\u001B[1;34m(self, messages, stop, run_manager, stream, **kwargs)\u001B[0m\n\u001B[0;32m    264\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m should_stream:\n\u001B[0;32m    265\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minvoke(prompt)\n\u001B[1;32m--> 267\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[43mresponse\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcode\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m200\u001B[39m:\n\u001B[0;32m    268\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(response)\n\u001B[0;32m    270\u001B[0m     content \u001B[38;5;241m=\u001B[39m response[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata\u001B[39m\u001B[38;5;124m\"\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mchoices\u001B[39m\u001B[38;5;124m\"\u001B[39m][\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcontent\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "\u001B[1;31mTypeError\u001B[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatZhipuAI\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "zhipuai_info = data_key.get(\"zhipuai\", {})\n",
    "zhipuai_api_key = zhipuai_info.get('APIKey')\n",
    "\n",
    "chat = ChatZhipuAI(\n",
    "    api_key=zhipuai_api_key,\n",
    "    model=\"glm-4\",\n",
    "    temperature=0.5,\n",
    ")\n",
    "messages = [\n",
    "    AIMessage(content=\"Hi.\"),\n",
    "    SystemMessage(content=\"Your role is a poet.\"),\n",
    "    HumanMessage(content=\"Write a short poem about AI in four lines.\"),\n",
    "]\n",
    "response = chat(messages)\n",
    "print(response.content)  # Displays the AI-generated poem"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T01:39:00.856057100Z",
     "start_time": "2024-04-03T01:39:00.821057200Z"
    }
   },
   "id": "d85923d0d7564ad3"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\systemEnvironment\\Anaconda3\\envs\\llm\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "[WARNING] [04-03 10:31:00] base.py:406 [t:21688]: retry is not available when stream is enabled\n",
      "[WARNING] [04-03 10:31:00] base.py:621 [t:21688]: This key `stop` does not seem to be a parameter that the model `ERNIE-Bot-turbo` will accept\n",
      "[WARNING] [04-03 10:31:00] base.py:621 [t:21688]: This key `messages` does not seem to be a parameter that the model `ERNIE-Bot-turbo` will accept\n",
      "[INFO] [04-03 10:31:00] openapi_requestor.py:336 [t:21688]: requesting llm api endpoint: /chat/eb-instant\n",
      "[ERROR] [04-03 10:31:01] openapi_requestor.py:222 [t:21688]: api request req_id:  failed with error code: 17, err msg: Open api daily request limit reached, please check https://cloud.baidu.com/doc/WENXINWORKSHOP/s/tlmyncueh\n"
     ]
    },
    {
     "ename": "APIError",
     "evalue": "api return error, req_id:  code: 17, msg: Open api daily request limit reached",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAPIError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[20], line 10\u001B[0m\n\u001B[0;32m      7\u001B[0m os\u001B[38;5;241m.\u001B[39menviron[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mQIANFAN_SK\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m qianfan_info\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mSK\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      9\u001B[0m llm \u001B[38;5;241m=\u001B[39m QianfanLLMEndpoint(streaming\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m---> 10\u001B[0m res \u001B[38;5;241m=\u001B[39m \u001B[43mllm\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mhi\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28mprint\u001B[39m(res)\n",
      "File \u001B[1;32mD:\\systemEnvironment\\Anaconda3\\envs\\llm\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:145\u001B[0m, in \u001B[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    143\u001B[0m     warned \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m    144\u001B[0m     emit_warning()\n\u001B[1;32m--> 145\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m wrapped(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\systemEnvironment\\Anaconda3\\envs\\llm\\lib\\site-packages\\langchain_core\\language_models\\llms.py:1028\u001B[0m, in \u001B[0;36mBaseLLM.__call__\u001B[1;34m(self, prompt, stop, callbacks, tags, metadata, **kwargs)\u001B[0m\n\u001B[0;32m   1021\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(prompt, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m   1022\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   1023\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mArgument `prompt` is expected to be a string. Instead found \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1024\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(prompt)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. If you want to run the LLM on multiple prompts, use \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1025\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`generate` instead.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1026\u001B[0m     )\n\u001B[0;32m   1027\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m (\n\u001B[1;32m-> 1028\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgenerate(\n\u001B[0;32m   1029\u001B[0m         [prompt],\n\u001B[0;32m   1030\u001B[0m         stop\u001B[38;5;241m=\u001B[39mstop,\n\u001B[0;32m   1031\u001B[0m         callbacks\u001B[38;5;241m=\u001B[39mcallbacks,\n\u001B[0;32m   1032\u001B[0m         tags\u001B[38;5;241m=\u001B[39mtags,\n\u001B[0;32m   1033\u001B[0m         metadata\u001B[38;5;241m=\u001B[39mmetadata,\n\u001B[0;32m   1034\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m   1035\u001B[0m     )\n\u001B[0;32m   1036\u001B[0m     \u001B[38;5;241m.\u001B[39mgenerations[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m   1037\u001B[0m     \u001B[38;5;241m.\u001B[39mtext\n\u001B[0;32m   1038\u001B[0m )\n",
      "File \u001B[1;32mD:\\systemEnvironment\\Anaconda3\\envs\\llm\\lib\\site-packages\\langchain_core\\language_models\\llms.py:748\u001B[0m, in \u001B[0;36mBaseLLM.generate\u001B[1;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001B[0m\n\u001B[0;32m    731\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    732\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    733\u001B[0m         )\n\u001B[0;32m    734\u001B[0m     run_managers \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m    735\u001B[0m         callback_manager\u001B[38;5;241m.\u001B[39mon_llm_start(\n\u001B[0;32m    736\u001B[0m             dumpd(\u001B[38;5;28mself\u001B[39m),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    746\u001B[0m         )\n\u001B[0;32m    747\u001B[0m     ]\n\u001B[1;32m--> 748\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate_helper(\n\u001B[0;32m    749\u001B[0m         prompts, stop, run_managers, \u001B[38;5;28mbool\u001B[39m(new_arg_supported), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m    750\u001B[0m     )\n\u001B[0;32m    751\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m output\n\u001B[0;32m    752\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(missing_prompts) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[1;32mD:\\systemEnvironment\\Anaconda3\\envs\\llm\\lib\\site-packages\\langchain_core\\language_models\\llms.py:606\u001B[0m, in \u001B[0;36mBaseLLM._generate_helper\u001B[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001B[0m\n\u001B[0;32m    604\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m run_manager \u001B[38;5;129;01min\u001B[39;00m run_managers:\n\u001B[0;32m    605\u001B[0m         run_manager\u001B[38;5;241m.\u001B[39mon_llm_error(e, response\u001B[38;5;241m=\u001B[39mLLMResult(generations\u001B[38;5;241m=\u001B[39m[]))\n\u001B[1;32m--> 606\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[0;32m    607\u001B[0m flattened_outputs \u001B[38;5;241m=\u001B[39m output\u001B[38;5;241m.\u001B[39mflatten()\n\u001B[0;32m    608\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m manager, flattened_output \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(run_managers, flattened_outputs):\n",
      "File \u001B[1;32mD:\\systemEnvironment\\Anaconda3\\envs\\llm\\lib\\site-packages\\langchain_core\\language_models\\llms.py:593\u001B[0m, in \u001B[0;36mBaseLLM._generate_helper\u001B[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001B[0m\n\u001B[0;32m    583\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_generate_helper\u001B[39m(\n\u001B[0;32m    584\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    585\u001B[0m     prompts: List[\u001B[38;5;28mstr\u001B[39m],\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    589\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[0;32m    590\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m LLMResult:\n\u001B[0;32m    591\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    592\u001B[0m         output \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m--> 593\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate(\n\u001B[0;32m    594\u001B[0m                 prompts,\n\u001B[0;32m    595\u001B[0m                 stop\u001B[38;5;241m=\u001B[39mstop,\n\u001B[0;32m    596\u001B[0m                 \u001B[38;5;66;03m# TODO: support multiple run managers\u001B[39;00m\n\u001B[0;32m    597\u001B[0m                 run_manager\u001B[38;5;241m=\u001B[39mrun_managers[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m run_managers \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    598\u001B[0m                 \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m    599\u001B[0m             )\n\u001B[0;32m    600\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m new_arg_supported\n\u001B[0;32m    601\u001B[0m             \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate(prompts, stop\u001B[38;5;241m=\u001B[39mstop)\n\u001B[0;32m    602\u001B[0m         )\n\u001B[0;32m    603\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    604\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m run_manager \u001B[38;5;129;01min\u001B[39;00m run_managers:\n",
      "File \u001B[1;32mD:\\systemEnvironment\\Anaconda3\\envs\\llm\\lib\\site-packages\\langchain_core\\language_models\\llms.py:1209\u001B[0m, in \u001B[0;36mLLM._generate\u001B[1;34m(self, prompts, stop, run_manager, **kwargs)\u001B[0m\n\u001B[0;32m   1206\u001B[0m new_arg_supported \u001B[38;5;241m=\u001B[39m inspect\u001B[38;5;241m.\u001B[39msignature(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call)\u001B[38;5;241m.\u001B[39mparameters\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_manager\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   1207\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m prompt \u001B[38;5;129;01min\u001B[39;00m prompts:\n\u001B[0;32m   1208\u001B[0m     text \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m-> 1209\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call(prompt, stop\u001B[38;5;241m=\u001B[39mstop, run_manager\u001B[38;5;241m=\u001B[39mrun_manager, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1210\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m new_arg_supported\n\u001B[0;32m   1211\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call(prompt, stop\u001B[38;5;241m=\u001B[39mstop, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1212\u001B[0m     )\n\u001B[0;32m   1213\u001B[0m     generations\u001B[38;5;241m.\u001B[39mappend([Generation(text\u001B[38;5;241m=\u001B[39mtext)])\n\u001B[0;32m   1214\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m LLMResult(generations\u001B[38;5;241m=\u001B[39mgenerations)\n",
      "File \u001B[1;32mD:\\systemEnvironment\\Anaconda3\\envs\\llm\\lib\\site-packages\\langchain_community\\llms\\baidu_qianfan_endpoint.py:179\u001B[0m, in \u001B[0;36mQianfanLLMEndpoint._call\u001B[1;34m(self, prompt, stop, run_manager, **kwargs)\u001B[0m\n\u001B[0;32m    177\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstreaming:\n\u001B[0;32m    178\u001B[0m     completion \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m--> 179\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m chunk \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stream(prompt, stop, run_manager, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    180\u001B[0m         completion \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m chunk\u001B[38;5;241m.\u001B[39mtext\n\u001B[0;32m    181\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m completion\n",
      "File \u001B[1;32mD:\\systemEnvironment\\Anaconda3\\envs\\llm\\lib\\site-packages\\langchain_community\\llms\\baidu_qianfan_endpoint.py:216\u001B[0m, in \u001B[0;36mQianfanLLMEndpoint._stream\u001B[1;34m(self, prompt, stop, run_manager, **kwargs)\u001B[0m\n\u001B[0;32m    214\u001B[0m params \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_convert_prompt_msg_params(prompt, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m{\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstream\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28;01mTrue\u001B[39;00m})\n\u001B[0;32m    215\u001B[0m params[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstop\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m stop\n\u001B[1;32m--> 216\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m res \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mdo(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams):\n\u001B[0;32m    217\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m res:\n\u001B[0;32m    218\u001B[0m         chunk \u001B[38;5;241m=\u001B[39m GenerationChunk(text\u001B[38;5;241m=\u001B[39mres[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mresult\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n",
      "File \u001B[1;32mD:\\systemEnvironment\\Anaconda3\\envs\\llm\\lib\\site-packages\\qianfan\\resources\\requestor\\openapi_requestor.py:279\u001B[0m, in \u001B[0;36mQfAPIRequestor._compensate_token_usage_stream\u001B[1;34m(self, resp, token_count)\u001B[0m\n\u001B[0;32m    277\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(resp, Iterator):\n\u001B[0;32m    278\u001B[0m     token_usage \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m--> 279\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m res \u001B[38;5;129;01min\u001B[39;00m resp:\n\u001B[0;32m    280\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mX-Ratelimit-Limit-Tokens\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m res\u001B[38;5;241m.\u001B[39mheaders:\n\u001B[0;32m    281\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_token_limiter\u001B[38;5;241m.\u001B[39mreset_once(\n\u001B[0;32m    282\u001B[0m                 \u001B[38;5;28mint\u001B[39m(res\u001B[38;5;241m.\u001B[39mheaders[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mX-Ratelimit-Limit-Tokens\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[0;32m    283\u001B[0m             )\n",
      "File \u001B[1;32mD:\\systemEnvironment\\Anaconda3\\envs\\llm\\lib\\site-packages\\qianfan\\resources\\requestor\\base.py:182\u001B[0m, in \u001B[0;36m_stream_latency.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    180\u001B[0m resp \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    181\u001B[0m sse_block_receive_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m--> 182\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m r \u001B[38;5;129;01min\u001B[39;00m resp:\n\u001B[0;32m    183\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m first_token_latency \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    184\u001B[0m         first_token_latency \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start_time\n",
      "File \u001B[1;32mD:\\systemEnvironment\\Anaconda3\\envs\\llm\\lib\\site-packages\\qianfan\\resources\\requestor\\openapi_requestor.py:117\u001B[0m, in \u001B[0;36mQfAPIRequestor._request_stream\u001B[1;34m(self, request, data_postprocess)\u001B[0m\n\u001B[0;32m    114\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    115\u001B[0m     \u001B[38;5;66;03m# the response might be error message in json format\u001B[39;00m\n\u001B[0;32m    116\u001B[0m     json_body \u001B[38;5;241m=\u001B[39m json\u001B[38;5;241m.\u001B[39mloads(body_str)\n\u001B[1;32m--> 117\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_check_error\u001B[49m\u001B[43m(\u001B[49m\u001B[43mjson_body\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    118\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m errors\u001B[38;5;241m.\u001B[39mAccessTokenExpiredError:\n\u001B[0;32m    119\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m token_refreshed:\n",
      "File \u001B[1;32mD:\\systemEnvironment\\Anaconda3\\envs\\llm\\lib\\site-packages\\qianfan\\resources\\requestor\\openapi_requestor.py:232\u001B[0m, in \u001B[0;36mQfAPIRequestor._check_error\u001B[1;34m(self, body)\u001B[0m\n\u001B[0;32m    227\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m error_code \u001B[38;5;129;01min\u001B[39;00m {\n\u001B[0;32m    228\u001B[0m     APIErrorCode\u001B[38;5;241m.\u001B[39mAPITokenExpired\u001B[38;5;241m.\u001B[39mvalue,\n\u001B[0;32m    229\u001B[0m     APIErrorCode\u001B[38;5;241m.\u001B[39mAPITokenInvalid\u001B[38;5;241m.\u001B[39mvalue,\n\u001B[0;32m    230\u001B[0m }:\n\u001B[0;32m    231\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m errors\u001B[38;5;241m.\u001B[39mAccessTokenExpiredError\n\u001B[1;32m--> 232\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m errors\u001B[38;5;241m.\u001B[39mAPIError(error_code, err_msg, req_id)\n",
      "\u001B[1;31mAPIError\u001B[0m: api return error, req_id:  code: 17, msg: Open api daily request limit reached"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from langchain_community.llms import QianfanLLMEndpoint\n",
    "\n",
    "qianfan_info = data_key.get(\"qianfan\", {})\n",
    "os.environ[\"QIANFAN_AK\"] = qianfan_info.get('AK')\n",
    "os.environ[\"QIANFAN_SK\"] = qianfan_info.get('SK')\n",
    "\n",
    "llm = QianfanLLMEndpoint(streaming=True)\n",
    "res = llm(\"hi\")\n",
    "print(res)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T02:31:01.344636400Z",
     "start_time": "2024-04-03T02:31:00.967630700Z"
    }
   },
   "id": "be972df664bf6ac4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "cf227997daab56aa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ed5819745289b964"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
